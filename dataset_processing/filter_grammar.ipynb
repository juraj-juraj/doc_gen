{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "docstrings = datasets.load_dataset(\"juraj-juraj/doc_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docstrings[\"validation\"][300][\"docstring\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "\n",
    "parser = Lark(r\"\"\"\n",
    "start: LINE+ \"\\n\"* parameters \"\\n\"*\n",
    "\n",
    "parameters.1: section+\n",
    "\n",
    "section.1: WORD \"\\n\" \"-\"+ \"\\n\"+ SENTENCE+ [\"\\n\"]\n",
    "SENTENCE: /./+ [\"\\n\"]\n",
    "LINE: /(.[^-])+/\"\\n\"\n",
    "\n",
    "\n",
    "// imports WORD from library\n",
    "%import common.WORD   \n",
    "\n",
    "// Disregard spaces in text\n",
    "%ignore \" \"\n",
    "%ignore \"\\t\" \"\"\")\n",
    "\n",
    "try:\n",
    "    \n",
    "    parsed = parser.parse(text=docstrings[\"validation\"][300][\"docstring\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"parsed !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Protocol\n",
    "\n",
    "\n",
    "class GrammarFilterI(Protocol):\n",
    "    def parse(self, comment: str) -> bool:\n",
    "        ...\n",
    "    \n",
    "    def __call__(self, comment: str) -> bool:\n",
    "        ...\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class GrammarFilter:\n",
    "    grammar: str\n",
    "    parser: Lark = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.parser = Lark(self.grammar)\n",
    "    \n",
    "    def parse(self, comment: str) -> bool:\n",
    "            try:\n",
    "                self.parser.parse(text=comment)\n",
    "            except Exception:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def __call__(self, comment: str) -> bool:\n",
    "        return self.parse(comment)\n",
    "\n",
    "numpydoc_parser = GrammarFilter(grammar=r\"\"\"\n",
    "start: LINE+ \"\\n\"* parameters \"\\n\"*\n",
    "\n",
    "parameters.1: section+\n",
    "\n",
    "section.1: WORD \"\\n\" \"-\"+ \"\\n\"+ SENTENCE+ [\"\\n\"]\n",
    "SENTENCE: /./+ [\"\\n\"]\n",
    "LINE: /(.[^-])+/\"\\n\"\n",
    "\n",
    "\n",
    "// imports WORD from library\n",
    "%import common.WORD   \n",
    "\n",
    "// Disregard spaces in text\n",
    "%ignore \" \"\n",
    "%ignore \"\\t\" \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docstrings = docstrings[\"validation\"].to_pandas()\n",
    "numpydoc_dataset = df_docstrings.iloc[[numpydoc_parser(docstring) for docstring in df_docstrings.docstring]]\n",
    "\n",
    "\n",
    "print(numpydoc_dataset.iloc[3][\"docstring\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datasets\n",
    "\n",
    "#docstrings = datasets.load_from_disk(\"../data/googlestyle_dataset_processed_2.ds\")\n",
    "docstrings = datasets.load_dataset(\"juraj-juraj/doc_gen\")\n",
    "\n",
    "print(f\"len docstrings: {len(docstrings['train'])}\")\n",
    "train_data = docstrings[\"train\"][::][\"docstring\"]\n",
    "\n",
    "print(f\"original seed: {random.seed}\")\n",
    "random.seed(time.time())\n",
    "\n",
    "for _ in range(0, 20):\n",
    "    print(train_data[random.randrange(0, len(train_data))], end=\"\\n\\n------------------\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_pickle(\"../data/unannotated_functions.pkl\")\n",
    "df = pd.DataFrame(raw_data)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "#df.iloc[0:100][\"function\"]\n",
    "with open(\"../evaluation/corpus.py\", mode=\"+a\") as f:\n",
    "    f.write(\"\\n\\n\".join(df.iloc[0:5][\"function\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random functions from dataset for corpus at evaluation\n",
    "\n",
    "import datasets\n",
    "\n",
    "data = datasets.load_from_disk(\"../data/googlestyle_dataset_processed_2.ds\")\n",
    "df = data[\"validation\"].to_pandas()\n",
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.iloc[0:120]\n",
    "\n",
    "with open(\"../evaluation/corpus_2.py\", mode=\"w\") as f:\n",
    "    for i in range(0, len(df)):\n",
    "        f.write(f\"\\\"\\\"\\\"{df.iloc[i]['docstring']}\\n\\\"\\\"\\\"\\n{df.iloc[i]['function']} \\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
