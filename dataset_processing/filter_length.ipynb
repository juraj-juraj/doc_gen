{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-220m\")\n",
    "\n",
    "inputs = tokenizer.encode(\" Predicts the model for the given image. Args: X_img_path: Path to the image to predict. knn_clf: The classifier to use. model_path: Path to the model to use. distance_threshold: The maximum number of times the model is found in the image. Returns: A list of the classifiers that are not within the threshold.\", truncation=True)\n",
    "print(f\" len inputs: {len(inputs)}\")\n",
    "print(tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_folder = \"../data/python_dataset.dse\"\n",
    "if(os.path.exists(dataset_folder)):\n",
    "    print(\"loading local dataset\")\n",
    "    docstrings = datasets.load_from_disk(dataset_folder)[\"train\"]\n",
    "else:\n",
    "    print(\"loading from dataset from huggingface\")\n",
    "    docstrings = datasets.load_dataset(\"juraj-juraj/doc_gen\")[\"train\"]\n",
    "\n",
    "docstring_lengths = [len(docstring) for docstring in docstrings[\"docstring\"]]\n",
    "\n",
    "docstring_lengths = list(filter(lambda x: x < 2000, docstring_lengths))\n",
    "\n",
    "plt.hist(docstring_lengths, bins=30, edgecolor='black')\n",
    "plt.xlabel('Docstring Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Docstring Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset = datasets.load_dataset(\"juraj-juraj/doc_gen\")\n",
    "\n",
    "train_dataset = pd.DataFrame.from_dict(dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lengths(dataset: pd.DataFrame, lower_bound: int = 50, high_bound: int = 500):\n",
    "    longer_than_lower = dataset[\"docstring\"].str.len() > lower_bound\n",
    "    shorter_than_higher = dataset[\"docstring\"].str.len() < high_bound\n",
    "    return dataset[shorter_than_higher & longer_than_lower]\n",
    "\n",
    "train_dataset = filter_lengths(train_dataset)\n",
    "train_dataset = train_dataset.reset_index()\n",
    "\n",
    "docstring_lengths = [len(docstring) for docstring in train_dataset[\"docstring\"]]\n",
    "docstring_lengths = list(filter(lambda x: x < 2000, docstring_lengths))\n",
    "\n",
    "\n",
    "plt.hist(docstring_lengths, bins=30, edgecolor='black')\n",
    "plt.xlabel('Docstring Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Docstring Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = train_dataset[train_dataset[\"docstring\"].str.len() > 50]\n",
    "train_dataset = train_dataset[train_dataset[\"docstring\"].str.len() < 500]\n",
    "\n",
    "eval_dataset = pd.DataFrame.from_dict(dataset[\"validation\"])\n",
    "eval_dataset = eval_dataset[eval_dataset[\"docstring\"].str.len() > 50]\n",
    "eval_dataset = eval_dataset[eval_dataset[\"docstring\"].str.len() < 500]\n",
    "\n",
    "test_dataset = pd.DataFrame.from_dict(dataset[\"test\"])\n",
    "test_dataset = test_dataset[test_dataset[\"docstring\"].str.len() > 50]\n",
    "test_dataset = test_dataset[test_dataset[\"docstring\"].str.len() < 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = train_dataset.copy().reset_index()\n",
    "s[[\"docstring\", \"function\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.reset_index()\n",
    "eval_dataset = eval_dataset.reset_index()\n",
    "test_dataset = test_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds_train = Dataset.from_pandas(train_dataset[[\"docstring\", \"function\"]])\n",
    "ds_validation = Dataset.from_pandas(eval_dataset[[\"docstring\", \"function\"]])\n",
    "ds_test = Dataset.from_pandas(test_dataset[[\"docstring\", \"function\"]])\n",
    "\n",
    "ds_train[\"docstring\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": ds_train,\n",
    "            \"validation\": ds_validation,\n",
    "            \"test\": ds_test,\n",
    "        }\n",
    "    )\n",
    "\n",
    "dataset_dict.save_to_disk(\"../docstring_len_filtered.ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = datasets.load_from_disk(\"../docstring_len_filtered.ds\")\n",
    "dataset_dict[\"train\"][\"docstring\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converts a style_dict to an xlsxwriter format dict\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        style_dict : style dictionary to convert\n",
      "        num_format_str : optional number format string\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"juraj-juraj/doc_gen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stackted recurrent neural networks GRU or LSTM\n",
      "\n",
      "        Args:\n",
      "            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n",
      "            n_hidden_list: list with number of hidden units at the ouput of each layer\n",
      "            seq_lengths: length of sequences for different length sequences in batch\n",
      "                can be None for maximum length as a length for every sample in the batch\n",
      "            cell_type: 'lstm' or 'gru'\n",
      "            use_peepholes: whether to use peephole connections (only 'lstm' case affected)\n",
      "            name: what variable_scope to use for the network parameters\n",
      "        Returns:\n",
      "            units: tensor at the output of the last recurrent layer\n",
      "                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n",
      "            last_units: tensor of last hidden states for GRU and tuple\n",
      "                of last hidden stated and last cell states for LSTM\n",
      "                dimensionality of cell states and hidden states are\n",
      "                similar and equal to [B x 2 * H], where B - batch\n",
      "                size and H is number of hidden units\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][20010][\"docstring\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
