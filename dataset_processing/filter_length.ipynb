{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " len inputs: 80\n",
      "<s> Predicts the model for the given image. Args: X_img_path: Path to the image to predict. knn_clf: The classifier to use. model_path: Path to the model to use. distance_threshold: The maximum number of times the model is found in the image. Returns: A list of the classifiers that are not within the threshold.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-220m\")\n",
    "\n",
    "inputs = tokenizer.encode(\" Predicts the model for the given image. Args: X_img_path: Path to the image to predict. knn_clf: The classifier to use. model_path: Path to the model to use. distance_threshold: The maximum number of times the model is found in the image. Returns: A list of the classifiers that are not within the threshold.\", truncation=True)\n",
    "print(f\" len inputs: {len(inputs)}\")\n",
    "print(tokenizer.decode(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if(os.path.exists(\"../docstring_len_filtered.ds\")):\n",
    "    docstrings = datasets.load_from_disk(\"../docstring_len_filtered.ds\")[\"train\"]\n",
    "else:\n",
    "    print(\"Loading dataset\")\n",
    "\n",
    "docstring_lengths = [len(docstring) for docstring in docstrings[\"docstring\"]]\n",
    "\n",
    "#docstring_lengths = list(filter(lambda x: x < 2000, docstring_lengths))\n",
    "\n",
    "plt.hist(docstring_lengths, bins=30, edgecolor='black')\n",
    "plt.xlabel('Docstring Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Docstring Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juraj/.local/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset = datasets.load_dataset(\"juraj-juraj/doc_gen\")\n",
    "\n",
    "train_dataset = pd.DataFrame.from_dict(dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = train_dataset[train_dataset[\"docstring\"].str.len() > 50]\n",
    "train_dataset = train_dataset[train_dataset[\"docstring\"].str.len() < 500]\n",
    "\n",
    "eval_dataset = pd.DataFrame.from_dict(dataset[\"validation\"])\n",
    "eval_dataset = eval_dataset[eval_dataset[\"docstring\"].str.len() > 50]\n",
    "eval_dataset = eval_dataset[eval_dataset[\"docstring\"].str.len() < 500]\n",
    "\n",
    "test_dataset = pd.DataFrame.from_dict(dataset[\"test\"])\n",
    "test_dataset = test_dataset[test_dataset[\"docstring\"].str.len() > 50]\n",
    "test_dataset = test_dataset[test_dataset[\"docstring\"].str.len() < 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docstring</th>\n",
       "      <th>function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>converts a style_dict to an xlsxwriter format ...</td>\n",
       "      <td>def convert(cls, style_dict, num_format_str=No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Convert DataFrame to Series with multi-level I...</td>\n",
       "      <td>def stack(frame, level=-1, dropna=True):\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parameters\\n    ----------\\n    s: string\\n   ...</td>\n",
       "      <td>def _split_line(s, parts):\\n    \\n    out = {}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parse a vector of float values representing IB...</td>\n",
       "      <td>def _parse_float_vec(vec):\\n    \\n\\n    dtype ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Get number of records in file.\\n\\n        This...</td>\n",
       "      <td>def _record_count(self):\\n        \\n\\n        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314708</th>\n",
       "      <td>Trim this fastqSequence in-place by removing &lt;...</td>\n",
       "      <td>def trimRight(self, amount):\\n    \\n    if amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314709</th>\n",
       "      <td>Trim this fastqSequence in-place by removing &lt;...</td>\n",
       "      <td>def trimLeft(self, amount):\\n    \\n    if amou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314710</th>\n",
       "      <td>Get the realtive quality score (i.e. the phred...</td>\n",
       "      <td>def getRelativeQualityScore(self, i, score_typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314711</th>\n",
       "      <td>:return: string representation of this NGS rea...</td>\n",
       "      <td>def to_fastq_str(self):\\n    \\n    return \"@\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314712</th>\n",
       "      <td>Test trying to make a read with mismatched seq...</td>\n",
       "      <td>def test_length_mismatch(self):\\n    \\n    sel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314713 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docstring  \\\n",
       "0       converts a style_dict to an xlsxwriter format ...   \n",
       "1       Convert DataFrame to Series with multi-level I...   \n",
       "2       Parameters\\n    ----------\\n    s: string\\n   ...   \n",
       "3       Parse a vector of float values representing IB...   \n",
       "4       Get number of records in file.\\n\\n        This...   \n",
       "...                                                   ...   \n",
       "314708  Trim this fastqSequence in-place by removing <...   \n",
       "314709  Trim this fastqSequence in-place by removing <...   \n",
       "314710  Get the realtive quality score (i.e. the phred...   \n",
       "314711  :return: string representation of this NGS rea...   \n",
       "314712  Test trying to make a read with mismatched seq...   \n",
       "\n",
       "                                                 function  \n",
       "0       def convert(cls, style_dict, num_format_str=No...  \n",
       "1       def stack(frame, level=-1, dropna=True):\\n    ...  \n",
       "2       def _split_line(s, parts):\\n    \\n    out = {}...  \n",
       "3       def _parse_float_vec(vec):\\n    \\n\\n    dtype ...  \n",
       "4       def _record_count(self):\\n        \\n\\n        ...  \n",
       "...                                                   ...  \n",
       "314708  def trimRight(self, amount):\\n    \\n    if amo...  \n",
       "314709  def trimLeft(self, amount):\\n    \\n    if amou...  \n",
       "314710  def getRelativeQualityScore(self, i, score_typ...  \n",
       "314711  def to_fastq_str(self):\\n    \\n    return \"@\" ...  \n",
       "314712  def test_length_mismatch(self):\\n    \\n    sel...  \n",
       "\n",
       "[314713 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = train_dataset.copy().reset_index()\n",
    "s[[\"docstring\", \"function\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.reset_index()\n",
    "eval_dataset = eval_dataset.reset_index()\n",
    "test_dataset = test_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['converts a style_dict to an xlsxwriter format dict\\n\\n        Parameters\\n        ----------\\n        style_dict : style dictionary to convert\\n        num_format_str : optional number format string',\n",
       " 'Convert DataFrame to Series with multi-level Index. Columns become the\\n    second level of the resulting hierarchical index\\n\\n    Returns\\n    -------\\n    stacked : Series',\n",
       " \"Parameters\\n    ----------\\n    s: string\\n        Fixed-length string to split\\n    parts: list of (name, length) pairs\\n        Used to break up string, name '_' will be filtered from output.\\n\\n    Returns\\n    -------\\n    Dict of name:contents of string at given location.\",\n",
       " 'Parse a vector of float values representing IBM 8 byte floats into\\n    native 8 byte floats.',\n",
       " 'Get number of records in file.\\n\\n        This is maybe suboptimal because we have to seek to the end of\\n        the file.\\n\\n        Side effect: returns file position to record_start.',\n",
       " 'Reads lines from Xport file and returns as dataframe\\n\\n        Parameters\\n        ----------\\n        size : int, defaults to None\\n            Number of lines to read.  If None, reads whole file.\\n\\n        Returns\\n        -------\\n        DataFrame',\n",
       " 'return a single array of a block that has a single dtype; if dtype is\\n    not None, coerce to this dtype',\n",
       " 'return an array of blocks that potentially have different dtypes',\n",
       " 'return an array of blocks that potentially have different dtypes (and\\n    are sparse)',\n",
       " 'Find the common dtype for `blocks`.\\n\\n    Parameters\\n    ----------\\n    blocks : List[Block]\\n\\n    Returns\\n    -------\\n    dtype : Optional[Union[np.dtype, ExtensionDtype]]\\n        None is returned when `blocks` is empty.',\n",
       " 'Merge blocks having same dtype, exclude non-consolidating blocks',\n",
       " 'Compare two array_like inputs of the same shape or two scalar values\\n\\n    Calls operator.eq or re.search, depending on regex argument. If regex is\\n    True, perform an element-wise regex matching.\\n\\n    Parameters\\n    ----------\\n    a : array_like or scalar\\n    b : array_like or scalar\\n    regex : bool, default False\\n\\n    Returns\\n    -------\\n    mask : array_like of bool',\n",
       " 'If two indices overlap, add suffixes to overlapping entries.\\n\\n    If corresponding suffix is empty, the entry is simply converted to string.',\n",
       " 'Apply function to all values found in index.\\n\\n    This includes transforming multiindex entries separately.\\n    Only apply function to one level of the MultiIndex if level is specified.',\n",
       " 'Faster version of set(arr) for sequences of small numbers.',\n",
       " 'Concatenate block managers into one.\\n\\n    Parameters\\n    ----------\\n    mgrs_indexers : list of (BlockManager, {axis: indexer,...}) tuples\\n    axes : list of Index\\n    concat_axis : int\\n    copy : bool',\n",
       " 'return an empty BlockManager with the items axis of len 0',\n",
       " 'Rename one of axes.\\n\\n        Parameters\\n        ----------\\n        mapper : unary callable\\n        axis : int\\n        copy : boolean, default True\\n        level : int, default None',\n",
       " 'return a dict of the counts of the function in BlockManager',\n",
       " 'Parameters\\n        ----------\\n        copy : boolean, default False\\n            Whether to copy the blocks',\n",
       " 'Parameters\\n        ----------\\n        copy : boolean, default False\\n            Whether to copy the blocks',\n",
       " \"Make deep or shallow copy of BlockManager\\n\\n        Parameters\\n        ----------\\n        deep : boolean o rstring, default True\\n            If False, return shallow copy (do not copy data)\\n            If 'all', copy data and a deep copy of the index\\n\\n        Returns\\n        -------\\n        copy : BlockManager\",\n",
       " 'Convert the blockmanager data into an numpy array.\\n\\n        Parameters\\n        ----------\\n        transpose : boolean, default False\\n            If True, transpose the return array\\n        items : list of strings or None\\n            Names of block items that will be included in the returned\\n            array. ``None`` means that all block items will be used\\n\\n        Returns\\n        -------\\n        arr : ndarray',\n",
       " 'Return ndarray from blocks with specified item order\\n        Items must be contained in the blocks',\n",
       " 'Return a dict of str(dtype) -> BlockManager\\n\\n        Parameters\\n        ----------\\n        copy : boolean, default True\\n\\n        Returns\\n        -------\\n        values : a dict of dtype -> BlockManager\\n\\n        Notes\\n        -----\\n        This consolidates based on str(dtype)',\n",
       " 'get a cross sectional for a given location in the\\n        items ; handle dups\\n\\n        return the result, is *could* be a view in the case of a\\n        single block',\n",
       " 'Join together blocks having same dtype\\n\\n        Returns\\n        -------\\n        y : BlockManager',\n",
       " 'Return values for selected item (ndarray or BlockManager).',\n",
       " 'Return the data as a SingleBlockManager if fastpath=True and possible\\n\\n        Otherwise return as a ndarray',\n",
       " 'Delete selected item (items if non-unique) in-place.',\n",
       " 'Set new item in-place. Does not consolidate. Adds new Block if not\\n        contained in the current set of items',\n",
       " 'Insert item at selected position.\\n\\n        Parameters\\n        ----------\\n        loc : int\\n        item : hashable\\n        value : array_like\\n        allow_duplicates: bool\\n            If False, trying to insert non-unique item will raise',\n",
       " \"Parameters\\n        ----------\\n        new_axis : Index\\n        indexer : ndarray of int64 or None\\n        axis : int\\n        fill_value : object\\n        allow_dups : bool\\n\\n        pandas-indexer with -1's only.\",\n",
       " 'Slice/take blocks along axis=0.\\n\\n        Overloaded for SingleBlock\\n\\n        Returns\\n        -------\\n        new_blocks : list of Block',\n",
       " 'Return a blockmanager with all blocks unstacked.\\n\\n        Parameters\\n        ----------\\n        unstacker_func : callable\\n            A (partially-applied) ``pd.core.reshape._Unstacker`` class.\\n        fill_value : Any\\n            fill_value for newly introduced missing values.\\n\\n        Returns\\n        -------\\n        unstacked : BlockManager',\n",
       " \"Delete single item from SingleBlockManager.\\n\\n        Ensures that self.blocks doesn't become empty.\",\n",
       " 'Concatenate a list of SingleBlockManagers into a single\\n        SingleBlockManager.\\n\\n        Used for pd.concat of Series objects with axis=0.\\n\\n        Parameters\\n        ----------\\n        to_concat : list of SingleBlockManagers\\n        new_axis : Index of the result\\n\\n        Returns\\n        -------\\n        SingleBlockManager',\n",
       " 'Gets called prior to a ufunc (and after)\\n\\n        See SparseArray.__array_wrap__ for detail.',\n",
       " 'Gets called after any ufunc or other array operations, necessary\\n        to pass on the index.',\n",
       " 'Construct SparseSeries from array.\\n\\n        .. deprecated:: 0.23.0\\n            Use the pd.SparseSeries(..) constructor instead.',\n",
       " 'return my self as a sparse array, do not copy by default',\n",
       " 'Return the i-th value or values in the SparseSeries by location\\n\\n        Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n\\n        Returns\\n        -------\\n        value : scalar (int) or Series (slice, sequence)',\n",
       " 'Return an object with absolute value taken. Only applicable to objects\\n        that are all numeric\\n\\n        Returns\\n        -------\\n        abs: same type as caller',\n",
       " 'Returns value occupying requested label, default to specified\\n        missing value if not present. Analogous to dict.get\\n\\n        Parameters\\n        ----------\\n        label : object\\n            Label value looking for\\n        default : object, optional\\n            Value to return if label not in index\\n\\n        Returns\\n        -------\\n        y : scalar',\n",
       " 'Retrieve single value at passed index label\\n\\n        .. deprecated:: 0.21.0\\n\\n        Please use .at[] or .iat[] accessors.\\n\\n        Parameters\\n        ----------\\n        index : label\\n        takeable : interpret the index as indexers, default False\\n\\n        Returns\\n        -------\\n        value : scalar value',\n",
       " 'Convert SparseSeries to a Series.\\n\\n        Returns\\n        -------\\n        s : Series',\n",
       " 'Make a copy of the SparseSeries. Only the actual sparse values need to\\n        be copied',\n",
       " 'Conform sparse values to new SparseIndex\\n\\n        Parameters\\n        ----------\\n        new_index : {BlockIndex, IntIndex}\\n\\n        Returns\\n        -------\\n        reindexed : SparseSeries',\n",
       " 'Cumulative sum of non-NA/null values.\\n\\n        When performing the cumulative summation, any non-NA/null values will\\n        be skipped. The resulting SparseSeries will preserve the locations of\\n        NaN values, but the fill value will be `np.nan` regardless.\\n\\n        Parameters\\n        ----------\\n        axis : {0}\\n\\n        Returns\\n        -------\\n        cumsum : SparseSeries',\n",
       " 'Analogous to Series.dropna. If fill_value=NaN, returns a dense Series',\n",
       " \"Combine Series values, choosing the calling Series's values\\n        first. Result index will be the union of the two indexes\\n\\n        Parameters\\n        ----------\\n        other : Series\\n\\n        Returns\\n        -------\\n        y : Series\",\n",
       " 'Create a cache of unique dates from an array of dates\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n    format : string\\n        Strftime format to parse time\\n    cache : boolean\\n        True attempts to create a cache of converted values\\n    convert_listlike : function\\n        Conversion function to apply on dates\\n\\n    Returns\\n    -------\\n    cache_array : Series\\n        Cache of converted, unique dates. Can be empty',\n",
       " \"Helper function for to_datetime.\\n    Adjust input argument to the specified origin\\n\\n    Parameters\\n    ----------\\n    arg : list, tuple, ndarray, Series, Index\\n        date to be adjusted\\n    origin : 'julian' or Timestamp\\n        origin offset for the arg\\n    unit : string\\n        passed unit from to_datetime, must be 'D'\\n\\n    Returns\\n    -------\\n    ndarray or scalar of adjusted date(s)\",\n",
       " \"try to parse the YYYYMMDD/%Y%m%d format, try to deal with NaT-like,\\n    arg is a passed in as an object dtype, but could really be ints/strings\\n    with nan-like/or floats (e.g. with nan)\\n\\n    Parameters\\n    ----------\\n    arg : passed value\\n    errors : 'raise','ignore','coerce'\",\n",
       " \"Returns a tuple containing the paramenter list with defaults\\n    and parameter list.\\n\\n    Examples\\n    --------\\n    >>> def f(a, b, c=2):\\n    >>>     return a * b * c\\n    >>> print(make_signature(f))\\n    (['a', 'b', 'c=2'], ['a', 'b', 'c'])\",\n",
       " 'Return a list of tuples of the (attr, formatted_value)',\n",
       " 'Returns the indices that would sort the index and its\\n        underlying data.\\n\\n        Returns\\n        -------\\n        argsorted : numpy array\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argsort',\n",
       " 'Determines if two Index objects contain the same elements.',\n",
       " 'Form the intersection of two Index objects.\\n\\n        Parameters\\n        ----------\\n        other : Index or array-like\\n        sort : False or None, default False\\n            Sort the resulting index if possible\\n\\n            .. versionadded:: 0.24.0\\n\\n            .. versionchanged:: 0.24.1\\n\\n               Changed the default to ``False`` to match the behaviour\\n               from before 0.24.0.\\n\\n        Returns\\n        -------\\n        intersection : Index',\n",
       " 'Returns the smallest element greater than or equal to the limit',\n",
       " 'Returns the largest element smaller than or equal to the limit',\n",
       " \"Extended Euclidean algorithms to solve Bezout's identity:\\n           a*x + b*y = gcd(x, y)\\n        Finds one particular solution for x, y: s, t\\n        Returns: gcd, s, t\",\n",
       " 'Conserve RangeIndex type for scalar and slice keys.',\n",
       " 'Convert the PandasArray to a :class:`numpy.ndarray`.\\n\\n        By default, this requires no coercion or copying of data.\\n\\n        Parameters\\n        ----------\\n        dtype : numpy.dtype\\n            The NumPy dtype to pass to :func:`numpy.asarray`.\\n        copy : bool, default False\\n            Whether to copy the underlying data.\\n\\n        Returns\\n        -------\\n        ndarray',\n",
       " 'Glues together two sets of strings using the amount of space requested.\\n    The idea is to prettify.\\n\\n    ----------\\n    space : int\\n        number of spaces for padding\\n    lists : str\\n        list of str which being joined\\n    strlen : callable\\n        function used to calculate the length of each str. Needed for unicode\\n        handling.\\n    justfunc : callable\\n        function used to justify str. Needed for unicode handling.',\n",
       " 'Perform ljust, center, rjust against string or list-like',\n",
       " 'internal. pprinter for iterables. you should probably use pprint_thing()\\n    rather then calling this directly.\\n\\n    bounds length of printed sequence, depending on options',\n",
       " 'internal. pprinter for iterables. you should probably use pprint_thing()\\n    rather then calling this directly.',\n",
       " 'Return a list of tuples of the (attr, formatted_value)\\n    for common attrs, including dtype, name, length\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        must be iterable\\n\\n    Returns\\n    -------\\n    list',\n",
       " 'Lag plot for time series.\\n\\n    Parameters\\n    ----------\\n    series : Time series\\n    lag : lag of the scatter plot, default 1\\n    ax : Matplotlib axis object, optional\\n    kwds : Matplotlib scatter method keyword arguments, optional\\n\\n    Returns\\n    -------\\n    class:`matplotlib.axis.Axes`',\n",
       " 'Autocorrelation plot for time series.\\n\\n    Parameters:\\n    -----------\\n    series: Time series\\n    ax: Matplotlib axis object, optional\\n    kwds : keywords\\n        Options to pass to matplotlib plotting method\\n\\n    Returns:\\n    -----------\\n    class:`matplotlib.axis.Axes`',\n",
       " 'Check a sequence of terms for instances of PandasObject.',\n",
       " 'Reconstruct an object given its type, raw value, and possibly empty\\n    (None) axes.\\n\\n    Parameters\\n    ----------\\n    typ : object\\n        A type\\n    obj : object\\n        The value to use in the type constructor\\n    axes : dict\\n        The axes to use to construct the resulting pandas object\\n\\n    Returns\\n    -------\\n    ret : typ\\n        An object of type ``typ`` with the value `obj` and possible axes\\n        `axes`.',\n",
       " 'Plots a Series on the given Matplotlib axes or the current axes\\n\\n    Parameters\\n    ----------\\n    axes : Axes\\n    series : Series\\n\\n    Notes\\n    _____\\n    Supports same kwargs as Axes.plot\\n\\n\\n    .. deprecated:: 0.23.0\\n       Use Series.plot() instead',\n",
       " 'Get the freq attribute of the ax object if set.\\n    Also checks shared axes (eg when using secondary yaxis, sharex=True\\n    or twinx)',\n",
       " 'Pretty-formats the date axis (x-axis).\\n\\n    Major and minor ticks are automatically set for the frequency of the\\n    current underlying series.  As the dynamic mode is activated by\\n    default, changing the limits of the x axis will intelligently change\\n    the positions of the ticks.',\n",
       " 'Return a unicode string representation for a particular DataFrame.',\n",
       " 'Return a html representation for a particular DataFrame.\\n\\n        Mainly for IPython notebook.',\n",
       " 'Matrix multiplication using binary `@` operator in Python>=3.5.',\n",
       " 'Write out the binary feather-format for DataFrames.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            string file path',\n",
       " 'Quickly retrieve single value at passed column and index.\\n\\n        .. deprecated:: 0.21.0\\n            Use .at[] or .iat[] accessors instead.\\n\\n        Parameters\\n        ----------\\n        index : row label\\n        col : column label\\n        takeable : interpret the index/col as indexers, default False\\n\\n        Returns\\n        -------\\n        scalar',\n",
       " 'Put single value at passed column and index.\\n\\n        .. deprecated:: 0.21.0\\n            Use .at[] or .iat[] accessors instead.\\n\\n        Parameters\\n        ----------\\n        index : row label\\n        col : column label\\n        value : scalar\\n        takeable : interpret the index/col as indexers, default False\\n\\n        Returns\\n        -------\\n        DataFrame\\n            If label pair is contained, will be reference to calling DataFrame,\\n            otherwise a new object.',\n",
       " 'Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n        axis : int\\n\\n        Notes\\n        -----\\n        If slice passed, the resulting data will be a view.',\n",
       " \"Ensure that if we don't have an index, that we can create one from the\\n        passed value.\",\n",
       " 'Add series to DataFrame in specified column.\\n\\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\\n        same length as the DataFrames index or an error will be thrown.\\n\\n        Series/TimeSeries will be conformed to the DataFrames index to\\n        ensure homogeneity.',\n",
       " 'Insert column into DataFrame at specified location.\\n\\n        Raises a ValueError if `column` is already contained in the DataFrame,\\n        unless `allow_duplicates` is set to True.\\n\\n        Parameters\\n        ----------\\n        loc : int\\n            Insertion index. Must verify 0 <= loc <= len(columns)\\n        column : string, number, or hashable object\\n            label of the inserted column\\n        value : int, Series, or array-like\\n        allow_duplicates : bool, optional',\n",
       " 'Swap levels i and j in a MultiIndex on a particular axis.\\n\\n        Parameters\\n        ----------\\n        i, j : int, string (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.',\n",
       " 'Rearrange index levels using input order. May not drop or\\n        duplicate levels.\\n\\n        Parameters\\n        ----------\\n        order : list of int or list of str\\n            List representing new level order. Reference level by number\\n            (position) or by key (label).\\n        axis : int\\n            Where to reorder levels.\\n\\n        Returns\\n        -------\\n        type of caller (new object)',\n",
       " 'Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'Infer and return an integer array of the values.\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : dtype, optional\\n        dtype to coerce\\n    copy : boolean, default False\\n\\n    Returns\\n    -------\\n    IntegerArray\\n\\n    Raises\\n    ------\\n    TypeError if incompatible types',\n",
       " 'Safely cast the values to the dtype if they\\n    are equivalent, meaning floats must be equivalent to the\\n    ints.',\n",
       " 'Coerce the input values array to numpy arrays with a mask\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : integer dtype\\n    mask : boolean 1D array, optional\\n    copy : boolean, default False\\n        if True, copy the input\\n\\n    Returns\\n    -------\\n    tuple of (values, mask)',\n",
       " 'Construction from a string, raise a TypeError if not\\n        possible',\n",
       " \"Returns a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don't include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\",\n",
       " 'Return values for sorting.\\n\\n        Returns\\n        -------\\n        ndarray\\n            The transformed values should maintain the ordering between values\\n            within the array.\\n\\n        See Also\\n        --------\\n        ExtensionArray.argsort',\n",
       " 'Parameters\\n        ----------\\n        result : array-like\\n        mask : array-like bool\\n        other : scalar or array-like\\n        op_name : str',\n",
       " 'return the length of a single non-tuple indexer which could be a slice',\n",
       " 'if we are index sliceable, then return my slicer, otherwise return None',\n",
       " 'reverse convert a missing indexer, which is a dict\\n    return the scalar indexer and a boolean indicating if we converted',\n",
       " \"create a filtered indexer that doesn't have any missing indexers\",\n",
       " \"Ensurse that a slice doesn't reduce to a Series or Scalar.\\n\\n    Any user-paseed `subset` should have this called on it\\n    to make sure we're always working with DataFrames.\",\n",
       " \"want nice defaults for background_gradient that don't break\\n    with non-numeric data. But if slice_ is passed go with that.\",\n",
       " 'validate that an positional indexer cannot enlarge its target\\n        will raise if needed, does not modify the indexer externally',\n",
       " 'Check whether there is the possibility to use ``_multi_take``.\\n        Currently the limit is that all axes being indexed must be indexed with\\n        list-likes.\\n\\n        Parameters\\n        ----------\\n        tup : tuple\\n            Tuple of indexers, one per axis\\n\\n        Returns\\n        -------\\n        boolean: Whether the current indexing can be passed through _multi_take',\n",
       " 'Create the indexers for the passed tuple of keys, and execute the take\\n        operation. This allows the take operation to be executed all at once -\\n        rather than once for each dimension - improving efficiency.\\n\\n        Parameters\\n        ----------\\n        tup : tuple\\n            Tuple of indexers, one per axis\\n\\n        Returns\\n        -------\\n        values: same type as the object being indexed',\n",
       " \"Convert indexing key into something we can use to do actual fancy\\n        indexing on an ndarray\\n\\n        Examples\\n        ix[:5] -> slice(0, 5)\\n        ix[[1,2,3]] -> [1,2,3]\\n        ix[['foo', 'bar', 'baz']] -> [i, j, k] (indices of foo, bar, baz)\\n\\n        Going by Zen of Python?\\n        'In the face of ambiguity, refuse the temptation to guess.'\\n        raise AmbiguousIndexError with integer labels?\\n        - No, prefer label-based indexing\",\n",
       " 'Transform a list of keys into a new array ready to be used as axis of\\n        the object we return (e.g. including NaNs).\\n\\n        Parameters\\n        ----------\\n        key : list-like\\n            Target labels\\n        axis: int\\n            Where the indexing is being made\\n\\n        Returns\\n        -------\\n        list-like of labels',\n",
       " 'this is pretty simple as we just have to deal with labels',\n",
       " 'Translate any partial string timestamp matches in key, returning the\\n        new key (GH 10331)',\n",
       " \"Check that 'key' is a valid position in the desired axis.\\n\\n        Parameters\\n        ----------\\n        key : int\\n            Requested position\\n        axis : int\\n            Desired axis\\n\\n        Returns\\n        -------\\n        None\\n\\n        Raises\\n        ------\\n        IndexError\\n            If 'key' is not a valid position in axis 'axis'\",\n",
       " 'Return Series values by list or array of integers\\n\\n        Parameters\\n        ----------\\n        key : list-like positional indexer\\n        axis : int (can only be zero)\\n\\n        Returns\\n        -------\\n        Series object',\n",
       " 'much simpler as we only have to deal with our valid types',\n",
       " \"require they keys to be the same type as the index (so we don't\\n        fallback)\",\n",
       " 'require integer args (and convert to label arguments)',\n",
       " 'create and return the block manager from a dataframe of series,\\n    columns, index',\n",
       " 'Conform a set of SparseSeries (with NaN fill_value) to a common SparseIndex\\n    corresponding to the locations where they all have data\\n\\n    Parameters\\n    ----------\\n    series_dict : dict or DataFrame\\n\\n    Notes\\n    -----\\n    Using the dumbest algorithm I could think of. Should put some more thought\\n    into this\\n\\n    Returns\\n    -------\\n    homogenized : dict of SparseSeries',\n",
       " 'Convert to dense DataFrame\\n\\n        Returns\\n        -------\\n        df : DataFrame',\n",
       " 'Get new SparseDataFrame applying func to each columns',\n",
       " 'Ratio of non-sparse points to total (dense) data points\\n        represented in the frame',\n",
       " 'Creates a new SparseArray from the input value.\\n\\n        Parameters\\n        ----------\\n        key : object\\n        value : scalar, Series, or array-like\\n        kwargs : dict\\n\\n        Returns\\n        -------\\n        sanitized_column : SparseArray',\n",
       " 'Returns a row (cross-section) from the SparseDataFrame as a Series\\n        object.\\n\\n        Parameters\\n        ----------\\n        key : some index contained in the index\\n\\n        Returns\\n        -------\\n        xs : Series',\n",
       " 'Returns a DataFrame with the rows/columns switched.',\n",
       " 'Return SparseDataFrame of cumulative sums over requested axis.\\n\\n        Parameters\\n        ----------\\n        axis : {0, 1}\\n            0 for row-wise, 1 for column-wise\\n\\n        Returns\\n        -------\\n        y : SparseDataFrame',\n",
       " 'Convert a conda package to its pip equivalent.\\n\\n    In most cases they are the same, those are the exceptions:\\n    - Packages that should be excluded (in `EXCLUDE`)\\n    - Packages that should be renamed (in `RENAME`)\\n    - A package requiring a specific version, in conda is defined with a single\\n      equal (e.g. ``pandas=1.0``) and in pip with two (e.g. ``pandas==1.0``)',\n",
       " 'try to do platform conversion, allow ndarray or list here',\n",
       " 'return a boolean if we have a nested object, e.g. a Series with 1 or\\n    more Series elements\\n\\n    This may not be necessarily be performant.',\n",
       " 'try to cast to the specified dtype (e.g. convert back to bool/int\\n    or could be an astype of float64->float32',\n",
       " 'interpret the dtype from a scalar or array. This is a convenience\\n    routines to infer dtype from a scalar or an array\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar/array belongs to pandas extension types is inferred as\\n        object',\n",
       " 'interpret the dtype from a scalar\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar belongs to pandas extension types is inferred as\\n        object',\n",
       " 'provide explicit type promotion and coercion\\n\\n    Parameters\\n    ----------\\n    values : the ndarray that we want to maybe upcast\\n    fill_value : what we want to fill with\\n    dtype : if None, then use the dtype of the values, else coerce to this type\\n    copy : if True always make a copy even if no upcast is required',\n",
       " 'Change string like dtypes to object for\\n    ``DataFrame.select_dtypes()``.',\n",
       " 'coerce the indexer input array to the smallest dtype possible',\n",
       " 'given a dtypes and a result set, coerce the result elements to the\\n    dtypes',\n",
       " \"Cast the elements of an array to a given dtype a nan-safe manner.\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    dtype : np.dtype\\n    copy : bool, default True\\n        If False, a view will be attempted but may fail, if\\n        e.g. the item sizes don't align.\\n    skipna: bool, default False\\n        Whether or not we should skip NaN when casting as a string-type.\\n\\n    Raises\\n    ------\\n    ValueError\\n        The dtype was a datetime64/timedelta64 dtype, but it had no unit.\",\n",
       " 'if we have an object dtype, try to coerce dates and/or numbers',\n",
       " 'if we have an object dtype, try to coerce dates and/or numbers',\n",
       " 'try to cast the array/value to a datetimelike dtype, converting float\\n    nan to iNaT',\n",
       " 'Find a common data type among the given dtypes.\\n\\n    Parameters\\n    ----------\\n    types : list of dtypes\\n\\n    Returns\\n    -------\\n    pandas extension or numpy dtype\\n\\n    See Also\\n    --------\\n    numpy.find_common_type',\n",
       " 'create np.ndarray of specified shape and dtype, filled with values\\n\\n    Parameters\\n    ----------\\n    shape : tuple\\n    value : scalar value\\n    dtype : np.dtype, optional\\n        dtype to coerce\\n\\n    Returns\\n    -------\\n    ndarray of shape, filled with value, of specified / inferred dtype',\n",
       " 'create a np.ndarray / pandas type of specified shape and dtype\\n    filled with values\\n\\n    Parameters\\n    ----------\\n    value : scalar value\\n    length : int\\n    dtype : pandas_dtype / np.dtype\\n\\n    Returns\\n    -------\\n    np.ndarray / pandas type of length, filled with value',\n",
       " 'Transform any list-like object in a 1-dimensional numpy array of object\\n    dtype.\\n\\n    Parameters\\n    ----------\\n    values : any iterable which has a len()\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `values` does not have a len()\\n\\n    Returns\\n    -------\\n    1-dimensional numpy array of dtype object',\n",
       " 'Make a scatter plot from two DataFrame columns\\n\\n    Parameters\\n    ----------\\n    data : DataFrame\\n    x : Column name for the x-axis values\\n    y : Column name for the y-axis values\\n    ax : Matplotlib axis object\\n    figsize : A tuple (width, height) in inches\\n    grid : Setting this to True will show the grid\\n    kwargs : other plotting keyword arguments\\n        To be passed to scatter function\\n\\n    Returns\\n    -------\\n    matplotlib.Figure',\n",
       " 'Grouped histogram\\n\\n    Parameters\\n    ----------\\n    data : Series/DataFrame\\n    column : object, optional\\n    by : object, optional\\n    ax : axes, optional\\n    bins : int, default 50\\n    figsize : tuple, optional\\n    layout : optional\\n    sharex : bool, default False\\n    sharey : bool, default False\\n    rot : int, default 90\\n    grid : bool, default True\\n    kwargs : dict, keyword arguments passed to matplotlib.Axes.hist\\n\\n    Returns\\n    -------\\n    collection of Matplotlib Axes',\n",
       " \"Tick creation within matplotlib is reasonably expensive and is\\n            internally deferred until accessed as Ticks are created/destroyed\\n            multiple times per draw. It's therefore beneficial for us to avoid\\n            accessing unless we will act on the Tick.\",\n",
       " 'Manage style and color based on column number and its label.\\n        Returns tuple of appropriate style and kwds which \"color\" may be added.',\n",
       " 'Return a list with distinct elements of \"objs\" (different ids).\\n    Preserves order.',\n",
       " 'Return the union or intersection of indexes.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index or list objects\\n        When intersect=True, do not accept list of lists.\\n    intersect : bool, default False\\n        If True, calculate the intersection between indexes. Otherwise,\\n        calculate the union.\\n    sort : bool, default False\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index',\n",
       " 'Return the union of indexes.\\n\\n    The behavior of sort and names is not consistent.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index or list objects\\n    sort : bool, default True\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index',\n",
       " \"Give a consensus 'names' to indexes.\\n\\n    If there's exactly one non-empty 'names', return this,\\n    otherwise, return empty.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index objects\\n\\n    Returns\\n    -------\\n    list\\n        A list representing the consensus 'names' found.\",\n",
       " 'Determine if all indexes contain the same elements.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index objects\\n\\n    Returns\\n    -------\\n    bool\\n        True if all indexes contain the same elements, False otherwise.',\n",
       " 'Convert SQL and params args to DBAPI2.0 compliant format.',\n",
       " 'Process parse_dates argument for read_sql functions',\n",
       " 'Force non-datetime columns to be read as such.\\n    Supports both string formatted and integer timestamp columns.',\n",
       " 'Returns a SQLAlchemy engine from a URI (if con is a string)\\n    else it just return con without modifying it.',\n",
       " 'Convenience function to return the correct PandasSQL subclass based on the\\n    provided parameters.',\n",
       " 'Execute SQL statement inserting data\\n\\n        Parameters\\n        ----------\\n        conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\\n        keys : list of str\\n           Column names\\n        data_iter : generator of list\\n           Each item contains a list of values to be inserted',\n",
       " 'Return a list of SQL statements that creates a table reflecting the\\n        structure of a DataFrame.  The first entry will be a CREATE TABLE\\n        statement while the rest will be CREATE INDEX statements.',\n",
       " 'Coerce to a categorical if a series is given.\\n\\n    Internal use ONLY.',\n",
       " 'utility routine to turn values into codes given the specified categories',\n",
       " \"Convert a set of codes for to a new set of categories\\n\\n    Parameters\\n    ----------\\n    codes : array\\n    old_categories, new_categories : Index\\n\\n    Returns\\n    -------\\n    new_codes : array\\n\\n    Examples\\n    --------\\n    >>> old_cat = pd.Index(['b', 'a', 'c'])\\n    >>> new_cat = pd.Index(['a', 'b'])\\n    >>> codes = np.array([0, 1, 1, 2])\\n    >>> _recode_for_categories(codes, old_cat, new_cat)\\n    array([ 1,  0,  0, -1])\",\n",
       " 'Factorize an input `values` into `categories` and `codes`. Preserves\\n    categorical dtype in `categories`.\\n\\n    *This is an internal function*\\n\\n    Parameters\\n    ----------\\n    values : list-like\\n\\n    Returns\\n    -------\\n    codes : ndarray\\n    categories : Index\\n        If `values` has a categorical dtype, then `categories` is\\n        a CategoricalIndex keeping the categories and order of `values`.',\n",
       " 'A higher-level wrapper over `_factorize_from_iterable`.\\n\\n    *This is an internal function*\\n\\n    Parameters\\n    ----------\\n    iterables : list-like of list-likes\\n\\n    Returns\\n    -------\\n    codes_list : list of ndarrays\\n    categories_list : list of Indexes\\n\\n    Notes\\n    -----\\n    See `_factorize_from_iterable` for more info.',\n",
       " 'Coerce this type to another dtype\\n\\n        Parameters\\n        ----------\\n        dtype : numpy dtype or pandas type\\n        copy : bool, default True\\n            By default, astype always returns a newly allocated object.\\n            If copy is set to False and dtype is categorical, the original\\n            object is returned.\\n\\n            .. versionadded:: 0.19.0',\n",
       " 'Get the codes.\\n\\n        Returns\\n        -------\\n        codes : integer array view\\n            A non writable view of the `codes` array.',\n",
       " \"Sets new categories inplace\\n\\n        Parameters\\n        ----------\\n        fastpath : bool, default False\\n           Don't perform validation of the categories for uniqueness or nulls\\n\\n        Examples\\n        --------\\n        >>> c = pd.Categorical(['a', 'b'])\\n        >>> c\\n        [a, b]\\n        Categories (2, object): [a, b]\\n\\n        >>> c._set_categories(pd.Index(['a', 'c']))\\n        >>> c\\n        [a, c]\\n        Categories (2, object): [a, c]\",\n",
       " \"Internal method for directly updating the CategoricalDtype\\n\\n        Parameters\\n        ----------\\n        dtype : CategoricalDtype\\n\\n        Notes\\n        -----\\n        We don't do any validation here. It's assumed that the dtype is\\n        a (valid) instance of `CategoricalDtype`.\",\n",
       " 'Set the ordered attribute to the boolean value.\\n\\n        Parameters\\n        ----------\\n        value : bool\\n           Set whether this categorical is ordered (True) or not (False).\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to the value.',\n",
       " 'Set the Categorical to be ordered.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to True.',\n",
       " 'Set the Categorical to be unordered.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to False.',\n",
       " 'Shift Categorical by desired number of periods.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to move, can be positive or negative\\n        fill_value : object, optional\\n            The scalar value to use for newly introduced missing values.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        shifted : Categorical',\n",
       " 'The numpy array interface.\\n\\n        Returns\\n        -------\\n        numpy.array\\n            A numpy array of either the specified dtype or,\\n            if dtype==None (default), the same dtype as\\n            categorical.categories.dtype.',\n",
       " 'Memory usage of my values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes',\n",
       " \"Return a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don't include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\",\n",
       " 'Return the values.\\n\\n        For internal compatibility with pandas formatting.\\n\\n        Returns\\n        -------\\n        numpy.array\\n            A numpy array of the same dtype as categorical.categories.dtype or\\n            Index if datetime / periods.',\n",
       " 'For correctly ranking ordered categorical data. See GH#15420\\n\\n        Ordered categorical data should be ranked on the basis of\\n        codes with -1 translated to NaN.\\n\\n        Returns\\n        -------\\n        numpy.array',\n",
       " 'Return a slice of myself.\\n\\n        For internal compatibility with numpy arrays.',\n",
       " 'a short repr displaying only max_vals and an optional (but default\\n        footer)',\n",
       " 'Item assignment.\\n\\n\\n        Raises\\n        ------\\n        ValueError\\n            If (one or more) Value is not in categories or if a assigned\\n            `Categorical` does not have the same categories',\n",
       " 'The minimum value of the object.\\n\\n        Only ordered `Categoricals` have a minimum!\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the `Categorical` is not `ordered`.\\n\\n        Returns\\n        -------\\n        min : the minimum of this `Categorical`',\n",
       " \"Returns the mode(s) of the Categorical.\\n\\n        Always returns `Categorical` even if only one value.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don't consider counts of NaN/NaT.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        modes : `Categorical` (sorted)\",\n",
       " 'Returns True if categorical arrays are equal.\\n\\n        Parameters\\n        ----------\\n        other : `Categorical`\\n\\n        Returns\\n        -------\\n        bool',\n",
       " 'Returns True if categoricals are the same dtype\\n          same categories, and same ordered\\n\\n        Parameters\\n        ----------\\n        other : Categorical\\n\\n        Returns\\n        -------\\n        bool',\n",
       " 'Describes this Categorical\\n\\n        Returns\\n        -------\\n        description: `DataFrame`\\n            A dataframe with frequency and counts by category.',\n",
       " 'Convert a list of objects to a timedelta index object.',\n",
       " 'Vectorized apply of DateOffset to DatetimeIndex,\\n        raises NotImplentedError for offsets without a\\n        vectorized implementation.\\n\\n        Parameters\\n        ----------\\n        i : DatetimeIndex\\n\\n        Returns\\n        -------\\n        y : DatetimeIndex',\n",
       " 'Roll provided date backward to next offset only if not on offset.',\n",
       " 'Roll provided date forward to next offset only if not on offset.',\n",
       " \"If n is positive, return tomorrow's business day opening time.\\n        Otherwise yesterday's business day's opening time.\\n\\n        Opening time always locates on BusinessDay.\\n        Otherwise, closing time may not if business hour extends over midnight.\",\n",
       " 'Roll provided date backward to next offset only if not on offset.',\n",
       " 'Roll provided date forward to next offset only if not on offset.',\n",
       " 'Define default roll function to be called in apply method.',\n",
       " 'Define default roll function to be called in apply method.',\n",
       " 'Add days portion of offset to DatetimeIndex i.\\n\\n        Parameters\\n        ----------\\n        i : DatetimeIndex\\n        roll : ndarray[int64_t]\\n\\n        Returns\\n        -------\\n        result : DatetimeIndex',\n",
       " 'Add self to the given DatetimeIndex, specialized for case where\\n        self.weekday is non-null.\\n\\n        Parameters\\n        ----------\\n        dtindex : DatetimeIndex\\n\\n        Returns\\n        -------\\n        result : DatetimeIndex',\n",
       " \"Find the day in the same month as other that has the same\\n        weekday as self.weekday and is the self.week'th such day in the month.\\n\\n        Parameters\\n        ----------\\n        other : datetime\\n\\n        Returns\\n        -------\\n        day : int\",\n",
       " 'Find the day in the same month as other that has the same\\n        weekday as self.weekday and is the last such day in the month.\\n\\n        Parameters\\n        ----------\\n        other: datetime\\n\\n        Returns\\n        -------\\n        day: int',\n",
       " 'Compute the vectorized membership of ``x in y`` if possible, otherwise\\n    use Python.',\n",
       " 'Compute the vectorized membership of ``x not in y`` if possible,\\n    otherwise use Python.',\n",
       " \"Cast an expression inplace.\\n\\n    Parameters\\n    ----------\\n    terms : Op\\n        The expression that should cast.\\n    acceptable_dtypes : list of acceptable numpy.dtype\\n        Will not cast if term's dtype in this list.\\n\\n        .. versionadded:: 0.19.0\\n\\n    dtype : str or numpy.dtype\\n        The dtype to cast to.\",\n",
       " \"search order for local (i.e., @variable) variables:\\n\\n        scope, key_variable\\n        [('locals', 'local_name'),\\n         ('globals', 'local_name'),\\n         ('locals', 'key'),\\n         ('globals', 'key')]\",\n",
       " 'Print a generic n-ary operator and its operands using infix\\n        notation',\n",
       " 'Recursively evaluate an expression in Python space.\\n\\n        Parameters\\n        ----------\\n        env : Scope\\n\\n        Returns\\n        -------\\n        object\\n            The result of an evaluated expression.',\n",
       " 'Evaluate a binary operation *before* being passed to the engine.\\n\\n        Parameters\\n        ----------\\n        env : Scope\\n        engine : str\\n        parser : str\\n        term_type : type\\n        eval_in_python : list\\n\\n        Returns\\n        -------\\n        term_type\\n            The \"pre-evaluated\" expression as an instance of ``term_type``',\n",
       " 'Convert datetimes to a comparable value in an expression.',\n",
       " 'Calculate appropriate figure size based on left and right data.',\n",
       " 'Plot left / right DataFrames in specified layout.\\n\\n        Parameters\\n        ----------\\n        left : list of DataFrames before operation is applied\\n        right : DataFrame of operation result\\n        labels : list of str to be drawn as titles of left DataFrames\\n        vertical : bool\\n            If True, use vertical layout. If False, use horizontal layout.',\n",
       " 'Convert each input to appropriate for table outplot',\n",
       " 'if the passed data is of datetime/timedelta type,\\n    this method converts it to numeric so that cut method can\\n    handle it',\n",
       " 'if the passed bin is of datetime/timedelta type,\\n    this method converts it to integer\\n\\n    Parameters\\n    ----------\\n    bins : list-like of bins\\n    dtype : dtype of data\\n\\n    Raises\\n    ------\\n    ValueError if bins are not of a compat dtype to dtype',\n",
       " 'Convert bins to a DatetimeIndex or TimedeltaIndex if the orginal dtype is\\n    datelike\\n\\n    Parameters\\n    ----------\\n    bins : list-like of bins\\n    dtype : dtype of data\\n\\n    Returns\\n    -------\\n    bins : Array-like of bins, DatetimeIndex or TimedeltaIndex if dtype is\\n           datelike',\n",
       " 'handles preprocessing for cut where we convert passed\\n    input to array, strip the index information and store it\\n    separately',\n",
       " 'handles post processing for the cut method where\\n    we combine the index information if the originally passed\\n    datatype was a series',\n",
       " 'Try to find the most capable encoding supported by the console.\\n    slightly modified from the way IPython handles the same issue.',\n",
       " \"Checks whether 'args' has length of at most 'compat_args'. Raises\\n    a TypeError if that is not the case, similar to in Python when a\\n    function is called with too many arguments.\",\n",
       " 'Check that the keys in `arg_val_dict` are mapped to their\\n    default values as specified in `compat_args`.\\n\\n    Note that this function is to be called only when it has been\\n    checked that arg_val_dict.keys() is a subset of compat_args',\n",
       " \"Checks whether 'kwargs' contains any keys that are not\\n    in 'compat_args' and raises a TypeError if there is one.\",\n",
       " 'Ensures that argument passed in arg_name is of type bool.',\n",
       " 'Potentially we might have a deprecation warning, show it\\n    but call the appropriate methods anyhow.',\n",
       " 'Return our appropriate resampler when grouping as well.',\n",
       " 'Utility frequency conversion method for Series/DataFrame.',\n",
       " 'Is the resampling from a DataFrame column or MultiIndex level.',\n",
       " 'Setup our binners.\\n\\n        Cache these as we are an immutable object',\n",
       " 'Create the BinGrouper, assume that self.set_grouper(obj)\\n        has already been called.',\n",
       " 'Call function producing a like-indexed Series on each group and return\\n        a Series with the transformed values.\\n\\n        Parameters\\n        ----------\\n        arg : function\\n            To apply to each group. Should return a Series with the same index.\\n\\n        Returns\\n        -------\\n        transformed : Series\\n\\n        Examples\\n        --------\\n        >>> resampled.transform(lambda x: (x - x.mean()) / x.std())',\n",
       " 'Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'If loffset is set, offset the result index.\\n\\n        This is NOT an idempotent routine, it will be applied\\n        exactly once to the result.\\n\\n        Parameters\\n        ----------\\n        result : Series or DataFrame\\n            the result of resample',\n",
       " 'Return the correct class for resampling with groupby.',\n",
       " 'Interpolate values according to different methods.\\n\\n        .. versionadded:: 0.18.1',\n",
       " 'Compute standard deviation of groups, excluding missing values.\\n\\n        Parameters\\n        ----------\\n        ddof : integer, default 1\\n            Degrees of freedom.',\n",
       " 'Compute variance of groups, excluding missing values.\\n\\n        Parameters\\n        ----------\\n        ddof : integer, default 1\\n            degrees of freedom',\n",
       " 'Dispatch to _upsample; we are stripping all of the _upsample kwargs and\\n        performing the original function call on the grouped object.',\n",
       " 'Downsample the cython defined function.\\n\\n        Parameters\\n        ----------\\n        how : string / cython mapped function\\n        **kwargs : kw args passed to how function',\n",
       " 'Adjust our binner when upsampling.\\n\\n        The range of a new index should not be outside specified range',\n",
       " \"Parameters\\n        ----------\\n        method : string {'backfill', 'bfill', 'pad',\\n            'ffill', 'asfreq'} method for upsampling\\n        limit : int, default None\\n            Maximum size gap to fill when reindexing\\n        fill_value : scalar, default None\\n            Value to use for missing values\\n\\n        See Also\\n        --------\\n        .fillna\",\n",
       " 'Downsample the cython defined function.\\n\\n        Parameters\\n        ----------\\n        how : string / cython mapped function\\n        **kwargs : kw args passed to how function',\n",
       " \"Parameters\\n        ----------\\n        method : string {'backfill', 'bfill', 'pad', 'ffill'}\\n            method for upsampling\\n        limit : int, default None\\n            Maximum size gap to fill when reindexing\\n        fill_value : scalar, default None\\n            Value to use for missing values\\n\\n        See Also\\n        --------\\n        .fillna\",\n",
       " \"Return my resampler or raise if we have an invalid axis.\\n\\n        Parameters\\n        ----------\\n        obj : input object\\n        kind : string, optional\\n            'period','timestamp','timedelta' are valid\\n\\n        Returns\\n        -------\\n        a Resampler\\n\\n        Raises\\n        ------\\n        TypeError if incompatible axis\",\n",
       " \"Parameters\\n    ----------\\n    arrays : generator\\n    num_items : int\\n\\n    Should be the same as CPython's tupleobject.c\",\n",
       " \"Hash an MultiIndex / list-of-tuples efficiently\\n\\n    .. versionadded:: 0.20.0\\n\\n    Parameters\\n    ----------\\n    vals : MultiIndex, list-of-tuples, or single tuple\\n    encoding : string, default 'utf8'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    ndarray of hashed values array\",\n",
       " \"Hash a single tuple efficiently\\n\\n    Parameters\\n    ----------\\n    val : single tuple\\n    encoding : string, default 'utf8'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    hash\",\n",
       " \"Hash a Categorical by hashing its categories, and then mapping the codes\\n    to the hashes\\n\\n    Parameters\\n    ----------\\n    c : Categorical\\n    encoding : string, default 'utf8'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    ndarray of hashed values array, same size as len(c)\",\n",
       " 'Hash scalar value\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash value, of length 1',\n",
       " 'Make sure the provided value for --single is a path to an existing\\n        .rst/.ipynb file, or a pandas object that can be imported.\\n\\n        For example, categorial.rst or pandas.DataFrame.head. For the latter,\\n        return the corresponding file path\\n        (e.g. reference/api/pandas.DataFrame.head.rst).',\n",
       " \"Execute a command as a OS terminal.\\n\\n        Parameters\\n        ----------\\n        *args : list of str\\n            Command and parameters to be executed\\n\\n        Examples\\n        --------\\n        >>> DocBuilder()._run_os('python', '--version')\",\n",
       " \"Call sphinx to build documentation.\\n\\n        Attribute `num_jobs` from the class is used.\\n\\n        Parameters\\n        ----------\\n        kind : {'html', 'latex'}\\n\\n        Examples\\n        --------\\n        >>> DocBuilder(num_jobs=4)._sphinx_build('html')\",\n",
       " 'Create in the build directory an html file with a redirect,\\n        for every row in REDIRECTS_FILE.',\n",
       " 'Render a DataFrame to a LaTeX tabular/longtable environment output.',\n",
       " 'r\"\"\"\\n        Combine columns belonging to a group to a single multicolumn entry\\n        according to self.multicolumn_format\\n\\n        e.g.:\\n        a &  &  & b & c &\\n        will become\\n        \\\\multicolumn{3}{l}{a} & b & \\\\multicolumn{2}{l}{c}',\n",
       " 'r\"\"\"\\n        Check following rows, whether row should be a multirow\\n\\n        e.g.:     becomes:\\n        a & 0 &   \\\\multirow{2}{*}{a} & 0 &\\n          & 1 &     & 1 &\\n        b & 0 &   \\\\cline{1-2}\\n                  b & 0 &',\n",
       " \"Checks whether the 'name' parameter for parsing is either\\n    an integer OR float that can SAFELY be cast to an integer\\n    without losing accuracy. Raises a ValueError if that is\\n    not the case.\\n\\n    Parameters\\n    ----------\\n    name : string\\n        Parameter name (used for error reporting)\\n    val : int or float\\n        The value to check\\n    min_val : int\\n        Minimum allowed value (val < min_val will result in a ValueError)\",\n",
       " 'Check if the `names` parameter contains duplicates.\\n\\n    If duplicates are found, we issue a warning before returning.\\n\\n    Parameters\\n    ----------\\n    names : array-like or None\\n        An array containing a list of the names used for the output DataFrame.\\n\\n    Returns\\n    -------\\n    names : array-like or None\\n        The original `names` parameter.',\n",
       " 'Check whether or not the `columns` parameter\\n    could be converted into a MultiIndex.\\n\\n    Parameters\\n    ----------\\n    columns : array-like\\n        Object which may or may not be convertible into a MultiIndex\\n\\n    Returns\\n    -------\\n    boolean : Whether or not columns could become a MultiIndex',\n",
       " \"Check whether or not the 'usecols' parameter\\n    is a callable.  If so, enumerates the 'names'\\n    parameter and returns a set of indices for\\n    each entry in 'names' that evaluates to True.\\n    If not a callable, returns 'usecols'.\",\n",
       " \"Check whether or not the 'parse_dates' parameter\\n    is a non-boolean scalar. Raises a ValueError if\\n    that is the case.\",\n",
       " 'extract and return the names, index_names, col_names\\n            header is a list-of-lists returned from the parsers',\n",
       " 'Infer types of values, possibly casting\\n\\n        Parameters\\n        ----------\\n        values : ndarray\\n        na_values : set\\n        try_num_bool : bool, default try\\n           try to cast values to numeric (first preference) or boolean\\n\\n        Returns:\\n        --------\\n        converted : ndarray\\n        na_count : int',\n",
       " 'Cast values to specified type\\n\\n        Parameters\\n        ----------\\n        values : ndarray\\n        cast_type : string or np.dtype\\n           dtype to cast values to\\n        column : string\\n            column name - used only for error reporting\\n\\n        Returns\\n        -------\\n        converted : ndarray',\n",
       " 'Set the columns that should not undergo dtype conversions.\\n\\n        Currently, any column that is involved with date parsing will not\\n        undergo such conversions.',\n",
       " 'Workhorse function for processing nested list into DataFrame\\n\\n        Should be replaced by np.genfromtxt eventually?',\n",
       " 'Sets self._col_indices\\n\\n        usecols_key is used if there are string usecols.',\n",
       " 'Checks whether the file begins with the BOM character.\\n        If it does, remove it. In addition, if there is quoting\\n        in the field subsequent to the BOM, remove it as well\\n        because it technically takes place at the beginning of\\n        the name, not the middle of it.',\n",
       " 'Alert a user about a malformed row.\\n\\n        If `self.error_bad_lines` is True, the alert will be `ParserError`.\\n        If `self.warn_bad_lines` is True, the alert will be printed out.\\n\\n        Parameters\\n        ----------\\n        msg : The error message to display.\\n        row_num : The row number where the parsing error occurred.\\n                  Because this row number is displayed, we 1-index,\\n                  even though we 0-index internally.',\n",
       " 'Wrapper around iterating through `self.data` (CSV source).\\n\\n        When a CSV error is raised, we check for specific\\n        error messages that allow us to customize the\\n        error message displayed to the user.\\n\\n        Parameters\\n        ----------\\n        row_num : The row number of the line being parsed.',\n",
       " 'Iterate through the lines and remove any that are\\n        either empty or contain only one whitespace value\\n\\n        Parameters\\n        ----------\\n        lines : array-like\\n            The array of lines that we are to filter.\\n\\n        Returns\\n        -------\\n        filtered_lines : array-like\\n            The same array of lines with the \"empty\" ones removed.',\n",
       " 'Try several cases to get lines:\\n\\n        0) There are headers on row 0 and row 1 and their\\n        total summed lengths equals the length of the next line.\\n        Treat row 0 as columns and row 1 as indices\\n        1) Look for implicit index: there are more columns\\n        on row 1 than row 0. If this is true, assume that row\\n        1 lists index columns and row 0 lists normal columns.\\n        2) Get index from the columns if it was listed.',\n",
       " 'For those classes for which we use ::\\n\\n    :template: autosummary/class_without_autosummary.rst\\n\\n    the documented attributes/methods have to be listed in the class\\n    docstring. However, if one of those lists is empty, we use \\'None\\',\\n    which then generates warnings in sphinx / ugly html output.\\n    This \"autodoc-process-docstring\" event connector removes that part\\n    from the processed docstring.',\n",
       " 'Pack object `o` and write it to `stream`\\n\\n    See :class:`Packer` for options.',\n",
       " 'Construct concatenation plan for given block manager and indexers.\\n\\n    Parameters\\n    ----------\\n    mgr : BlockManager\\n    indexers : dict of {axis: indexer}\\n\\n    Returns\\n    -------\\n    plan : list of (BlockPlacement, JoinUnit) tuples',\n",
       " 'Concatenate values from several join units along selected axis.',\n",
       " 'Return dtype and N/A values to use when concatenating specified units.\\n\\n    Returned N/A value may be None which means there was no casting involved.\\n\\n    Returns\\n    -------\\n    dtype\\n    na',\n",
       " 'Check if the join units consist of blocks of uniform type that can\\n    be concatenated using Block.concat_same_type instead of the generic\\n    concatenate_join_units (which uses `_concat._concat_compat`).',\n",
       " \"Reduce join_unit's shape along item axis to length.\\n\\n    Extra items that didn't fit are returned as a separate block.\",\n",
       " 'Combine multiple concatenation plans into one.\\n\\n    existing_plan is updated in-place.',\n",
       " 'Temporarily set a parameter value using the with statement.\\n        Aliasing allowed.',\n",
       " 'Convert from datetime to SIF. http://www.stata.com/help.cgi?datetime\\n\\n    Parameters\\n    ----------\\n    dates : Series\\n        Series or array containing datetime.datetime or datetime64[ns] to\\n        convert to the Stata Internal Format given by fmt\\n    fmt : str\\n        The format to convert to. Can be, tc, td, tw, tm, tq, th, ty',\n",
       " 'Convert dtype types to stata types. Returns the byte of the given ordinal.\\n    See TYPE_MAP and comments for an explanation. This is also explained in\\n    the dta spec.\\n    1 - 244 are strings of this length\\n                         Pandas    Stata\\n    251 - for int8      byte\\n    252 - for int16     int\\n    253 - for int32     long\\n    254 - for float32   float\\n    255 - for double    double\\n\\n    If there are dates to convert, then dtype will already have the correct\\n    type inserted.',\n",
       " 'Map numpy dtype to stata\\'s default format for this type. Not terribly\\n    important since users can change this in Stata. Semantics are\\n\\n    object  -> \"%DDs\" where DD is the length of the string.  If not a string,\\n                raise ValueError\\n    float64 -> \"%10.0g\"\\n    float32 -> \"%9.0g\"\\n    int64   -> \"%9.0g\"\\n    int32   -> \"%12.0g\"\\n    int16   -> \"%8.0g\"\\n    int8    -> \"%8.0g\"\\n    strl    -> \"%9s\"',\n",
       " \"Takes a bytes instance and pads it with null bytes until it's length chars.\",\n",
       " 'Parameters\\n        ----------\\n        byteorder : str\\n            Byte order of the output\\n        encoding : str\\n            File encoding\\n\\n        Returns\\n        -------\\n        value_label : bytes\\n            Bytes containing the formatted value label',\n",
       " 'Helper to call encode before writing to file for Python 3 compat.',\n",
       " 'Check for categorical columns, retain categorical information for\\n        Stata file and convert categorical data to int',\n",
       " 'Checks floating point data columns for nans, and replaces these with\\n        the generic Stata for missing value (.)',\n",
       " 'Checks column names to ensure that they are valid Stata column names.\\n        This includes checks for:\\n            * Non-string names\\n            * Stata keywords\\n            * Variables that start with numbers\\n            * Variables with names that are too long\\n\\n        When an illegal variable name is detected, it is converted, and if\\n        dates are exported, the variable name is propagated to the date\\n        conversion dictionary',\n",
       " 'Close the file if it was created by the writer.\\n\\n        If a buffer or file-like object was passed in, for example a GzipFile,\\n        then leave this file open for the caller to close. In either case,\\n        attempt to flush the file contents to ensure they are written to disk\\n        (if supported)',\n",
       " 'Generates the binary blob of GSOs that is written to the dta file.\\n\\n        Parameters\\n        ----------\\n        gso_table : OrderedDict\\n            Ordered dictionary (str, vo)\\n\\n        Returns\\n        -------\\n        gso : bytes\\n            Binary content of dta file to be placed between strl tags\\n\\n        Notes\\n        -----\\n        Output format depends on dta version.  117 uses two uint32s to\\n        express v and o while 118+ uses a uint32 for v and a uint64 for o.',\n",
       " 'Called twice during file write. The first populates the values in\\n        the map with 0s.  The second call writes the final map locations when\\n        all blocks have been written.',\n",
       " 'Update column names for conversion to strl if they might have been\\n        changed to comply with Stata naming rules',\n",
       " 'Convert columns to StrLs if either very large or in the\\n        convert_strl variable',\n",
       " 'Register Pandas Formatters and Converters with matplotlib\\n\\n    This function modifies the global ``matplotlib.units.registry``\\n    dictionary. Pandas adds custom converters for\\n\\n    * pd.Timestamp\\n    * pd.Period\\n    * np.datetime64\\n    * datetime.datetime\\n    * datetime.date\\n    * datetime.time\\n\\n    See Also\\n    --------\\n    deregister_matplotlib_converter',\n",
       " \"Remove pandas' formatters and converters\\n\\n    Removes the custom converters added by :func:`register`. This\\n    attempts to set the state of the registry back to the state before\\n    pandas registered its own units. Converters for pandas' own types like\\n    Timestamp and Period are removed completely. Converters for types\\n    pandas overwrites, like ``datetime.datetime``, are restored to their\\n    original value.\\n\\n    See Also\\n    --------\\n    deregister_matplotlib_converters\",\n",
       " 'Convert :mod:`datetime` to the Gregorian date as UTC float days,\\n    preserving hours, minutes, seconds and microseconds.  Return value\\n    is a :func:`float`.',\n",
       " 'Returns a default spacing between consecutive ticks for annual data.',\n",
       " 'Returns the indices where the given period changes.\\n\\n    Parameters\\n    ----------\\n    dates : PeriodIndex\\n        Array of intervals to monitor.\\n    period : string\\n        Name of the period to monitor.',\n",
       " \"Returns true if the ``label_flags`` indicate there is at least one label\\n    for this level.\\n\\n    if the minimum view limit is not an exact integer, then the first tick\\n    label won't be shown, so we must adjust for that.\",\n",
       " 'Return the time of day as a formatted string.\\n\\n        Parameters\\n        ----------\\n        x : float\\n            The time of day specified as seconds since 00:00 (midnight),\\n            with up to microsecond precision.\\n        pos\\n            Unused\\n\\n        Returns\\n        -------\\n        str\\n            A string in HH:MM:SS.mmmuuu format. Microseconds,\\n            milliseconds and seconds are only displayed if non-zero.',\n",
       " 'Return the :class:`~matplotlib.units.AxisInfo` for *unit*.\\n\\n        *unit* is a tzinfo instance or None.\\n        The *axis* argument is required but not used.',\n",
       " 'Sets the view limits to the nearest multiples of base that contain the\\n        data.',\n",
       " \"Sets index names to 'index' for regular, or 'level_x' for Multi\",\n",
       " 'Find the appropriate name to pin to an operation result.  This result\\n    should always be either an Index or a Series.\\n\\n    Parameters\\n    ----------\\n    left : {Series, Index}\\n    right : object\\n\\n    Returns\\n    -------\\n    name : object\\n        Usually a string',\n",
       " 'Try to find a name to attach to the result of an operation between\\n    a and b.  If only one of these has a `name` attribute, return that\\n    name.  Otherwise return a consensus name if they match of None if\\n    they have different names.\\n\\n    Parameters\\n    ----------\\n    a : object\\n    b : object\\n\\n    Returns\\n    -------\\n    name : str or None\\n\\n    See Also\\n    --------\\n    pandas.core.common.consensus_name_attr',\n",
       " 'Cast non-pandas objects to pandas types to unify behavior of arithmetic\\n    and comparison operations.\\n\\n    Parameters\\n    ----------\\n    obj: object\\n\\n    Returns\\n    -------\\n    out : object\\n\\n    Notes\\n    -----\\n    Be careful to call this *after* determining the `name` attribute to be\\n    attached to the result of the arithmetic operation.',\n",
       " 'Return a binary method that always raises a TypeError.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    invalid_op : function',\n",
       " 'Find the keyword arguments to pass to numexpr for the given operation.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    eval_kwargs : dict\\n\\n    Examples\\n    --------\\n    >>> _gen_eval_kwargs(\"__add__\")\\n    {}\\n\\n    >>> _gen_eval_kwargs(\"rtruediv\")\\n    {\\'reversed\\': True, \\'truediv\\': True}',\n",
       " 'Find the appropriate fill value to use when filling in undefined values\\n    in the results of the given operation caused by operating on\\n    (generally dividing by) zero.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    fill_value : {None, np.nan, np.inf}',\n",
       " 'Find the operation string, if any, to pass to numexpr for this\\n    operation.\\n\\n    Parameters\\n    ----------\\n    op : binary operator\\n    cls : class\\n\\n    Returns\\n    -------\\n    op_str : string or None',\n",
       " 'Find the name to attach to this method according to conventions\\n    for special and non-special methods.\\n\\n    Parameters\\n    ----------\\n    op : binary operator\\n    special : bool\\n\\n    Returns\\n    -------\\n    op_name : str',\n",
       " \"Make the appropriate substitutions for the given operation and class-typ\\n    into either _flex_doc_SERIES or _flex_doc_FRAME to return the docstring\\n    to attach to a generated method.\\n\\n    Parameters\\n    ----------\\n    op_name : str {'__add__', '__sub__', ... '__eq__', '__ne__', ...}\\n    typ : str {series, 'dataframe']}\\n\\n    Returns\\n    -------\\n    doc : str\",\n",
       " 'If a non-None fill_value is given, replace null entries in left and right\\n    with this value, but only in positions where _one_ of left/right is null,\\n    not both.\\n\\n    Parameters\\n    ----------\\n    left : array-like\\n    right : array-like\\n    fill_value : object\\n\\n    Returns\\n    -------\\n    left : array-like\\n    right : array-like\\n\\n    Notes\\n    -----\\n    Makes copies if fill_value is not None',\n",
       " 'Apply the function `op` to only non-null points in x and y.\\n\\n    Parameters\\n    ----------\\n    x : array-like\\n    y : array-like\\n    op : binary operation\\n    allowed_types : class or tuple of classes\\n\\n    Returns\\n    -------\\n    result : ndarray[bool]',\n",
       " 'If the given arithmetic operation fails, attempt it again on\\n    only the non-null elements of the input array(s).\\n\\n    Parameters\\n    ----------\\n    x : np.ndarray\\n    y : np.ndarray, Series, Index\\n    op : binary operator',\n",
       " 'If a comparison has mismatched types and is not necessarily meaningful,\\n    follow python3 conventions by:\\n\\n        - returning all-False for equality\\n        - returning all-True for inequality\\n        - raising TypeError otherwise\\n\\n    Parameters\\n    ----------\\n    left : array-like\\n    right : scalar, array-like\\n    op : operator.{eq, ne, lt, le, gt}\\n\\n    Raises\\n    ------\\n    TypeError : on inequality comparisons',\n",
       " 'Identify cases where a DataFrame operation should dispatch to its\\n    Series counterpart.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : DataFrame\\n    op : binary operator\\n\\n    Returns\\n    -------\\n    override : bool',\n",
       " 'Evaluate the frame operation func(left, right) by evaluating\\n    column-by-column, dispatching to the Series implementation.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : scalar or DataFrame\\n    func : arithmetic or comparison operator\\n    str_rep : str or None, default None\\n    axis : {None, 0, 1, \"index\", \"columns\"}\\n\\n    Returns\\n    -------\\n    DataFrame',\n",
       " 'Wrap Series left in the given index_class to delegate the operation op\\n    to the index implementation.  DatetimeIndex and TimedeltaIndex perform\\n    type checking, timezone handling, overflow checks, etc.\\n\\n    Parameters\\n    ----------\\n    op : binary operator (operator.add, operator.sub, ...)\\n    left : Series\\n    right : object\\n    index_class : DatetimeIndex or TimedeltaIndex\\n\\n    Returns\\n    -------\\n    result : object, usually DatetimeIndex, TimedeltaIndex, or Series',\n",
       " 'Assume that left or right is a Series backed by an ExtensionArray,\\n    apply the operator defined by op.',\n",
       " 'Find the appropriate operation-wrappers to use when defining flex/special\\n    arithmetic, boolean, and comparison operations with the given class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n\\n    Returns\\n    -------\\n    arith_flex : function or None\\n    comp_flex : function or None\\n    arith_special : function\\n    comp_special : function\\n    bool_special : function\\n\\n    Notes\\n    -----\\n    None is only returned for SparseArray',\n",
       " 'Adds the full suite of special arithmetic methods (``__add__``,\\n    ``__sub__``, etc.) to the class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n        special methods will be defined and pinned to this class',\n",
       " 'Adds the full suite of flex arithmetic methods (``pow``, ``mul``, ``add``)\\n    to the class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n        flex methods will be defined and pinned to this class',\n",
       " 'If the raw op result has a non-None name (e.g. it is an Index object) and\\n    the name argument is None, then passing name to the constructor will\\n    not be enough; we still need to override the name attribute.',\n",
       " 'divmod returns a tuple of like indexed series instead of a single series.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " \"Apply binary operator `func` to self, other using alignment and fill\\n    conventions determined by the fill_value, axis, and level kwargs.\\n\\n    Parameters\\n    ----------\\n    self : DataFrame\\n    other : Series\\n    func : binary operator\\n    fill_value : object, default None\\n    axis : {0, 1, 'columns', 'index', None}, default None\\n    level : int or None, default None\\n\\n    Returns\\n    -------\\n    result : DataFrame\",\n",
       " 'convert rhs to meet lhs dims if input is list, tuple or np.ndarray',\n",
       " 'For SparseSeries operation, coerce to float64 if the result is expected\\n    to have NaN or inf values\\n\\n    Parameters\\n    ----------\\n    left : SparseArray\\n    right : SparseArray\\n    opname : str\\n\\n    Returns\\n    -------\\n    left : SparseArray\\n    right : SparseArray',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'If a `periods` argument is passed to the Datetime/Timedelta Array/Index\\n    constructor, cast it to an integer.\\n\\n    Parameters\\n    ----------\\n    periods : None, float, int\\n\\n    Returns\\n    -------\\n    periods : None or int\\n\\n    Raises\\n    ------\\n    TypeError\\n        if periods is None, float, or int',\n",
       " 'Check that the `closed` argument is among [None, \"left\", \"right\"]\\n\\n    Parameters\\n    ----------\\n    closed : {None, \"left\", \"right\"}\\n\\n    Returns\\n    -------\\n    left_closed : bool\\n    right_closed : bool\\n\\n    Raises\\n    ------\\n    ValueError : if argument is not among valid values',\n",
       " 'If the user passes a freq and another freq is inferred from passed data,\\n    require that they match.\\n\\n    Parameters\\n    ----------\\n    freq : DateOffset or None\\n    inferred_freq : DateOffset or None\\n    freq_infer : bool\\n\\n    Returns\\n    -------\\n    freq : DateOffset or None\\n    freq_infer : bool\\n\\n    Notes\\n    -----\\n    We assume at this point that `maybe_infer_freq` has been called, so\\n    `freq` is either a DateOffset object or None.',\n",
       " 'Comparing a DateOffset to the string \"infer\" raises, so we need to\\n    be careful about comparisons.  Make a dummy variable `freq_infer` to\\n    signify the case where the given freq is \"infer\" and set freq to None\\n    to avoid comparison trouble later on.\\n\\n    Parameters\\n    ----------\\n    freq : {DateOffset, None, str}\\n\\n    Returns\\n    -------\\n    freq : {DateOffset, None}\\n    freq_infer : bool',\n",
       " 'Helper for coercing an input scalar or array to i8.\\n\\n    Parameters\\n    ----------\\n    other : 1d array\\n    to_utc : bool, default False\\n        If True, convert the values to UTC before extracting the i8 values\\n        If False, extract the i8 values directly.\\n\\n    Returns\\n    -------\\n    i8 1d array',\n",
       " 'Construct a scalar type from a string.\\n\\n        Parameters\\n        ----------\\n        value : str\\n\\n        Returns\\n        -------\\n        Period, Timestamp, or Timedelta, or NaT\\n            Whatever the type of ``self._scalar_type`` is.\\n\\n        Notes\\n        -----\\n        This should call ``self._check_compatible_with`` before\\n        unboxing the result.',\n",
       " \"Unbox the integer value of a scalar `value`.\\n\\n        Parameters\\n        ----------\\n        value : Union[Period, Timestamp, Timedelta]\\n\\n        Returns\\n        -------\\n        int\\n\\n        Examples\\n        --------\\n        >>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP\\n        10000000000\",\n",
       " 'Verify that `self` and `other` are compatible.\\n\\n        * DatetimeArray verifies that the timezones (if any) match\\n        * PeriodArray verifies that the freq matches\\n        * Timedelta has no verification\\n\\n        In each case, NaT is considered compatible.\\n\\n        Parameters\\n        ----------\\n        other\\n\\n        Raises\\n        ------\\n        Exception',\n",
       " 'This getitem defers to the underlying array, which by-definition can\\n        only handle list-likes, slices, and integer scalars',\n",
       " 'Repeat elements of an array.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.repeat',\n",
       " \"Return a Series containing counts of unique values.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don't include counts of NaT values.\\n\\n        Returns\\n        -------\\n        Series\",\n",
       " 'Parameters\\n        ----------\\n        result : a ndarray\\n        fill_value : object, default iNaT\\n        convert : string/dtype or None\\n\\n        Returns\\n        -------\\n        result : ndarray with values replace by the fill_value\\n\\n        mask the result if needed, convert to the provided dtype if its not\\n        None\\n\\n        This is an internal routine.',\n",
       " 'Validate that a frequency is compatible with the values of a given\\n        Datetime Array/Index or Timedelta Array/Index\\n\\n        Parameters\\n        ----------\\n        index : DatetimeIndex or TimedeltaIndex\\n            The index on which to determine if the given frequency is valid\\n        freq : DateOffset\\n            The frequency to validate',\n",
       " \"Add a timedelta-like, Tick or TimedeltaIndex-like object\\n        to self, yielding an int64 numpy array\\n\\n        Parameters\\n        ----------\\n        delta : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]\\n\\n        Notes\\n        -----\\n        The result's name is set outside of _add_delta by the calling\\n        method (__add__ or __sub__), if necessary (i.e. for Indexes).\",\n",
       " 'Add a delta of a timedeltalike\\n        return the i8 result view',\n",
       " 'Add a delta of a TimedeltaIndex\\n        return the i8 result view',\n",
       " 'Subtract a Period Array/Index from self.  This is only valid if self\\n        is itself a Period Array/Index, raises otherwise.  Both objects must\\n        have the same frequency.\\n\\n        Parameters\\n        ----------\\n        other : PeriodIndex or PeriodArray\\n\\n        Returns\\n        -------\\n        result : np.ndarray[object]\\n            Array of DateOffset objects; nulls represented by NaT.',\n",
       " 'Add or subtract array-like of integers equivalent to applying\\n        `_time_shift` pointwise.\\n\\n        Parameters\\n        ----------\\n        other : Index, ExtensionArray, np.ndarray\\n            integer-dtype\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self',\n",
       " 'Add or subtract array-like of DateOffset objects\\n\\n        Parameters\\n        ----------\\n        other : Index, np.ndarray\\n            object-dtype containing pd.DateOffset objects\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self',\n",
       " 'Shift each value by `periods`.\\n\\n        Note this is different from ExtensionArray.shift, which\\n        shifts the *position* of each element, padding the end with\\n        missing values.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to shift by.\\n        freq : pandas.DateOffset, pandas.Timedelta, or string\\n            Frequency increment to shift by.',\n",
       " 'Return the minimum value of the Array or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Index.min : Return the minimum value in an Index.\\n        Series.min : Return the minimum value in a Series.',\n",
       " 'Return the maximum value of the Array or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Index.max : Return the maximum value in an Index.\\n        Series.max : Return the maximum value in a Series.',\n",
       " 'Wrap comparison operations to convert Period-like to PeriodDtype',\n",
       " 'Helper function to render a consistent error message when raising\\n    IncompatibleFrequency.\\n\\n    Parameters\\n    ----------\\n    left : PeriodArray\\n    right : DateOffset, Period, ndarray, or timedelta-like\\n\\n    Raises\\n    ------\\n    IncompatibleFrequency',\n",
       " 'If both a dtype and a freq are available, ensure they match.  If only\\n    dtype is available, extract the implied freq.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype\\n    freq : DateOffset or None\\n\\n    Returns\\n    -------\\n    freq : DateOffset\\n\\n    Raises\\n    ------\\n    ValueError : non-period dtype\\n    IncompatibleFrequency : mismatch between dtype and freq',\n",
       " \"Convert an datetime-like array to values Period ordinals.\\n\\n    Parameters\\n    ----------\\n    data : Union[Series[datetime64[ns]], DatetimeIndex, ndarray[datetime64ns]]\\n    freq : Optional[Union[str, Tick]]\\n        Must match the `freq` on the `data` if `data` is a DatetimeIndex\\n        or Series.\\n    tz : Optional[tzinfo]\\n\\n    Returns\\n    -------\\n    ordinals : ndarray[int]\\n    freq : Tick\\n        The frequencey extracted from the Series or DatetimeIndex if that's\\n        used.\",\n",
       " 'Construct a PeriodArray from a datetime64 array\\n\\n        Parameters\\n        ----------\\n        data : ndarray[datetime64[ns], datetime64[ns, tz]]\\n        freq : str or Tick\\n        tz : tzinfo, optional\\n\\n        Returns\\n        -------\\n        PeriodArray[freq]',\n",
       " \"Cast to DatetimeArray/Index.\\n\\n        Parameters\\n        ----------\\n        freq : string or DateOffset, optional\\n            Target frequency. The default is 'D' for week or longer,\\n            'S' otherwise\\n        how : {'s', 'e', 'start', 'end'}\\n\\n        Returns\\n        -------\\n        DatetimeArray/Index\",\n",
       " 'Shift each value by `periods`.\\n\\n        Note this is different from ExtensionArray.shift, which\\n        shifts the *position* of each element, padding the end with\\n        missing values.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to shift by.\\n        freq : pandas.DateOffset, pandas.Timedelta, or string\\n            Frequency increment to shift by.',\n",
       " 'Parameters\\n        ----------\\n        other : timedelta, Tick, np.timedelta64\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]',\n",
       " 'Parameters\\n        ----------\\n        other : TimedeltaArray or ndarray[timedelta64]\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]',\n",
       " 'Add a timedelta-like, Tick, or TimedeltaIndex-like object\\n        to self, yielding a new PeriodArray\\n\\n        Parameters\\n        ----------\\n        other : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : PeriodArray',\n",
       " 'Detect missing values. Treat None, NaN, INF, -INF as null.\\n\\n    Parameters\\n    ----------\\n    arr: ndarray or object value\\n\\n    Returns\\n    -------\\n    boolean ndarray or boolean',\n",
       " 'Parameters\\n    ----------\\n    arr: a numpy array\\n    fill_value: fill value, default to np.nan\\n\\n    Returns\\n    -------\\n    True if we can fill using this fill_value',\n",
       " 'infer the fill value for the nan/NaT from the provided\\n    scalar/ndarray/list-like if we are a NaT, return the correct dtyped\\n    element to provide proper block construction',\n",
       " 'if we have a compatible fill_value and arr dtype, then fill',\n",
       " 'Return array-like containing only true/non-NaN values, possibly empty.',\n",
       " 'Helper function to convert DataFrame and Series to matplotlib.table\\n\\n    Parameters\\n    ----------\\n    ax : Matplotlib axes object\\n    data : DataFrame or Series\\n        data for table contents\\n    kwargs : keywords, optional\\n        keyword arguments which passed to matplotlib.table.table.\\n        If `rowLabels` or `colLabels` is not specified, data index or column\\n        name will be used.\\n\\n    Returns\\n    -------\\n    matplotlib table object',\n",
       " 'fast version of transform, only applicable to\\n        builtin/cythonizable functions',\n",
       " 'Calcuate pct_change of each value to previous entry in group',\n",
       " 'sub-classes to define\\n        return a sliced object\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'If we have categorical groupers, then we want to make sure that\\n        we have a fully reindex-output to the levels. These may have not\\n        participated in the groupings (e.g. may have all been\\n        nan groups);\\n\\n        This can re-expand the output space',\n",
       " 'Overridden method to join grouped columns in output',\n",
       " \"Flatten an arbitrarily nested sequence.\\n\\n    Parameters\\n    ----------\\n    l : sequence\\n        The non string sequence to flatten\\n\\n    Notes\\n    -----\\n    This doesn't consider strings sequences.\\n\\n    Returns\\n    -------\\n    flattened : generator\",\n",
       " 'To avoid numpy DeprecationWarnings, cast float to integer where valid.\\n\\n    Parameters\\n    ----------\\n    val : scalar\\n\\n    Returns\\n    -------\\n    outval : scalar',\n",
       " 'Transform label or iterable of labels to array, for use in Index.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype\\n        If specified, use as dtype of the resulting array, otherwise infer.\\n\\n    Returns\\n    -------\\n    array',\n",
       " 'Evaluate possibly callable input using obj and kwargs if it is callable,\\n    otherwise return as it is.\\n\\n    Parameters\\n    ----------\\n    maybe_callable : possibly a callable\\n    obj : NDFrame\\n    **kwargs',\n",
       " 'Helper function for processing random_state arguments.\\n\\n    Parameters\\n    ----------\\n    state : int, np.random.RandomState, None.\\n        If receives an int, passes to np.random.RandomState() as seed.\\n        If receives an np.random.RandomState object, just returns object.\\n        If receives `None`, returns np.random.\\n        If receives anything else, raises an informative ValueError.\\n        Default None.\\n\\n    Returns\\n    -------\\n    np.random.RandomState',\n",
       " 'Returns a function that will map names/labels, dependent if mapper\\n    is a dict, Series or just a function.',\n",
       " 'return the correct fill value for the dtype of the values',\n",
       " 'utility to get the values view, mask, dtype\\n    if necessary copy and mask using the specified fill_value\\n    copy = True will force the copy',\n",
       " 'Return the missing value for `values`\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis : int or None\\n        axis for the reduction\\n\\n    Returns\\n    -------\\n    result : scalar or ndarray\\n        For 1-D values, returns a scalar of the correct missing type.\\n        For 2-D values, returns a 1-D array where each element is missing.',\n",
       " 'Sum the elements along an axis ignoring NaNs\\n\\n    Parameters\\n    ----------\\n    values : ndarray[dtype]\\n    axis: int, optional\\n    skipna : bool, default True\\n    min_count: int, default 0\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : dtype\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, np.nan])\\n    >>> nanops.nansum(s)\\n    3.0',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 2])\\n    >>> nanops.nanmedian(s)\\n    2.0',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    --------\\n    result : int\\n        The index of max value in specified axis or -1 in the NA case\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\\n    >>> nanops.nanargmax(s)\\n    4',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    --------\\n    result : int\\n        The index of min value in specified axis or -1 in the NA case\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\\n    >>> nanops.nanargmin(s)\\n    0',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray[dtype]\\n    axis: int, optional\\n    skipna : bool, default True\\n    min_count: int, default 0\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : dtype\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan])\\n    >>> nanops.nanprod(s)\\n    6.0\\n\\n    Returns\\n    --------\\n    The product of all elements on a given axis. ( NaNs are treated as 1)',\n",
       " 'Wraper for np.percentile that skips missing values, specialized to\\n    1-dimensional case.\\n\\n    Parameters\\n    ----------\\n    values : array over which to find quantiles\\n    mask : ndarray[bool]\\n        locations in values that should be considered missing\\n    q : scalar or array of quantile indices to find\\n    na_value : scalar\\n        value to return for empty or all-null values\\n    interpolation : str\\n\\n    Returns\\n    -------\\n    quantiles : scalar or array',\n",
       " 'Wraper for np.percentile that skips missing values.\\n\\n    Parameters\\n    ----------\\n    values : array over which to find quantiles\\n    q : scalar or array of quantile indices to find\\n    axis : {0, 1}\\n    na_value : scalar\\n        value to return for empty or all-null values\\n    mask : ndarray[bool]\\n        locations in values that should be considered missing\\n    ndim : {1, 2}\\n    interpolation : str\\n\\n    Returns\\n    -------\\n    quantiles : scalar or array',\n",
       " 'r\"\"\"\\n    Read text from clipboard and pass to read_csv. See read_csv for the\\n    full argument list\\n\\n    Parameters\\n    ----------\\n    sep : str, default \\'\\\\s+\\'\\n        A string or regex delimiter. The default of \\'\\\\s+\\' denotes\\n        one or more whitespace characters.\\n\\n    Returns\\n    -------\\n    parsed : DataFrame',\n",
       " 'Get an iterator given an integer, slice or container.\\n\\n    Parameters\\n    ----------\\n    skiprows : int, slice, container\\n        The iterator to use to skip rows; can also be a slice.\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `skiprows` is not a slice, integer, or Container\\n\\n    Returns\\n    -------\\n    it : iterable\\n        A proper iterator to use to skip rows of a DataFrame.',\n",
       " 'Try to read from a url, file or string.\\n\\n    Parameters\\n    ----------\\n    obj : str, unicode, or file-like\\n\\n    Returns\\n    -------\\n    raw_text : str',\n",
       " \"Build an xpath expression to simulate bs4's ability to pass in kwargs to\\n    search for attributes when using the lxml parser.\\n\\n    Parameters\\n    ----------\\n    attrs : dict\\n        A dict of HTML attributes. These are NOT checked for validity.\\n\\n    Returns\\n    -------\\n    expr : unicode\\n        An XPath expression that checks for the given HTML attributes.\",\n",
       " 'Choose the parser based on the input flavor.\\n\\n    Parameters\\n    ----------\\n    flavor : str\\n        The type of parser to use. This must be a valid backend.\\n\\n    Returns\\n    -------\\n    cls : _HtmlFrameParser subclass\\n        The parser class based on the requested input flavor.\\n\\n    Raises\\n    ------\\n    ValueError\\n        * If `flavor` is not a valid backend.\\n    ImportError\\n        * If you do not have the requested `flavor`',\n",
       " 'Parse and return all tables from the DOM.\\n\\n        Returns\\n        -------\\n        list of parsed (header, body, footer) tuples from tables.',\n",
       " 'Given a list of <tr>s, return a list of text rows.\\n\\n        Parameters\\n        ----------\\n        rows : list of node-like\\n            List of <tr>s\\n\\n        Returns\\n        -------\\n        list of list\\n            Each returned row is a list of str text.\\n\\n        Notes\\n        -----\\n        Any cell with ``rowspan`` or ``colspan`` will have its contents copied\\n        to subsequent cells.',\n",
       " 'Return list of tables, potentially removing hidden elements\\n\\n        Parameters\\n        ----------\\n        tbl_list : list of node-like\\n            Type of list elements will vary depending upon parser used\\n        attr_name : str\\n            Name of the accessor for retrieving HTML attributes\\n\\n        Returns\\n        -------\\n        list of node-like\\n            Return type matches `tbl_list`',\n",
       " 'Raises\\n        ------\\n        ValueError\\n            * If a URL that lxml cannot parse is passed.\\n\\n        Exception\\n            * Any other ``Exception`` thrown. For example, trying to parse a\\n              URL that is syntactically correct on a machine with no internet\\n              connection will fail.\\n\\n        See Also\\n        --------\\n        pandas.io.html._HtmlFrameParser._build_doc',\n",
       " 'Parameters\\n    ----------\\n    l : list of arrays\\n\\n    Returns\\n    -------\\n    a set of kinds that exist in this list of arrays',\n",
       " 'return appropriate class of Series concat\\n    input is either dict or array-like',\n",
       " 'return appropriate class of DataFrame-like concat\\n    if all blocks are sparse, return SparseDataFrame\\n    otherwise, return 1st obj',\n",
       " \"provide concatenation of an array of arrays each of which is a single\\n    'normalized' dtypes (in that for example, if it's object, then it is a\\n    non-datetimelike and provide a combined dtype for the resulting array that\\n    preserves the overall dtype if possible)\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes\",\n",
       " 'Concatenate an object/categorical array of arrays, each of which is a\\n    single dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : int\\n        Axis to provide concatenation in the current implementation this is\\n        always 0, e.g. we only have 1D categoricals\\n\\n    Returns\\n    -------\\n    Categorical\\n        A single array, preserving the combined dtypes',\n",
       " 'provide concatenation of an datetimelike array of arrays each of which is a\\n    single M8[ns], datetimet64[ns, tz] or m8[ns] dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n    typs : set of to_concat dtypes\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes',\n",
       " 'concat DatetimeIndex with the same tz\\n    all inputs must be DatetimeIndex\\n    it is used in DatetimeIndex.append also',\n",
       " 'concat all inputs as object. DatetimeIndex, TimedeltaIndex and\\n    PeriodIndex are converted to object dtype before concatenation',\n",
       " 'provide concatenation of an sparse/dense array of arrays each of which is a\\n    single dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n    typs : set of to_concat dtypes\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes',\n",
       " 'Concatenates multiple RangeIndex instances. All members of \"indexes\" must\\n    be of type RangeIndex; result will be RangeIndex if possible, Int64Index\\n    otherwise. E.g.:\\n    indexes = [RangeIndex(3), RangeIndex(3, 6)] -> RangeIndex(6)\\n    indexes = [RangeIndex(3), RangeIndex(4, 6)] -> Int64Index([0,1,2,4,5])',\n",
       " 'Given an index, find the level length for each element.\\n\\n    Optional argument is a list of index positions which\\n    should not be visible.\\n\\n    Result is a dictionary of (level, inital_position): span',\n",
       " 'Convert the DataFrame in `self.data` and the attrs from `_build_styles`\\n        into a dictionary of {head, body, uuid, cellstyle}.',\n",
       " \"Update the state of the Styler.\\n\\n        Collects a mapping of {index_label: ['<property>: <value>']}.\\n\\n        attrs : Series or DataFrame\\n        should contain strings of '<property>: <value>;<prop2>: <val2>'\\n        Whitespace shouldn't matter and the final trailing ';' shouldn't\\n        matter.\",\n",
       " 'Execute the style functions built up in `self._todo`.\\n\\n        Relies on the conventions that all style functions go through\\n        .apply or .applymap. The append styles to apply as tuples of\\n\\n        (application method, *args, **kwargs)',\n",
       " 'Hide columns from rendering.\\n\\n        .. versionadded:: 0.23.0\\n\\n        Parameters\\n        ----------\\n        subset : IndexSlice\\n            An argument to ``DataFrame.loc`` that identifies which columns\\n            are hidden.\\n\\n        Returns\\n        -------\\n        self : Styler',\n",
       " 'Shade the background ``null_color`` for missing values.\\n\\n        Parameters\\n        ----------\\n        null_color : str\\n\\n        Returns\\n        -------\\n        self : Styler',\n",
       " 'Factory function for creating a subclass of ``Styler``\\n        with a custom template and Jinja environment.\\n\\n        Parameters\\n        ----------\\n        searchpath : str or list\\n            Path or paths of directories containing the templates\\n        name : str\\n            Name of your custom template to use for rendering\\n\\n        Returns\\n        -------\\n        MyStyler : subclass of Styler\\n            Has the correct ``env`` and ``template`` class attributes set.',\n",
       " 'Parameters\\n        ----------\\n        dtype : ExtensionDtype',\n",
       " 'Parameters\\n        ----------\\n        dtype : PandasExtensionDtype or string\\n\\n        Returns\\n        -------\\n        return the first matching dtype, otherwise return None',\n",
       " 'Return a string representation for a particular object.',\n",
       " \"provide compat for construction of strings to numpy datetime64's with\\n    tz-changes in 1.11 that make '2015-01-01 09:00:00Z' show a deprecation\\n    warning, when need to pass '2015-01-01 09:00:00'\",\n",
       " \"provide compat for construction of an array of strings to a\\n    np.array(..., dtype=np.datetime64(..))\\n    tz-changes in 1.11 that make '2015-01-01 09:00:00Z' show a deprecation\\n    warning, when need to pass '2015-01-01 09:00:00'\",\n",
       " 'Check if key is a float and has a decimal. If it has, return False.',\n",
       " 'we always want to get an index value, never a value',\n",
       " 'Determines if two Index objects contain the same elements.',\n",
       " 'ensure that the where is a Term or a list of Term\\n    this makes sure that we are capturing the scope of variables\\n    that are passed\\n    create the terms here with a frame_level=2 (we are 2 levels down)',\n",
       " 'Check if a given group is a metadata group for a given parent_group.',\n",
       " 'coerce the values to a DatetimeIndex if tz is set\\n    preserve the input shape if possible\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    tz : string/pickled tz object\\n    preserve_UTC : boolean,\\n        preserve the UTC of the result\\n    coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray',\n",
       " 'we take a string-like that is object dtype and coerce to a fixed size\\n    string type\\n\\n    Parameters\\n    ----------\\n    data : a numpy array of object dtype\\n    encoding : None or string-encoding\\n    errors : handler for encoding errors\\n    itemsize : integer, optional, defaults to the max length of the strings\\n\\n    Returns\\n    -------\\n    data in a fixed-length string dtype, encoded to bytes if needed',\n",
       " \"inverse of _convert_string_array\\n\\n    Parameters\\n    ----------\\n    data : fixed length string dtyped array\\n    nan_rep : the storage repr of NaN, optional\\n    encoding : the encoding of the data, optional\\n    errors : handler for encoding errors, default 'strict'\\n\\n    Returns\\n    -------\\n    an object array of the decoded data\",\n",
       " \"check for existence of this key\\n              can match the exact pathname or the pathnm w/o the leading '/'\",\n",
       " \"Open the file in the specified mode\\n\\n        Parameters\\n        ----------\\n        mode : {'a', 'w', 'r', 'r+'}, default 'a'\\n            See HDFStore docstring or tables.open_file for info about modes\",\n",
       " 'Force all buffered modifications to be written to disk.\\n\\n        Parameters\\n        ----------\\n        fsync : bool (default False)\\n          call ``os.fsync()`` on the file handle to force writing to disk.\\n\\n        Notes\\n        -----\\n        Without ``fsync=True``, flushing may not guarantee that the OS writes\\n        to disk. With fsync, the operation will block until the OS claims the\\n        file has been written; however, other caching layers may still\\n        interfere.',\n",
       " 'Retrieve pandas object stored in file\\n\\n        Parameters\\n        ----------\\n        key : object\\n\\n        Returns\\n        -------\\n        obj : same type as object stored in file',\n",
       " 'return the selection as an Index\\n\\n        Parameters\\n        ----------\\n        key : object\\n        where : list of Term (or convertible) objects, optional\\n        start : integer (defaults to None), row number to start selection\\n        stop  : integer (defaults to None), row number to stop selection',\n",
       " 'return a single column from the table. This is generally only useful to\\n        select an indexable\\n\\n        Parameters\\n        ----------\\n        key : object\\n        column: the column of interest\\n\\n        Exceptions\\n        ----------\\n        raises KeyError if the column is not found (or key is not a valid\\n            store)\\n        raises ValueError if the column can not be extracted individually (it\\n            is part of a data block)',\n",
       " 'Create a pytables index on the table\\n        Parameters\\n        ----------\\n        key : object (the node to index)\\n\\n        Exceptions\\n        ----------\\n        raises if the node is not a table',\n",
       " 'return a list of all the top-level nodes (that are not themselves a\\n        pandas storage object)',\n",
       " 'return the node with the key or None if it does not exist',\n",
       " 'return the storer object for a key, raise if not in the file',\n",
       " 'Print detailed information on the store.\\n\\n        .. versionadded:: 0.21.0',\n",
       " 'validate / deprecate formats; return the new kwargs',\n",
       " 'infer this column from the table: create and return a new object',\n",
       " 'set the values from this selection: take = take ownership',\n",
       " 'maybe set a string col itemsize:\\n               min_itemsize can be an integer or a dict with this columns name\\n               with an integer size',\n",
       " 'validate this column: return the compared against itemsize',\n",
       " 'set/update the info for this indexable with the key/value\\n            if there is a conflict raise/warn as needed',\n",
       " 'validate that kind=category does not change the categories',\n",
       " 'set the values from this selection: take = take ownership',\n",
       " 'validate that we have the same order as the existing & same dtype',\n",
       " 'set the data from this selection (and convert to the correct dtype\\n        if we can)',\n",
       " 'infer the axes of my storer\\n              return a boolean indicating if we have a valid storer or not',\n",
       " 'support fully deleting the node in its entirety (only) - where\\n        specification must be None',\n",
       " 'remove table keywords from kwargs and return\\n        raise if any keywords are passed which are not-None',\n",
       " 'write it as a collection of individual sparse series',\n",
       " 'validate that we can store the multi-index; reset and return the\\n        new object',\n",
       " 'return a tuple of my permutated axes, non_indexable at the front',\n",
       " 'return a dict of the kinds allowable columns for this object',\n",
       " 'write out a meta data array to the key as a fixed-format Series\\n\\n        Parameters\\n        ----------\\n        key : string\\n        values : ndarray',\n",
       " \"validate the min_itemisze doesn't contain items that are not in the\\n        axes this needs data_columns to be defined\",\n",
       " 'create and return the axes sniffed from the table: return boolean\\n        for success',\n",
       " 'take the input data_columns and min_itemize and create a data\\n        columns spec',\n",
       " 'create the description of the table from the axes & values',\n",
       " 'select coordinates (row numbers) from a table; return the\\n        coordinates object',\n",
       " 'return a single column from the table, generally only indexables\\n        are interesting',\n",
       " 'we have n indexable columns, with an arbitrary number of data\\n        axes',\n",
       " 'we form the data into a 2-d including indexes,values,mask\\n            write chunk-by-chunk',\n",
       " 'Parameters\\n        ----------\\n        rows : an empty memory space where we are putting the chunk\\n        indexes : an array of the indexes\\n        mask : an array of the masks\\n        values : an array of the values',\n",
       " \"Cast to a NumPy array with 'dtype'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray\\n            NumPy ndarray with 'dtype' for its dtype.\",\n",
       " 'Compute the ExtensionArray of unique values.\\n\\n        Returns\\n        -------\\n        uniques : ExtensionArray',\n",
       " 'Make an alias for a method of the underlying ExtensionArray.\\n\\n    Parameters\\n    ----------\\n    array_method : method on an Array class\\n\\n    Returns\\n    -------\\n    method',\n",
       " 'Create a comparison method that dispatches to ``cls.values``.',\n",
       " 'Determines if two Index objects contain the same elements.',\n",
       " 'Return the minimum value of the Index or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Series.min : Return the minimum value in a Series.',\n",
       " 'Returns the indices of the minimum values along an axis.\\n\\n        See `numpy.ndarray.argmin` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin',\n",
       " 'Return the maximum value of the Index or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Series.max : Return the maximum value in a Series.',\n",
       " 'Returns the indices of the maximum values along an axis.\\n\\n        See `numpy.ndarray.argmax` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax',\n",
       " 'Return a list of tuples of the (attr,formatted_value).',\n",
       " \"We don't allow integer or float indexing on datetime-like when using\\n        loc.\\n\\n        Parameters\\n        ----------\\n        key : label of the slice bound\\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\",\n",
       " 'Add in the datetimelike methods (as we may have to override the\\n        superclass).',\n",
       " 'Compute boolean array of whether each index value is found in the\\n        passed set of values.\\n\\n        Parameters\\n        ----------\\n        values : set or sequence of values\\n\\n        Returns\\n        -------\\n        is_contained : ndarray (boolean dtype)',\n",
       " 'Return a summarized representation.\\n\\n        Parameters\\n        ----------\\n        name : str\\n            name to use in the summary representation\\n\\n        Returns\\n        -------\\n        String with a summarized representation of the index',\n",
       " 'Replaces values in a Series using the fill method specified when no\\n    replacement value is given in the replace method',\n",
       " 'Construct and returns axes if supplied in args/kwargs.\\n\\n        If require_all, raise if all axis arguments are not supplied\\n        return a tuple of (axes, kwargs).\\n\\n        sentinel specifies the default parameter when an axis is not\\n        supplied; useful to distinguish when a user explicitly passes None\\n        in scenarios where None has special meaning.',\n",
       " \"Return the space character free column resolvers of a dataframe.\\n\\n        Column names with spaces are 'cleaned up' so that they can be referred\\n        to by backtick quoting.\\n        Used in :meth:`DataFrame.eval`.\",\n",
       " 'Interchange axes and swap values axes appropriately.\\n\\n        Returns\\n        -------\\n        y : same as input',\n",
       " 'Swap levels i and j in a MultiIndex on a particular axis\\n\\n        Parameters\\n        ----------\\n        i, j : int, str (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        swapped : same type as caller (new object)\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.',\n",
       " 'Return the bool of a single element PandasObject.\\n\\n        This must be a boolean scalar value, either True or False.  Raise a\\n        ValueError if the PandasObject does not have exactly 1 element, or that\\n        element is not boolean',\n",
       " 'Check whether `key` is ambiguous.\\n\\n        By ambiguous, we mean that it matches both a level of the input\\n        `axis` and a label of the other axis.\\n\\n        Parameters\\n        ----------\\n        key: str or object\\n            label or level name\\n        axis: int, default 0\\n            Axis that levels are associated with (0 for index, 1 for columns)\\n\\n        Raises\\n        ------\\n        ValueError: `key` is ambiguous',\n",
       " 'Not a real Jupyter special repr method, but we use the same\\n        naming convention.',\n",
       " 'Get item from object for given key (DataFrame column, Panel slice,\\n        etc.). Returns default value if not found.\\n\\n        Parameters\\n        ----------\\n        key : object\\n\\n        Returns\\n        -------\\n        value : same type as items contained in object',\n",
       " 'Return the cached item, item represents a label indexer.',\n",
       " 'Set the _cacher attribute on the calling object with a weakref to\\n        cacher.',\n",
       " 'Return the cached item, item represents a positional indexer.',\n",
       " 'See if we need to update our parent cacher if clear, then clear our\\n        cache.\\n\\n        Parameters\\n        ----------\\n        clear : boolean, default False\\n            clear the item cache\\n        verify_is_copy : boolean, default True\\n            provide is_copy checks',\n",
       " 'Construct a slice of this container.\\n\\n        kind parameter is maintained for compatibility with Series slicing.',\n",
       " 'Check if we are a view, have a cacher, and are of mixed type.\\n        If so, then force a setitem_copy check.\\n\\n        Should be called just near setting a value\\n\\n        Will return a boolean if it we are a view and are cached, but a\\n        single-dtype meaning that the cacher should be updated following\\n        setting.',\n",
       " 'Return data corresponding to axis labels matching criteria.\\n\\n        .. deprecated:: 0.21.0\\n            Use df.loc[df.index.map(crit)] to select via labels\\n\\n        Parameters\\n        ----------\\n        crit : function\\n            To be called on each index (label). Should return True or False\\n        axis : int\\n\\n        Returns\\n        -------\\n        selection : same type as caller',\n",
       " \"Drop labels from specified axis. Used in the ``drop`` method\\n        internally.\\n\\n        Parameters\\n        ----------\\n        labels : single label or list-like\\n        axis : int or axis name\\n        level : int or level name, default None\\n            For MultiIndex\\n        errors : {'ignore', 'raise'}, default 'raise'\\n            If 'ignore', suppress error and existing labels are dropped.\",\n",
       " 'Replace self internals with result.\\n\\n        Parameters\\n        ----------\\n        verify_is_copy : boolean, default True\\n            provide is_copy checks',\n",
       " 'Propagate metadata from other to self.\\n\\n        Parameters\\n        ----------\\n        other : the object from which to get the attributes that we are going\\n            to propagate\\n        method : optional, a passed method name ; possibly to take different\\n            types of propagation actions based on this',\n",
       " 'After regular attribute access, try looking up the name\\n        This allows simpler access to columns for interactive use.',\n",
       " 'After regular attribute access, try setting the name\\n        This allows simpler access to columns for interactive use.',\n",
       " \"add the string-like attributes from the info_axis.\\n        If info_axis is a MultiIndex, it's first level values are used.\",\n",
       " 'Consolidate _data -- if the blocks have changed, then clear the\\n        cache',\n",
       " 'Compute NDFrame with \"consolidated\" internals (data of each dtype\\n        grouped together in a single ndarray).\\n\\n        Parameters\\n        ----------\\n        inplace : boolean, default False\\n            If False return new object, otherwise modify existing object\\n\\n        Returns\\n        -------\\n        consolidated : same type as caller',\n",
       " 'check whether we allow in-place setting with this type of value',\n",
       " 'Convert the frame to a dict of dtype -> Constructor Types that each has\\n        a homogeneous dtype.\\n\\n        .. deprecated:: 0.21.0\\n\\n        NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in\\n              as_matrix)\\n\\n        Parameters\\n        ----------\\n        copy : boolean, default True\\n\\n        Returns\\n        -------\\n        values : a dict of dtype -> Constructor Types',\n",
       " 'Return a dict of dtype -> Constructor Types that\\n        each is a homogeneous dtype.\\n\\n        Internal ONLY',\n",
       " 'Equivalent to public method `where`, except that `other` is not\\n        applied as a function even if callable. Used in __setitem__.',\n",
       " 'Validate percentiles (used by describe and quantile).',\n",
       " 'Add the operations to the cls; evaluate the doc strings again',\n",
       " 'Add the series only operations to the cls; evaluate the doc\\n        strings again.',\n",
       " 'Add the series or dataframe only operations to the cls; evaluate\\n        the doc strings again.',\n",
       " \"Retrieves the index of the first valid value.\\n\\n        Parameters\\n        ----------\\n        how : {'first', 'last'}\\n            Use this parameter to change between the first or last valid index.\\n\\n        Returns\\n        -------\\n        idx_first_valid : type of index\",\n",
       " 'Reset cached properties. If ``key`` is passed, only clears that key.',\n",
       " 'Generates the total memory usage for an object that returns\\n        either a value or Series of values',\n",
       " 'if arg is a string, then try to operate on it:\\n        - try to find a function (or attribute) on ourselves\\n        - try to find a numpy function\\n        - raise',\n",
       " 'provide an implementation for the aggregators\\n\\n        Parameters\\n        ----------\\n        arg : string, dict, function\\n        *args : args to pass on to the function\\n        **kwargs : kwargs to pass on to the function\\n\\n        Returns\\n        -------\\n        tuple of result, how\\n\\n        Notes\\n        -----\\n        how can be a string describe the required post-processing, or\\n        None if not required',\n",
       " 'return a new object with the replacement attributes',\n",
       " 'Return the size of the dtype of the item of the underlying data.\\n\\n        .. deprecated:: 0.23.0',\n",
       " 'Return the base object if the memory of the underlying data is shared.\\n\\n        .. deprecated:: 0.23.0',\n",
       " 'The data as an ndarray, possibly losing information.\\n\\n        The expectation is that this is cheap to compute, and is primarily\\n        used for interacting with our indexers.\\n\\n        - categorical -> codes',\n",
       " 'Return an ndarray of the maximum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax',\n",
       " 'Return a ndarray of the minimum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin',\n",
       " 'Return a list of the values.\\n\\n        These are each a scalar type, which is a Python scalar\\n        (for str, int, float) or a pandas scalar\\n        (for Timestamp/Timedelta/Interval/Period)\\n\\n        Returns\\n        -------\\n        list\\n\\n        See Also\\n        --------\\n        numpy.ndarray.tolist',\n",
       " 'Return an iterator of the values.\\n\\n        These are each a scalar type, which is a Python scalar\\n        (for str, int, float) or a pandas scalar\\n        (for Timestamp/Timedelta/Interval/Period)',\n",
       " 'Memory usage of the values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False or if used on PyPy',\n",
       " \"Return the argument with an initial component of ~ or ~user\\n       replaced by that user's home directory.\\n\\n    Parameters\\n    ----------\\n    filepath_or_buffer : object to be converted if possible\\n\\n    Returns\\n    -------\\n    expanded_filepath_or_buffer : an expanded filepath or the\\n                                  input if not expandable\",\n",
       " 'Wrap comparison operations to convert timedelta-like to timedelta64',\n",
       " 'Convert an ndarray with integer-dtype to timedelta64[ns] dtype, treating\\n    the integers as multiples of the given timedelta unit.\\n\\n    Parameters\\n    ----------\\n    data : numpy.ndarray with integer-dtype\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n\\n    Returns\\n    -------\\n    numpy.ndarray : timedelta64[ns] array converted from data\\n    bool : whether a copy was made',\n",
       " 'Add DatetimeArray/Index or ndarray[datetime64] to TimedeltaArray.',\n",
       " 'Return a dataframe of the components (days, hours, minutes,\\n        seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas.\\n\\n        Returns\\n        -------\\n        a DataFrame',\n",
       " 'Add engine to the excel writer registry.io.excel.\\n\\n    You must use this method to integrate with ``to_excel``.\\n\\n    Parameters\\n    ----------\\n    klass : ExcelWriter',\n",
       " \"Convert Excel column name like 'AB' to 0-based column index.\\n\\n    Parameters\\n    ----------\\n    x : str\\n        The Excel column name to convert to a 0-based column index.\\n\\n    Returns\\n    -------\\n    num : int\\n        The column index corresponding to the name.\\n\\n    Raises\\n    ------\\n    ValueError\\n        Part of the Excel column name was invalid.\",\n",
       " \"Convert comma separated list of column names and ranges to indices.\\n\\n    Parameters\\n    ----------\\n    areas : str\\n        A string containing a sequence of column ranges (or areas).\\n\\n    Returns\\n    -------\\n    cols : list\\n        A list of 0-based column indices.\\n\\n    Examples\\n    --------\\n    >>> _range2cols('A:E')\\n    [0, 1, 2, 3, 4]\\n    >>> _range2cols('A,C,Z:AB')\\n    [0, 2, 25, 26, 27]\",\n",
       " 'Convert `usecols` into a compatible format for parsing in `parsers.py`.\\n\\n    Parameters\\n    ----------\\n    usecols : object\\n        The use-columns object to potentially convert.\\n\\n    Returns\\n    -------\\n    converted : object\\n        The compatible format of `usecols`.',\n",
       " 'Forward fill blank entries in row but only inside the same parent index.\\n\\n    Used for creating headers in Multiindex.\\n    Parameters\\n    ----------\\n    row : list\\n        List of items in a single row.\\n    control_row : list of bool\\n        Helps to determine if particular column is in same parent index as the\\n        previous value. Used to stop propagation of empty cells between\\n        different indexes.\\n\\n    Returns\\n    ----------\\n    Returns changed row and control_row',\n",
       " 'Pop the header name for MultiIndex parsing.\\n\\n    Parameters\\n    ----------\\n    row : list\\n        The data row to parse for the header name.\\n    index_col : int, list\\n        The index columns for our data. Assumed to be non-null.\\n\\n    Returns\\n    -------\\n    header_name : str\\n        The extracted header name.\\n    trimmed_row : list\\n        The original data row with the header name removed.',\n",
       " \"Replace a number with its hexadecimal representation. Used to tag\\n    temporary variables with their calling scope's id.\",\n",
       " 'Return a prettier version of obj\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        Object to pretty print\\n\\n    Returns\\n    -------\\n    s : str\\n        Pretty print object repr',\n",
       " \"Resolve a variable name in a possibly local context\\n\\n        Parameters\\n        ----------\\n        key : str\\n            A variable name\\n        is_local : bool\\n            Flag indicating whether the variable is local or not (prefixed with\\n            the '@' symbol)\\n\\n        Returns\\n        -------\\n        value : object\\n            The value of a particular variable\",\n",
       " 'Replace a variable name, with a potentially new value.\\n\\n        Parameters\\n        ----------\\n        old_key : str\\n            Current variable name to replace\\n        new_key : str\\n            New variable name to replace `old_key` with\\n        new_value : object\\n            Value to be replaced along with the possible renaming',\n",
       " \"Get specifically scoped variables from a list of stack frames.\\n\\n        Parameters\\n        ----------\\n        stack : list\\n            A list of stack frames as returned by ``inspect.stack()``\\n        scopes : sequence of strings\\n            A sequence containing valid stack frame attribute names that\\n            evaluate to a dictionary. For example, ('locals', 'globals')\",\n",
       " 'Update the current scope by going back `level` levels.\\n\\n        Parameters\\n        ----------\\n        level : int or None, optional, default None',\n",
       " 'Add a temporary variable to the scope.\\n\\n        Parameters\\n        ----------\\n        value : object\\n            An arbitrary object to be assigned to a temporary variable.\\n\\n        Returns\\n        -------\\n        name : basestring\\n            The name of the temporary variable created.',\n",
       " 'Return the full scope for use with passing to engines transparently\\n        as a mapping.\\n\\n        Returns\\n        -------\\n        vars : DeepChainMap\\n            All variables in this scope.',\n",
       " 'Construct Series from array.\\n\\n        .. deprecated :: 0.23.0\\n            Use pd.Series(..) constructor instead.',\n",
       " 'Return object Series which contains boxed values.\\n\\n        .. deprecated :: 0.23.0\\n\\n           Use ``astype(object)`` instead.\\n\\n        *this is an internal non-public method*',\n",
       " 'Return selected slices of an array along given axis as a Series.\\n\\n        .. deprecated:: 0.24.0\\n\\n        See Also\\n        --------\\n        numpy.ndarray.compress',\n",
       " 'Return the i-th value or values in the Series by location.\\n\\n        Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n\\n        Returns\\n        -------\\n        scalar (int) or Series (slice, sequence)',\n",
       " 'Return a unicode string representation for a particular DataFrame.',\n",
       " 'Convert Series to DataFrame.\\n\\n        Parameters\\n        ----------\\n        name : object, default None\\n            The passed name should substitute for the series name (if it has\\n            one).\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame representation of Series.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\\n        ...               name=\"vals\")\\n        >>> s.to_frame()\\n          vals\\n        0    a\\n        1    b\\n        2    c',\n",
       " \"Convert Series to SparseSeries.\\n\\n        Parameters\\n        ----------\\n        kind : {'block', 'integer'}, default 'block'\\n        fill_value : float, defaults to NaN (missing)\\n            Value to use for filling NaN values.\\n\\n        Returns\\n        -------\\n        SparseSeries\\n            Sparse representation of the Series.\",\n",
       " 'Set the Series name.\\n\\n        Parameters\\n        ----------\\n        name : str\\n        inplace : bool\\n            whether to modify `self` directly or return a copy',\n",
       " 'Swap levels i and j in a MultiIndex.\\n\\n        Parameters\\n        ----------\\n        i, j : int, str (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        Series\\n            Series with levels swapped in MultiIndex.\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.',\n",
       " 'Rearrange index levels using input order.\\n\\n        May not drop or duplicate levels.\\n\\n        Parameters\\n        ----------\\n        order : list of int representing new level order\\n               (reference level by number or key)\\n\\n        Returns\\n        -------\\n        type of caller (new object)',\n",
       " 'Perform a reduction operation.\\n\\n        If we have an ndarray as a value, then simply perform the operation,\\n        otherwise delegate to the object.',\n",
       " 'Conform Series to new index with optional filling logic.\\n\\n        .. deprecated:: 0.21.0\\n            Use ``Series.reindex`` instead.',\n",
       " 'Return Series without null values.\\n\\n        .. deprecated:: 0.23.0\\n            Use :meth:`Series.dropna` instead.',\n",
       " \"Cast to DatetimeIndex of Timestamps, at *beginning* of period.\\n\\n        Parameters\\n        ----------\\n        freq : str, default frequency of PeriodIndex\\n            Desired frequency.\\n        how : {'s', 'e', 'start', 'end'}\\n            Convention for converting period to timestamp; start of period\\n            vs. end.\\n        copy : bool, default True\\n            Whether or not to return a copy.\\n\\n        Returns\\n        -------\\n        Series with DatetimeIndex\",\n",
       " 'Convert Series from DatetimeIndex to PeriodIndex with desired\\n        frequency (inferred from index if not passed).\\n\\n        Parameters\\n        ----------\\n        freq : str, default None\\n            Frequency associated with the PeriodIndex.\\n        copy : bool, default True\\n            Whether or not to return a copy.\\n\\n        Returns\\n        -------\\n        Series\\n            Series with index converted to PeriodIndex.',\n",
       " 'Create a 0-dim ndarray containing the fill value\\n\\n    Parameters\\n    ----------\\n    arr : SparseArray\\n\\n    Returns\\n    -------\\n    fill_value : ndarray\\n        0-dim ndarray with just the fill value.\\n\\n    Notes\\n    -----\\n    coerce fill_value to arr dtype if possible\\n    int64 SparseArray can have NaN as fill_value if there is no missing',\n",
       " 'Perform a binary operation between two arrays.\\n\\n    Parameters\\n    ----------\\n    left : Union[SparseArray, ndarray]\\n    right : Union[SparseArray, ndarray]\\n    op : Callable\\n        The binary operation to perform\\n    name str\\n        Name of the callable.\\n\\n    Returns\\n    -------\\n    SparseArray',\n",
       " 'return an ndarray for our input,\\n    in a platform independent manner',\n",
       " \"Convert ndarray to sparse format\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    kind : {'block', 'integer'}\\n    fill_value : NaN or another value\\n    dtype : np.dtype, optional\\n    copy : bool, default False\\n\\n    Returns\\n    -------\\n    (sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)\",\n",
       " 'The percent of non- ``fill_value`` points, as decimal.\\n\\n        Examples\\n        --------\\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\\n        >>> s.density\\n        0.6',\n",
       " 'Get the location of the first missing value.\\n\\n        Returns\\n        -------\\n        int',\n",
       " \"Returns a Series containing counts of unique values.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don't include counts of NaN, even if NaN is in sp_values.\\n\\n        Returns\\n        -------\\n        counts : Series\",\n",
       " 'Tests whether all elements evaluate True\\n\\n        Returns\\n        -------\\n        all : bool\\n\\n        See Also\\n        --------\\n        numpy.all',\n",
       " 'Tests whether at least one of elements evaluate True\\n\\n        Returns\\n        -------\\n        any : bool\\n\\n        See Also\\n        --------\\n        numpy.any',\n",
       " 'Sum of non-NA/null values\\n\\n        Returns\\n        -------\\n        sum : float',\n",
       " 'Mean of non-NA/null values\\n\\n        Returns\\n        -------\\n        mean : float',\n",
       " 'Tokenize a Python source code string.\\n\\n    Parameters\\n    ----------\\n    source : str\\n        A Python source code string',\n",
       " 'Replace ``&`` with ``and`` and ``|`` with ``or`` so that bitwise\\n    precedence is changed to boolean precedence.\\n\\n    Parameters\\n    ----------\\n    tok : tuple of int, str\\n        ints correspond to the all caps constants in the tokenize module\\n\\n    Returns\\n    -------\\n    t : tuple of int, str\\n        Either the input or token or the replacement values',\n",
       " 'Filter out AST nodes that are subclasses of ``superclass``.',\n",
       " 'Return a function that raises a NotImplementedError with a passed node\\n    name.',\n",
       " 'Decorator to disallow certain nodes from parsing. Raises a\\n    NotImplementedError instead.\\n\\n    Returns\\n    -------\\n    disallowed : callable',\n",
       " 'Return a function to create an op class with its symbol already passed.\\n\\n    Returns\\n    -------\\n    f : callable',\n",
       " 'return a boolean whether I can attempt conversion to a TimedeltaIndex',\n",
       " 'Returns a FrozenList with other concatenated to the end of self.\\n\\n        Parameters\\n        ----------\\n        other : array-like\\n            The array-like whose elements we are concatenating.\\n\\n        Returns\\n        -------\\n        diff : FrozenList\\n            The collection difference between self and other.',\n",
       " 'Returns a FrozenList with elements from other removed from self.\\n\\n        Parameters\\n        ----------\\n        other : array-like\\n            The array-like whose elements we are removing self.\\n\\n        Returns\\n        -------\\n        diff : FrozenList\\n            The collection difference between self and other.',\n",
       " 'Return a unicode string representation for this object.',\n",
       " 'Find indices to insert `value` so as to maintain order.\\n\\n        For full documentation, see `numpy.searchsorted`\\n\\n        See Also\\n        --------\\n        numpy.searchsorted : Equivalent function.',\n",
       " 'Segregate Series based on type and coerce into matrices.\\n\\n    Needs to handle a lot of exceptional cases.',\n",
       " 'Extract from a masked rec array and create the manager.',\n",
       " 'Segregate Series based on type and coerce into matrices.\\n    Needs to handle a lot of exceptional cases.',\n",
       " 'Sanitize an index type to return an ndarray of the underlying, pass\\n    through a non-Index.',\n",
       " 'Sanitize input data to an ndarray, copy if specified, coerce to the\\n    dtype if specified.',\n",
       " \"Make sure a valid engine is passed.\\n\\n    Parameters\\n    ----------\\n    engine : str\\n\\n    Raises\\n    ------\\n    KeyError\\n      * If an invalid engine is passed\\n    ImportError\\n      * If numexpr was requested but doesn't exist\\n\\n    Returns\\n    -------\\n    string engine\",\n",
       " 'Make sure a valid parser is passed.\\n\\n    Parameters\\n    ----------\\n    parser : str\\n\\n    Raises\\n    ------\\n    KeyError\\n      * If an invalid parser is passed',\n",
       " \"Parameters\\n        ----------\\n        codes : optional list\\n            Codes to check for validity. Defaults to current codes.\\n        levels : optional list\\n            Levels to check for validity. Defaults to current levels.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If length of levels and codes don't match, if the codes for any\\n            level would exceed level bounds, or there are any duplicate levels.\",\n",
       " 'return a boolean if we need a qualified .info display',\n",
       " 'return the number of bytes in the underlying data\\n        deeply introspect the level data if deep=True\\n\\n        include the engine hashtable\\n\\n        *this is in internal routine*',\n",
       " 'Return a list of tuples of the (attr,formatted_value)',\n",
       " 'return if the index is monotonic increasing (only equal or\\n        increasing) values.',\n",
       " 'validate and return the hash for the provided key\\n\\n        *this is internal for use for the cython routines*\\n\\n        Parameters\\n        ----------\\n        key : string or tuple\\n\\n        Returns\\n        -------\\n        np.uint64\\n\\n        Notes\\n        -----\\n        we need to stringify if we have mixed levels',\n",
       " 'Return vector of label values for requested level,\\n        equal to the length of the index\\n\\n        **this is an internal method**\\n\\n        Parameters\\n        ----------\\n        level : int level\\n        unique : bool, default False\\n            if True, drop duplicated values\\n\\n        Returns\\n        -------\\n        values : ndarray',\n",
       " 'Append a collection of Index options together\\n\\n        Parameters\\n        ----------\\n        other : Index or list/tuple of indices\\n\\n        Returns\\n        -------\\n        appended : Index',\n",
       " 'Make new MultiIndex with passed list of codes deleted\\n\\n        Parameters\\n        ----------\\n        codes : array-like\\n            Must be a list of tuples\\n        level : int or level name, default None\\n\\n        Returns\\n        -------\\n        dropped : MultiIndex',\n",
       " 'Rearrange levels using input order. May not drop or duplicate levels\\n\\n        Parameters\\n        ----------',\n",
       " 'we categorizing our codes by using the\\n        available categories (all, not just observed)\\n        excluding any missing ones (-1); this is in preparation\\n        for sorting, where we need to disambiguate that -1 is not\\n        a valid valid',\n",
       " 'Parameters\\n        ----------\\n        keyarr : list-like\\n            Indexer to convert.\\n\\n        Returns\\n        -------\\n        tuple (indexer, keyarr)\\n            indexer is an ndarray or None if cannot convert\\n            keyarr are tuple-safe keys',\n",
       " \"Create index with target's values (move/add/delete values as necessary)\\n\\n        Returns\\n        -------\\n        new_index : pd.MultiIndex\\n            Resulting index\\n        indexer : np.ndarray or None\\n            Indices of output values in original index.\",\n",
       " 'Slice index between two labels / tuples, return new MultiIndex\\n\\n        Parameters\\n        ----------\\n        before : label or tuple, can be partial. Default None\\n            None defaults to start\\n        after : label or tuple, can be partial. Default None\\n            None defaults to end\\n\\n        Returns\\n        -------\\n        truncated : MultiIndex',\n",
       " 'Determines if two MultiIndex objects have the same labeling information\\n        (the levels themselves do not necessarily have to be the same)\\n\\n        See Also\\n        --------\\n        equal_levels',\n",
       " 'Return True if the levels of both MultiIndex objects are the same',\n",
       " 'Form the intersection of two MultiIndex objects.\\n\\n        Parameters\\n        ----------\\n        other : MultiIndex or array / Index of tuples\\n        sort : False or None, default False\\n            Sort the resulting MultiIndex if possible\\n\\n            .. versionadded:: 0.24.0\\n\\n            .. versionchanged:: 0.24.1\\n\\n               Changed the default from ``True`` to ``False``, to match\\n               behaviour from before 0.24.0\\n\\n        Returns\\n        -------\\n        Index',\n",
       " 'Compute set difference of two MultiIndex objects\\n\\n        Parameters\\n        ----------\\n        other : MultiIndex\\n        sort : False or None, default None\\n            Sort the resulting MultiIndex if possible\\n\\n            .. versionadded:: 0.24.0\\n\\n            .. versionchanged:: 0.24.1\\n\\n               Changed the default value from ``True`` to ``None``\\n               (without change in behaviour).\\n\\n        Returns\\n        -------\\n        diff : MultiIndex',\n",
       " 'Make new MultiIndex inserting new item at location\\n\\n        Parameters\\n        ----------\\n        loc : int\\n        item : tuple\\n            Must be same length as number of levels in the MultiIndex\\n\\n        Returns\\n        -------\\n        new_index : Index',\n",
       " 'Make new index with passed location deleted\\n\\n        Returns\\n        -------\\n        new_index : MultiIndex',\n",
       " 'routine to ensure that our data is of the correct\\n    input dtype for lower-level routines\\n\\n    This will coerce:\\n    - ints -> int64\\n    - uint -> uint64\\n    - bool -> uint64 (TODO this should be uint8)\\n    - datetimelike -> i8\\n    - datetime64tz -> i8 (in local tz)\\n    - categorical -> codes\\n\\n    Parameters\\n    ----------\\n    values : array-like\\n    dtype : pandas_dtype, optional\\n        coerce to this dtype\\n\\n    Returns\\n    -------\\n    (ndarray, pandas_dtype, algo dtype as a string)',\n",
       " 'reverse of _ensure_data\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    dtype : pandas_dtype\\n    original : ndarray-like\\n\\n    Returns\\n    -------\\n    Index for extension types, otherwise ndarray casted to dtype',\n",
       " 'Parameters\\n    ----------\\n    values : arraylike\\n\\n    Returns\\n    -------\\n    tuples(hashtable class,\\n           vector class,\\n           values,\\n           dtype,\\n           ndtype)',\n",
       " 'Compute locations of to_match into values\\n\\n    Parameters\\n    ----------\\n    to_match : array-like\\n        values to find positions of\\n    values : array-like\\n        Unique set of values\\n    na_sentinel : int, default -1\\n        Value to mark \"not found\"\\n\\n    Examples\\n    --------\\n\\n    Returns\\n    -------\\n    match : ndarray of integers',\n",
       " 'Compute the isin boolean array\\n\\n    Parameters\\n    ----------\\n    comps : array-like\\n    values : array-like\\n\\n    Returns\\n    -------\\n    boolean array same length as comps',\n",
       " 'Parameters\\n    ----------\\n    values : arraylike\\n    dropna : boolean\\n\\n    Returns\\n    -------\\n    (uniques, counts)',\n",
       " \"Returns the mode(s) of an array.\\n\\n    Parameters\\n    ----------\\n    values : array-like\\n        Array over which to check for duplicate values.\\n    dropna : boolean, default True\\n        Don't consider counts of NaN/NaT.\\n\\n        .. versionadded:: 0.24.0\\n\\n    Returns\\n    -------\\n    mode : Series\",\n",
       " 'Specialized Cython take which sets NaN values in one pass',\n",
       " 'difference of n between self,\\n    analogous to s-s.shift(n)\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    n : int\\n        number of periods\\n    axis : int\\n        axis to shift on\\n\\n    Returns\\n    -------\\n    shifted',\n",
       " 'For arbitrary (MultiIndexed) SparseSeries return\\n    (v, i, j, ilabels, jlabels) where (v, (i, j)) is suitable for\\n    passing to scipy.sparse.coo constructor.',\n",
       " 'Convert a SparseSeries to a scipy.sparse.coo_matrix using index\\n    levels row_levels, column_levels as the row and column\\n    labels respectively. Returns the sparse_matrix, row and column labels.',\n",
       " 'Convert a scipy.sparse.coo_matrix to a SparseSeries.\\n    Use the defaults given in the SparseSeries constructor.',\n",
       " 'Wrap comparison operations to convert datetime-like to datetime64',\n",
       " 'Convert data based on dtype conventions, issuing deprecation warnings\\n    or errors where appropriate.\\n\\n    Parameters\\n    ----------\\n    data : np.ndarray or pd.Index\\n    copy : bool\\n\\n    Returns\\n    -------\\n    data : np.ndarray or pd.Index\\n    copy : bool\\n\\n    Raises\\n    ------\\n    TypeError : PeriodDType data is passed',\n",
       " 'If a timezone is inferred from data, check that it is compatible with\\n    the user-provided timezone, if any.\\n\\n    Parameters\\n    ----------\\n    tz : tzinfo or None\\n    inferred_tz : tzinfo or None\\n\\n    Returns\\n    -------\\n    tz : tzinfo or None\\n\\n    Raises\\n    ------\\n    TypeError : if both timezones are present but do not match',\n",
       " 'Check that a dtype, if passed, represents either a numpy datetime64[ns]\\n    dtype or a pandas DatetimeTZDtype.\\n\\n    Parameters\\n    ----------\\n    dtype : object\\n\\n    Returns\\n    -------\\n    dtype : None, numpy.dtype, or DatetimeTZDtype\\n\\n    Raises\\n    ------\\n    ValueError : invalid dtype\\n\\n    Notes\\n    -----\\n    Unlike validate_tz_from_dtype, this does _not_ allow non-existent\\n    tz errors to go through',\n",
       " 'If the given dtype is a DatetimeTZDtype, extract the implied\\n    tzinfo object from it and check that it does not conflict with the given\\n    tz.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, str\\n    tz : None, tzinfo\\n\\n    Returns\\n    -------\\n    tz : consensus tzinfo\\n\\n    Raises\\n    ------\\n    ValueError : on tzinfo mismatch',\n",
       " 'If a timezone is not explicitly given via `tz`, see if one can\\n    be inferred from the `start` and `end` endpoints.  If more than one\\n    of these inputs provides a timezone, require that they all agree.\\n\\n    Parameters\\n    ----------\\n    start : Timestamp\\n    end : Timestamp\\n    tz : tzinfo or None\\n\\n    Returns\\n    -------\\n    tz : tzinfo or None\\n\\n    Raises\\n    ------\\n    TypeError : if start and end timezones do not agree',\n",
       " 'Localize a start or end Timestamp to the timezone of the corresponding\\n    start or end Timestamp\\n\\n    Parameters\\n    ----------\\n    ts : start or end Timestamp to potentially localize\\n    is_none : argument that should be None\\n    is_not_none : argument that should not be None\\n    freq : Tick, DateOffset, or None\\n    tz : str, timezone object or None\\n\\n    Returns\\n    -------\\n    ts : Timestamp',\n",
       " 'Return an iterator over the boxed values\\n\\n        Yields\\n        -------\\n        tstamp : Timestamp',\n",
       " 'subtract DatetimeArray/Index or ndarray[datetime64]',\n",
       " 'Add a timedelta-like, Tick, or TimedeltaIndex-like object\\n        to self, yielding a new DatetimeArray\\n\\n        Parameters\\n        ----------\\n        other : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : DatetimeArray',\n",
       " 'Calculate TimedeltaArray of difference between index\\n        values and index converted to PeriodArray at specified\\n        freq. Used for vectorized offsets\\n\\n        Parameters\\n        ----------\\n        freq : Period frequency\\n\\n        Returns\\n        -------\\n        TimedeltaArray/Index',\n",
       " 'Returns numpy array of datetime.time. The time part of the Timestamps.',\n",
       " 'Convert Datetime Array to float64 ndarray of Julian Dates.\\n        0 Julian date is noon January 1, 4713 BC.\\n        http://en.wikipedia.org/wiki/Julian_day',\n",
       " 'Validate the docstring for the given func_name\\n\\n    Parameters\\n    ----------\\n    func_name : function\\n        Function whose docstring will be evaluated (e.g. pandas.read_csv).\\n\\n    Returns\\n    -------\\n    dict\\n        A dictionary containing all the information obtained from validating\\n        the docstring.',\n",
       " \"Import Python object from its name as string.\\n\\n        Parameters\\n        ----------\\n        name : str\\n            Object name to import (e.g. pandas.Series.str.upper)\\n\\n        Returns\\n        -------\\n        object\\n            Python object that can be a class, method, function...\\n\\n        Examples\\n        --------\\n        >>> Docstring._load_obj('pandas.Series')\\n        <class 'pandas.core.series.Series'>\",\n",
       " 'Find the Python object that contains the source code of the object.\\n\\n        This is useful to find the place in the source code (file and line\\n        number) where a docstring is defined. It does not currently work for\\n        all cases, but it should help find some (properties...).',\n",
       " 'File name where the object is implemented (e.g. pandas/core/frame.py).',\n",
       " 'Check if the docstrings method can return something.\\n\\n        Bare returns, returns valued None and returns from nested functions are\\n        disconsidered.\\n\\n        Returns\\n        -------\\n        bool\\n            Whether the docstrings method can return something.',\n",
       " 'Convert numpy types to Python types for the Excel writers.\\n\\n        Parameters\\n        ----------\\n        val : object\\n            Value to be written into cells\\n\\n        Returns\\n        -------\\n        Tuple with the first element being the converted value and the second\\n            being an optional format',\n",
       " \"checks that path's extension against the Writer's supported\\n        extensions.  If it isn't supported, raises UnsupportedFiletypeError.\",\n",
       " 'Parse specified sheet(s) into a DataFrame\\n\\n        Equivalent to read_excel(ExcelFile, ...)  See the read_excel\\n        docstring for more info on accepted parameters',\n",
       " 'Validate that the where statement is of the right type.\\n\\n    The type may either be String, Expr, or list-like of Exprs.\\n\\n    Parameters\\n    ----------\\n    w : String term expression, Expr, or list-like of Exprs.\\n\\n    Returns\\n    -------\\n    where : The original where clause if the check was successful.\\n\\n    Raises\\n    ------\\n    TypeError : An invalid data type was passed in for w (e.g. dict).',\n",
       " 'loose checking if s is a pytables-acceptable expression',\n",
       " 'convert the expression that is in the term to something that is\\n        accepted by pytables',\n",
       " 'quote the string if not encoded\\n            else encode and return',\n",
       " 'wrapper around numpy.result_type which overcomes the NPY_MAXARGS (32)\\n    argument limit',\n",
       " \"If 'Series.argmin' is called via the 'numpy' library,\\n    the third parameter in its signature is 'out', which\\n    takes either an ndarray or 'None', so check if the\\n    'skipna' parameter is either an instance of ndarray or\\n    is None, since 'skipna' itself should be a boolean\",\n",
       " \"If 'Series.argmax' is called via the 'numpy' library,\\n    the third parameter in its signature is 'out', which\\n    takes either an ndarray or 'None', so check if the\\n    'skipna' parameter is either an instance of ndarray or\\n    is None, since 'skipna' itself should be a boolean\",\n",
       " \"If 'Categorical.argsort' is called via the 'numpy' library, the\\n    first parameter in its signature is 'axis', which takes either\\n    an integer or 'None', so check if the 'ascending' parameter has\\n    either integer type or is None, since 'ascending' itself should\\n    be a boolean\",\n",
       " \"If 'NDFrame.clip' is called via the numpy library, the third\\n    parameter in its signature is 'out', which can takes an ndarray,\\n    so check if the 'axis' parameter is an instance of ndarray, since\\n    'axis' itself should either be an integer or None\",\n",
       " \"If this function is called via the 'numpy' library, the third\\n    parameter in its signature is 'dtype', which takes either a\\n    'numpy' dtype or 'None', so check if the 'skipna' parameter is\\n    a boolean or not\",\n",
       " \"If this function is called via the 'numpy' library, the third\\n    parameter in its signature is 'axis', which takes either an\\n    ndarray or 'None', so check if the 'convert' parameter is either\\n    an instance of ndarray or is None\",\n",
       " \"'args' and 'kwargs' should be empty, except for allowed\\n    kwargs because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\",\n",
       " \"'args' and 'kwargs' should be empty because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\",\n",
       " 'Ensure that the axis argument passed to min, max, argmin, or argmax is\\n    zero or None, as otherwise it will be incorrectly ignored.\\n\\n    Parameters\\n    ----------\\n    axis : int or None\\n\\n    Raises\\n    ------\\n    ValueError',\n",
       " 'Load msgpack pandas object from the specified\\n    file path\\n\\n    THIS IS AN EXPERIMENTAL LIBRARY and the storage format\\n    may not be stable until a future release.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : string File path, BytesIO like or string\\n    encoding : Encoding for decoding msgpack str type\\n    iterator : boolean, if True, return an iterator to the unpacker\\n               (default is False)\\n\\n    Returns\\n    -------\\n    obj : same type as object stored in file',\n",
       " 'Convert strings to complex number instance with specified numpy type.',\n",
       " 'Unpack a packed object, return an iterator\\n    Note: packed lists will be returned as tuples',\n",
       " 'At this point, the data either has a `read` attribute (e.g. a file\\n        object or a StringIO) or is a string that is a JSON document.\\n\\n        If self.chunksize, we prepare the data for the `__next__` method.\\n        Otherwise, we read it into memory for the `read` method.',\n",
       " 'The function read_json accepts three input types:\\n            1. filepath (string-like)\\n            2. file-like object (e.g. open file object, StringIO)\\n            3. JSON string\\n\\n        This method turns (1) into (2) to simplify the rest of the processing.\\n        It returns input types (2) and (3) unchanged.',\n",
       " 'Combines a list of JSON objects into one JSON object.',\n",
       " \"Checks that dict has only the appropriate keys for orient='split'.\",\n",
       " 'Take a conversion function and possibly recreate the frame.',\n",
       " 'Return a formatter function for a range of timedeltas.\\n    These will all have the same format argument\\n\\n    If box, then show the return in quotes',\n",
       " 'Separates the real and imaginary parts from the complex number, and\\n    executes the _trim_zeros_float method on each of those.',\n",
       " 'Trims zeros, leaving just one before the decimal points if need be.',\n",
       " 'Alter default behavior on how float is formatted in DataFrame.\\n    Format float in engineering format. By accuracy, we mean the number of\\n    decimal digits after the floating point.\\n\\n    See also EngFormatter.',\n",
       " 'For each index in each level the function returns lengths of indexes.\\n\\n    Parameters\\n    ----------\\n    levels : list of lists\\n        List of values on for level.\\n    sentinel : string, optional\\n        Value which states that no new index starts on there.\\n\\n    Returns\\n    ----------\\n    Returns list of maps. For each level returns map of indexes (key is index\\n    in row and value is length of index).',\n",
       " 'Appends lines to a buffer.\\n\\n    Parameters\\n    ----------\\n    buf\\n        The buffer to write to\\n    lines\\n        The lines to append.',\n",
       " 'Calculate display width considering unicode East Asian Width',\n",
       " 'Render a DataFrame to a list of columns (as lists of strings).',\n",
       " 'Render a DataFrame to a console-friendly tabular output.',\n",
       " 'Render a DataFrame to a LaTeX tabular/longtable environment output.',\n",
       " 'Returns a function to be applied on each value to format it',\n",
       " 'Returns the float values converted into strings using\\n        the parameters given at initialisation, as a numpy array',\n",
       " 'Given an Interval or IntervalIndex, return the corresponding interval with\\n    closed bounds.',\n",
       " 'helper for interval_range to check if start/end are valid types',\n",
       " 'helper for interval_range to check type compat of start/end/freq',\n",
       " \"Provide method name lookup and completion\\n        Only provide 'public' methods.\",\n",
       " \"Add accessors to cls from the delegate class.\\n\\n        Parameters\\n        ----------\\n        cls : the class to add the methods/properties to\\n        delegate : the class to get methods/properties & doc-strings\\n        accessors : string list of accessors to add\\n        typ : 'property' or 'method'\\n        overwrite : boolean, default False\\n           overwrite the method/property in the target class if it exists.\",\n",
       " 'evaluate and return the expression of the op on a and b\\n\\n        Parameters\\n        ----------\\n\\n        op :    the actual operand\\n        op_str: the string version of the op\\n        a :     left operand\\n        b :     right operand\\n        use_numexpr : whether to try to use numexpr (default True)',\n",
       " 'evaluate the where condition cond on a and b\\n\\n        Parameters\\n        ----------\\n\\n        cond : a boolean array\\n        a :    return if cond is True\\n        b :    return if cond is False\\n        use_numexpr : whether to try to use numexpr (default True)',\n",
       " 'Convert CSS declarations to ExcelWriter style\\n\\n        Parameters\\n        ----------\\n        declarations_str : str\\n            List of CSS declarations.\\n            e.g. \"font-weight: bold; background: blue\"\\n\\n        Returns\\n        -------\\n        xlstyle : dict\\n            A style as interpreted by ExcelWriter when found in\\n            ExcelCell.style.',\n",
       " 'Write a DataFrame to the feather-format\\n\\n    Parameters\\n    ----------\\n    df : DataFrame\\n    path : string file path, or file-like object',\n",
       " 'A special case for _generate_range_overflow_safe where `periods * stride`\\n    can be calculated without overflowing int64 bounds.',\n",
       " 'Check to see if we can set a locale, and subsequently get the locale,\\n    without raising an Exception.\\n\\n    Parameters\\n    ----------\\n    lc : str\\n        The locale to attempt to set.\\n    lc_var : int, default `locale.LC_ALL`\\n        The category of the locale being set.\\n\\n    Returns\\n    -------\\n    is_valid : bool\\n        Whether the passed locale can be set',\n",
       " 'Return a list of normalized locales that do not throw an ``Exception``\\n    when set.\\n\\n    Parameters\\n    ----------\\n    locales : str\\n        A string where each locale is separated by a newline.\\n    normalize : bool\\n        Whether to call ``locale.normalize`` on each locale.\\n\\n    Returns\\n    -------\\n    valid_locales : list\\n        A list of valid locales.',\n",
       " 'Ensure that an array object has a float dtype if possible.\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array whose data type we want to enforce as float.\\n\\n    Returns\\n    -------\\n    float_arr : The original array cast to the float dtype if\\n                possible. Otherwise, the original array is returned.',\n",
       " 'Ensure that an array-like object is a Categorical (if not already).\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array that we want to convert into a Categorical.\\n\\n    Returns\\n    -------\\n    cat_arr : The original array cast as a Categorical. If it already\\n              is a Categorical, we return as is.',\n",
       " 'evaluate if the tipo is a subclass of the klasses\\n    and not a datetimelike',\n",
       " 'Check whether an array-like is a periodical index.\\n\\n    .. deprecated:: 0.24.0\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array-like to check.\\n\\n    Returns\\n    -------\\n    boolean\\n        Whether or not the array-like is a periodical index.\\n\\n    Examples\\n    --------\\n    >>> is_period([1, 2, 3])\\n    False\\n    >>> is_period(pd.Index([1, 2, 3]))\\n    False\\n    >>> is_period(pd.PeriodIndex([\"2017-01-01\"], freq=\"D\"))\\n    True',\n",
       " 'Check whether an array-like is a datetime array-like or DatetimeIndex.\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array-like to check.\\n\\n    Returns\\n    -------\\n    boolean\\n        Whether or not the array-like is a datetime array-like or\\n        DatetimeIndex.\\n\\n    Examples\\n    --------\\n    >>> is_datetime_arraylike([1, 2, 3])\\n    False\\n    >>> is_datetime_arraylike(pd.Index([1, 2, 3]))\\n    False\\n    >>> is_datetime_arraylike(pd.DatetimeIndex([1, 2, 3]))\\n    True',\n",
       " 'Return a boolean if the condition is satisfied for the arr_or_dtype.\\n\\n    Parameters\\n    ----------\\n    arr_or_dtype : array-like, str, np.dtype, or ExtensionArrayType\\n        The array-like or dtype object whose dtype we want to extract.\\n    condition : callable[Union[np.dtype, ExtensionDtype]]\\n\\n    Returns\\n    -------\\n    bool',\n",
       " 'Get the dtype instance associated with an array\\n    or dtype object.\\n\\n    Parameters\\n    ----------\\n    arr_or_dtype : array-like\\n        The array-like or dtype object whose dtype we want to extract.\\n\\n    Returns\\n    -------\\n    obj_dtype : The extract dtype instance from the\\n                passed in array or dtype object.\\n\\n    Raises\\n    ------\\n    TypeError : The passed in object is None.',\n",
       " 'Return a boolean if the condition is satisfied for the arr_or_dtype.\\n\\n    Parameters\\n    ----------\\n    arr_or_dtype : array-like\\n        The array-like or dtype object whose dtype we want to extract.\\n    condition : callable[Union[np.dtype, ExtensionDtypeType]]\\n\\n    Returns\\n    -------\\n    bool : if the condition is satisifed for the arr_or_dtype',\n",
       " 'Get a numpy dtype.type-style object for a dtype object.\\n\\n    This methods also includes handling of the datetime64[ns] and\\n    datetime64[ns, TZ] objects.\\n\\n    If no dtype can be found, we return ``object``.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, type\\n        The dtype object whose numpy dtype.type-style\\n        object we want to extract.\\n\\n    Returns\\n    -------\\n    dtype_object : The extracted numpy dtype.type-style object.',\n",
       " 'Check whether the dtype is a date-like dtype. Raises an error if invalid.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, type\\n        The dtype to check.\\n\\n    Raises\\n    ------\\n    TypeError : The dtype could not be casted to a date-like dtype.\\n    ValueError : The dtype is an illegal date-like dtype (e.g. the\\n                 the frequency provided is too specific)',\n",
       " 'Convert input into a pandas only dtype object or a numpy dtype object.\\n\\n    Parameters\\n    ----------\\n    dtype : object to be converted\\n\\n    Returns\\n    -------\\n    np.dtype or a pandas dtype\\n\\n    Raises\\n    ------\\n    TypeError if not a dtype',\n",
       " 'groupby & merge; we are always performing a left-by type operation\\n\\n    Parameters\\n    ----------\\n    by: field to group\\n    on: duplicates field\\n    left: left frame\\n    right: right frame\\n    _merge_pieces: function for merging\\n    check_duplicates: boolean, default True\\n        should we check & clean duplicates',\n",
       " \"Parameters\\n    ----------\\n    left_keys: ndarray, Index, Series\\n    right_keys: ndarray, Index, Series\\n    sort: boolean, default False\\n    how: string {'inner', 'outer', 'left', 'right'}, default 'inner'\\n\\n    Returns\\n    -------\\n    tuple of (left_indexer, right_indexer)\\n        indexers into the left_keys, right_keys\",\n",
       " 'Create a join index by rearranging one index to match another\\n\\n        Parameters\\n        ----------\\n        index: Index being rearranged\\n        other_index: Index used to supply values not found in index\\n        indexer: how to rearrange index\\n        how: replacement is only necessary if indexer based on other_index\\n\\n        Returns\\n        -------\\n        join_index',\n",
       " 'Note: has side effects (copy/delete key columns)\\n\\n        Parameters\\n        ----------\\n        left\\n        right\\n        on\\n\\n        Returns\\n        -------\\n        left_keys, right_keys',\n",
       " \"Check whether 'other' is equal to self.\\n\\n        By default, 'other' is considered equal if either\\n\\n        * it's a string matching 'self.name'.\\n        * it's an instance of this type and all of the\\n          the attributes in ``self._metadata`` are equal between\\n          `self` and `other`.\\n\\n        Parameters\\n        ----------\\n        other : Any\\n\\n        Returns\\n        -------\\n        bool\",\n",
       " 'Auxiliary function for :meth:`str.cat`\\n\\n    Parameters\\n    ----------\\n    list_of_columns : list of numpy arrays\\n        List of arrays to be concatenated with sep;\\n        these arrays may not contain NaNs!\\n    sep : string\\n        The separator string for concatenating the columns\\n\\n    Returns\\n    -------\\n    nd.array\\n        The concatenation of list_of_columns with sep',\n",
       " 'Find groups in each string in the Series using passed regular\\n    expression. This function is called from\\n    str_extract(expand=False), and can return Series, DataFrame, or\\n    Index.',\n",
       " 'For each subject string in the Series, extract groups from the\\n    first match of regular expression pat. This function is called from\\n    str_extract(expand=True), and always returns a DataFrame.',\n",
       " \"Strip whitespace (including newlines) from each string in the\\n    Series/Index.\\n\\n    Parameters\\n    ----------\\n    to_strip : str or unicode\\n    side : {'left', 'right', 'both'}, default 'both'\\n\\n    Returns\\n    -------\\n    Series or Index\",\n",
       " 'Decode character string in the Series/Index using indicated encoding.\\n    Equivalent to :meth:`str.decode` in python2 and :meth:`bytes.decode` in\\n    python3.\\n\\n    Parameters\\n    ----------\\n    encoding : str\\n    errors : str, optional\\n\\n    Returns\\n    -------\\n    Series or Index',\n",
       " 'Encode character string in the Series/Index using indicated encoding.\\n    Equivalent to :meth:`str.encode`.\\n\\n    Parameters\\n    ----------\\n    encoding : str\\n    errors : str, optional\\n\\n    Returns\\n    -------\\n    encoded : Series/Index of objects',\n",
       " 'Copy a docstring from another source function (if present)',\n",
       " \"Return the Unicode normal form for the strings in the Series/Index.\\n        For more information on the forms, see the\\n        :func:`unicodedata.normalize`.\\n\\n        Parameters\\n        ----------\\n        form : {'NFC', 'NFKC', 'NFD', 'NFKD'}\\n            Unicode form\\n\\n        Returns\\n        -------\\n        normalized : Series/Index of objects\",\n",
       " 'Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'Raise exception with existing traceback.\\n    If traceback is not passed, uses sys.exc_info() to get traceback.',\n",
       " 'converts a style_dict to an openpyxl style object\\n        Parameters\\n        ----------\\n        style_dict : style dictionary to convert',\n",
       " \"Convert ``color_spec`` to an openpyxl v2 Color object\\n        Parameters\\n        ----------\\n        color_spec : str, dict\\n            A 32-bit ARGB hex string, or a dict with zero or more of the\\n            following keys.\\n                'rgb'\\n                'indexed'\\n                'auto'\\n                'theme'\\n                'tint'\\n                'index'\\n                'type'\\n        Returns\\n        -------\\n        color : openpyxl.styles.Color\",\n",
       " \"Convert ``side_spec`` to an openpyxl v2 Side object\\n        Parameters\\n        ----------\\n        side_spec : str, dict\\n            A string specifying the border style, or a dict with zero or more\\n            of the following keys (or their synonyms).\\n                'style' ('border_style')\\n                'color'\\n        Returns\\n        -------\\n        side : openpyxl.styles.Side\",\n",
       " 'construct and return a row or column based frame apply object',\n",
       " 'we have an empty result; at least 1 axis is 0\\n\\n        we will try to apply the function to an empty\\n        series in order to see if this is a reduction function',\n",
       " 'infer the results to the same shape as the input object',\n",
       " \"Numpy version of itertools.product.\\n    Sometimes faster (for large inputs)...\\n\\n    Parameters\\n    ----------\\n    X : list-like of list-likes\\n\\n    Returns\\n    -------\\n    product : list of ndarrays\\n\\n    Examples\\n    --------\\n    >>> cartesian_product([list('ABC'), [1, 2]])\\n    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='|S1'),\\n    array([1, 2, 1, 2, 1, 2])]\\n\\n    See Also\\n    --------\\n    itertools.product : Cartesian product of input iterables.  Equivalent to\\n        nested for-loops.\",\n",
       " 'str->list\\n    Convert XML to URL List.\\n    From Biligrab.',\n",
       " 'From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110',\n",
       " 'Downloads a Sina video by its unique vid.\\n    http://video.sina.com.cn/',\n",
       " 'Downloads a Sina video by its unique vkey.\\n    http://video.sina.com/',\n",
       " 'self, str->None\\n        \\n        Keyword arguments:\\n        self: self\\n        vid: The video ID for BokeCC cloud, something like\\n        FE3BB999594978049C33DC5901307461\\n        \\n        Calls the prepare() to download the video.\\n        \\n        If no title is provided, this method shall try to find a proper title\\n        with the information providin within the\\n        returned content of the API.',\n",
       " 'Format text with color or other effects into ANSI escaped string.',\n",
       " 'str->dict\\n    Information for CKPlayer API content.',\n",
       " 'Splicing URLs according to video ID to get video details',\n",
       " 'Override the original one\\n        Ugly ugly dirty hack',\n",
       " 'str, str, str, bool, bool ->None\\n\\n    Download Acfun video by vid.\\n\\n    Call Acfun API, decide which site to use, and pass the job to its\\n    extractor.',\n",
       " 'str, str->True\\n    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!\\n    You can basicly download anything with this function\\n    but better leave it alone with',\n",
       " 'Scans through a string for substrings matched some patterns (first-subgroups only).\\n\\n    Args:\\n        text: A string to be scanned.\\n        patterns: Arbitrary number of regex patterns.\\n\\n    Returns:\\n        When only one pattern is given, returns a string (None if no match found).\\n        When more than one pattern are given, returns a list of strings ([] if no match found).',\n",
       " 'Scans through a string for substrings matched some patterns.\\n\\n    Args:\\n        text: A string to be scanned.\\n        patterns: a list of regex pattern.\\n\\n    Returns:\\n        a list if matched. empty if not.',\n",
       " 'Parses the query string of a URL and returns the value of a parameter.\\n\\n    Args:\\n        url: A URL.\\n        param: A string representing the name of the parameter.\\n\\n    Returns:\\n        The value of the parameter.',\n",
       " 'Decompresses data for Content-Encoding: deflate.\\n    (the zlib compression is used.)',\n",
       " 'Gets the content of a URL via sending a HTTP GET request.\\n\\n    Args:\\n        url: A URL.\\n        headers: Request headers used by the client.\\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\\n\\n    Returns:\\n        The content as a string.',\n",
       " 'Post the content of a URL via sending a HTTP POST request.\\n\\n    Args:\\n        url: A URL.\\n        headers: Request headers used by the client.\\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\\n\\n    Returns:\\n        The content as a string.',\n",
       " \"Overload default print function as py (<3.3) does not support 'flush' keyword.\\n    Although the function name can be same as print to get itself overloaded automatically,\\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\",\n",
       " 'JSON, int, int, int->str\\n    \\n    Get a proper title with courseid+topicID+partID.',\n",
       " 'int->None\\n    \\n    Download a WHOLE course.\\n    Reuse the API call to save time.',\n",
       " 'int, int, int->None\\n    \\n    Download ONE PART of the course.',\n",
       " 'int, int->list\\n        \\n        Get the height of the videos.\\n        \\n        Since brightcove is using 3 kinds of links: rtmp, http and https,\\n        we will be using the HTTPS one to make it secure.\\n        \\n        If somehow akamaihd.net is blocked by the Great Fucking Wall,\\n        change the \"startswith https\" to http.',\n",
       " 'Preview version of Xception network. Not tested yet - use at own risk. No pretrained model yet.',\n",
       " 'Method returns a RNN_Learner object, that wraps an instance of the RNN_Encoder module.\\n\\n        Args:\\n            opt_fn (Optimizer): the torch optimizer function to use\\n            emb_sz (int): embedding size\\n            n_hid (int): number of hidden inputs\\n            n_layers (int): number of hidden layers\\n            kwargs: other arguments\\n\\n        Returns:\\n            An instance of the RNN_Learner class.',\n",
       " 'Return list of files in `path` that have a suffix in `extensions`; optionally `recurse`.',\n",
       " 'Load an empty `DataBunch` from the exported file in `path/fname` with optional `tfms`.',\n",
       " 'Reconstruct one of the underlying item for its data `t`.',\n",
       " 'Create a new `ItemList` from `items`, keeping the same attributes.',\n",
       " 'Create an `ItemList` in `path` from the filenames that have a suffix in `extensions`.\\n        `recurse` determines if we search subfolders.',\n",
       " 'Create an `ItemList` in `path` from the inputs in the `cols` of `df`.',\n",
       " 'Create an `ItemList` in `path` from the inputs in the `cols` of `path/csv_name`',\n",
       " 'Use only a sample of `sample_pct`of the full dataset and an optional `seed`.',\n",
       " 'Only keep elements for which `func` returns `True`.',\n",
       " 'Only keep filenames in `include` folder or reject the ones in `exclude`.',\n",
       " 'Keep random sample of `items` with probability `p` and an optional `seed`.',\n",
       " \"Don't split the data and create an empty validation set.\",\n",
       " 'Split the data between `train_idx` and `valid_idx`.',\n",
       " 'Split the data according to the indexes in `valid_idx`.',\n",
       " 'Split the data depending on the folder (`train` or `valid`) in which the filenames are.',\n",
       " 'Split the items randomly by putting `valid_pct` in the validation set, optional `seed` can be passed.',\n",
       " 'Split the items into train set with size `train_size * n` and valid set with size `valid_size * n`.',\n",
       " 'Split the data by result of `func` (which returns `True` for validation set).',\n",
       " 'Split the data by using the names in `valid_names` for validation.',\n",
       " 'Split the data by using the names in `fname` for the validation set. `path` will override `self.path`.',\n",
       " 'Split the data from the `col` in the dataframe in `self.inner_df`.',\n",
       " 'Return `label_cls` or guess one from the first element of `labels`.',\n",
       " 'Label `self.items` from the values in `cols` in `self.inner_df`.',\n",
       " 'Give a label to each filename depending on its folder.',\n",
       " 'Apply the re in `pat` to determine the label of every filename.  If `full_path`, search in the full name.',\n",
       " 'Generate classes from `items` by taking the sorted unique values.',\n",
       " 'Use the labels in `train_labels` and `valid_labels` to label the data. `label_cls` will overwrite the default.',\n",
       " 'Set `tfms` to be applied to the xs of the train and validation set.',\n",
       " 'Set `tfms` to be applied to the ys of the train and validation set.',\n",
       " 'Read the default class processors if none have been set.',\n",
       " 'Create an `DataBunch` from self, `path` will override `self.path`, `kwargs` are passed to `DataBunch.create`.',\n",
       " 'Add test set containing `items` with an arbitrary `label`.',\n",
       " 'Add test set containing items from `test_folder` and an arbitrary `label`.',\n",
       " 'Create a `LabelLists` with empty sets from the serialized `state`.',\n",
       " 'Create a `LabelLists` with empty sets from the serialized file in `path/fn`.',\n",
       " 'For inference, will briefly replace the dataset with one that only contains `item`.',\n",
       " 'Create `pd.DataFrame` containing `items` from `self.x` and `self.y`.',\n",
       " 'Save `self.to_df()` to a CSV file in `self.path`/`dest`.',\n",
       " 'Export the minimal state and save it in `fn` to load an empty version for inference.',\n",
       " 'Load the state in `fn` to create an empty `LabelList` for inference.',\n",
       " 'Launch the processing on `self.x` and `self.y` with `xp` and `yp`.',\n",
       " 'Set the `tfms` and `tfm_y` value to be applied to the inputs and targets.',\n",
       " 'Create a new `ItemList` from `items`, keeping the same attributes.',\n",
       " 'Parse the docstring into its components.\\n\\n    :return: a dictionary of form\\n              {\\n                  \"short_description\": ...,\\n                  \"long_description\": ...,\\n                  \"params\": [{\"name\": ..., \"doc\": ...}, ...],\\n                  \"vals\": [{\"name\": ..., \"doc\": ...}, ...],\\n                  \"return\": ...\\n              }',\n",
       " \"Return env var value if it's defined and not an empty string, or return Unknown\",\n",
       " 'Suggest how to improve the setup to speed things up',\n",
       " 'Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.',\n",
       " 'Exponentially anneal from `start` to `end` as pct goes from 0.0 to 1.0.',\n",
       " 'Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.',\n",
       " 'Create an `optim.Optimizer` from `opt_func` with `lr`. Set lr on `layer_groups`.',\n",
       " 'Create a new `OptimWrapper` from `self` with another `layer_groups` but the same hyper-parameters.',\n",
       " 'Create a new `OptimWrapper` from `self` with another `layer_groups` but the same hyper-parameters.',\n",
       " 'Set beta (or alpha as makes sense for given optimizer).',\n",
       " 'Read the values inside the optimizer for the hyper-parameters.',\n",
       " 'Set `val` inside the optimizer dictionary at `key`.',\n",
       " 'Read a hyperparameter `key` in the optimizer dictionary.',\n",
       " 'Return the inner state of the `Callback`, `minimal` or not.',\n",
       " 'Update metric computation with `last_output` and `last_target`.',\n",
       " 'Initialize our optimization params based on our annealing schedule.',\n",
       " 'Take one step forward on the annealing schedule for the optim params.',\n",
       " 'Distributed training of Imagenette.\\n    Fastest multi-gpu speed is if you run with: python -m fastai.launch',\n",
       " 'Download a dataset.\\n\\n        Args:\\n            save_path : str\\n                A directory to save the data to.\\n            dataset : str, optional\\n                A specific dataset to download.\\n                Note: this must include the file extension.\\n                If None, options will be presented for you\\n                to choose from.\\n\\n        Returns:\\n            save_path_full : str\\n                The absolute path to the downloaded data.',\n",
       " 'A basic critic for images `n_channels` x `in_size` x `in_size`.',\n",
       " 'A basic generator from `noise_sz` to images `n_channels` x `in_size` x `in_size`.',\n",
       " 'Define loss functions for a GAN from `loss_gen` and `loss_crit`.',\n",
       " 'Compute accuracy after expanding `y_true` to the size of `y_pred`.',\n",
       " 'Put the model in generator mode if `gen_mode`, in critic mode otherwise.',\n",
       " 'Evaluate the `output` with the critic then uses `self.loss_funcG` to combine it with `target`.',\n",
       " 'Create some `fake_pred` with the generator from `input` and compare them to `real_pred` in `self.loss_funcD`.',\n",
       " 'Create the optimizers for the generator and critic if necessary, initialize smootheners.',\n",
       " \"Clamp the weights with `self.clip` if it's not None, return the correct input.\",\n",
       " 'Put the various losses in the recorder and show a sample image.',\n",
       " 'Switch the model, if `gen_mode` is provided, in the desired mode.',\n",
       " 'Create a WGAN from `data`, `generator` and `critic`.',\n",
       " 'Shows `ys` (target images) on a figure of `figsize`.',\n",
       " 'Get the indexes of the layers where the size of the activation changes.',\n",
       " 'Search for `n_images` images on Google, matching `search_term` and `size` requirements,\\n    download them into `path`/`search_term` and verify them, using `max_workers` threads.',\n",
       " 'Build Google Images Search Url params and return them as a string.',\n",
       " 'Return a Google Images Search URL for a given search term.',\n",
       " 'Parse the Google Images Search for urls and return the image metadata as tuples (fname, url).',\n",
       " 'Parse the google images html to img tuples containining `(fname, url)`',\n",
       " 'Parse the Google Images Search for urls and return the image metadata as tuples (fname, url).\\n    Use this for downloads of >100 images. Requires `selenium`.',\n",
       " \"Downloads images in `img_tuples` to `label_path`. \\n    If the directory doesn't exist, it'll be created automatically.\\n    Uses `parallel` to speed things up in `max_workers` when the system has enough CPU cores.\\n    If something doesn't work, try setting up `max_workers=0` to debug.\",\n",
       " 'Downloads a single image from Google Search results to `label_path`\\n    given an `img_tuple` that contains `(fname, url)` of an image to download.\\n    `i` is just an iteration number `int`.',\n",
       " 'Setup path to save images to, init the UI, and render the widgets.',\n",
       " 'Download button click handler: validate search term and download images.',\n",
       " 'Cleanup learn model weights disturbed during LRFinder exploration.',\n",
       " 'Default constructor for the WeightDrop module\\n\\n        Args:\\n            module (torch.nn.Module): A pytorch layer being wrapped\\n            dropout (float): a dropout value to apply\\n            weights (list(str)): the parameters of the wrapped **module**\\n                which should be fractionally dropped.',\n",
       " 'for each string defined in self.weights, the corresponding\\n        attribute in the wrapped module is referenced, then deleted, and subsequently\\n        registered as a new parameter with a slightly modified name.\\n\\n        Args:\\n            None\\n\\n         Returns:\\n             None',\n",
       " \"Uses pytorch's built-in dropout function to apply dropout to the parameters of\\n        the wrapped module.\\n\\n        Args:\\n            None\\n        Returns:\\n            None\",\n",
       " 'Load a saved `DataBunch` from `path/file`. `file` can be file-like (file or buffer)',\n",
       " 'Create a `DataBunch` from `train_ds`, `valid_ds` and maybe `test_ds` with a batch size of `bs`. Passes `**dl_kwargs` to `DataLoader()`',\n",
       " 'Returns appropriate `Dataset` for validation, training, or test (`ds_type`).',\n",
       " 'Returns a list of all DeviceDataLoaders. If you need a specific DeviceDataLoader, access via the relevant property (`train_dl`, `valid_dl`, etc) as the index of DLs in this list is not guaranteed to remain constant.',\n",
       " 'Save the `DataBunch` in `self.path/file`. `file` can be file-like (file or buffer)',\n",
       " 'Add the `items` as a test set. Pass along `label` otherwise label them with `EmptyLabel`.',\n",
       " 'Get one batch from the data loader of `ds_type`. Optionally `detach` and `denorm`.',\n",
       " 'Get `item` into a batch. Optionally `detach` and `denorm`.',\n",
       " 'Export the minimal state of `self` for inference in `self.path/file`. `file` can be file-like (file or buffer)',\n",
       " 'Check the underlying data in the training set can be properly loaded.',\n",
       " 'Explore lr from `start_lr` to `end_lr` over `num_it` iterations in `learn`. If `stop_div`, stops when loss diverges.',\n",
       " 'Add mixup https://arxiv.org/abs/1710.09412 to `learn`.',\n",
       " 'Create a `ClassificationInterpretation` object from `learner` on `ds_type` with `tta`.',\n",
       " 'If we have `last_metrics` plot them in our pbar graph',\n",
       " 'accumulated step and reset samples, True will result in no stepping',\n",
       " 'step the rest of the accumulated grads if not perfectly divisible',\n",
       " 'Create an instance of `ClassificationInterpretation`',\n",
       " 'Plot the confusion matrix, with `title` and using `cmap`.',\n",
       " 'Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.',\n",
       " '`k` largest(/smallest) losses and indexes, defaulting to all losses (sorted by `largest`).',\n",
       " 'Calculates the F-beta score (the weighted harmonic mean of precision and recall).\\n    This is the micro averaged version where the true positives, false negatives and\\n    false positives are calculated globally (as opposed to on a per label basis).\\n\\n    beta == 1 places equal weight on precision and recall, b < 1 emphasizes precision and\\n    beta > 1 favors recall.',\n",
       " 'Distributed training of Imagenet. Fastest speed is if you run with: python -m fastai.launch',\n",
       " 'Cut off the body of a typically pretrained `model` at `cut` (int) or cut the model as specified by `cut(model)` (function).',\n",
       " 'Model head that takes `nf` features, runs through `lin_ftrs`, and about `nc` classes.',\n",
       " 'Create an instance of `ClassificationInterpretation`. `tta` indicates if we want to use Test Time Augmentation.',\n",
       " 'Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.',\n",
       " 'Show images in `top_losses` along with their prediction, actual, loss, and probability of predicted class in a multilabeled dataset.',\n",
       " 'Sorts `ds_type` dataset by top losses and returns dataset and sorted indices.',\n",
       " 'For a LabelList `ll_input`, resize each image to `size` using `resize_method` and `padding_mode`.',\n",
       " 'Gets the indices for the most similar images in `ds_type` dataset',\n",
       " 'Gets activations at the layer specified by `hook`, applies `pool` of dim `pool_dim` and concatenates',\n",
       " 'Computes the similarity function between each embedding of `t1` and `t2` matrices.',\n",
       " 'Returns the `n` largest indices from a numpy array `arr`.',\n",
       " 'Sorts `similarities` and return the indexes in pairs ordered by highest similarity.',\n",
       " 'Returns an image widget for specified file name `img`.',\n",
       " 'Make a horizontal box with `children` and `layout`.',\n",
       " 'Create a list of images, filenames and labels but first removing files that are not supposed to be displayed.',\n",
       " 'Relabel images by moving from parent dir with old label `class_old` to parent dir with new label `class_new`.',\n",
       " \"Handler for 'Next Batch' button click. Delete all flagged images and renders next batch.\",\n",
       " 'Check if current batch contains already deleted images.',\n",
       " 'Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Args:\\n            img (Tensor): Tensor image of size (C, H, W).\\n        Returns:\\n            Tensor: Image with n_holes of dimension length x length cut out of it.',\n",
       " 'Store all collected nbval tests for evaluation on finish',\n",
       " 'Make report in form of two notebooks.\\n\\n        Use nbdime diff-web to present the difference between reference\\n        cells and test cells.',\n",
       " \"BatchNorm layers to have parameters in single precision.\\n    Find all layers and convert them back to float. This can't\\n    be done with built in .apply as that function will apply\\n    fn to all modules, parameters, and buffers. Thus we wouldn't\\n    be able to guard the float conversion based on the module type.\",\n",
       " 'Creates a fp32 copy of model parameters and sets optimizer parameters',\n",
       " 'Start coverage reporting in kernel.\\n\\n    Currently supported kernel languages are:\\n     - Python',\n",
       " 'Finish coverage reporting in kernel.\\n\\n    The coverage should previously have been started with\\n    setup_coverage.',\n",
       " 'Create a suffix for nbval data file depending on pytest-cov config.',\n",
       " 'Convert `b` to an int or list of ints (if `is_listy`); raises exception if not convertible',\n",
       " 'List of label subdirectories in imagenet-style `folder`.',\n",
       " 'Given `arrs` is [a,b,...] and `mask`index - return[(a[mask],a[~mask]),(b[mask],b[~mask]),...].',\n",
       " 'Randomly split `arrs` with `valid_pct` ratio. good for creating validation set.',\n",
       " 'Build log-stepped array from `start` to `stop` in `n` steps.',\n",
       " 'Download `url` to `dest` unless it exists and not `overwrite`.',\n",
       " 'Return `Path(path)/Path(fname)`, `path` defaults to current dir.',\n",
       " 'Return `ndarray` of `str` of lines of text from `path`.',\n",
       " 'Split `kwargs` between those expected by `func` and the others.',\n",
       " 'Same as `np.array` but also handles generators. `kwargs` are passed to `np.array` with `dtype`.',\n",
       " 'Put the texts in `items` in an HTML table, `widths` are the widths of the columns in %.',\n",
       " 'Call `func` on every element of `arr` in parallel using `max_workers`.',\n",
       " 'Like `plt.subplots` but with consistent axs shape, `kwargs` passed to `fig.suptitle` with `title`',\n",
       " 'Return the representation of the first  `n_max` elements in `items`.',\n",
       " 'Create and return a tmp filename, optionally at a specific path. `os.remove` when done with it.',\n",
       " 'Subclass this method if you want to customize the way this `ItemBase` is shown on `ax`.',\n",
       " 'Create a seuence Conv2d->BatchNorm2d->LeakyReLu layer.',\n",
       " 'starts with conv layer - `ch_in` channels in - then has `num_blocks` `ResLayer`',\n",
       " 'Create a Learner for collaborative filtering on `data`.',\n",
       " 'Create a `DataBunch` suitable for collaborative filtering from `ratings`.',\n",
       " 'Fetch item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)',\n",
       " 'Bias for item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)',\n",
       " 'Bias for item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)',\n",
       " 'Draws a representation of a random forest in IPython.\\n    Parameters:\\n    -----------\\n    t: The tree you wish to draw\\n    df: The data used to train the tree. This is used to get the names of the features.',\n",
       " \"Changes Scikit learn's random forests to give each tree a random sample of\\n    n random rows.\",\n",
       " 'Execute notebook `fname` with `metadata` for preprocessing.',\n",
       " 'Create the documentation notebook for module `mod_name` in path `dest_path`',\n",
       " 'Search a given `path_dir` and return all the modules contained inside except those in `exclude`',\n",
       " 'Read a notebook in `fname` and return its corresponding json',\n",
       " 'Build a dictionary containing the position of the `cells`.',\n",
       " 'Create documentation links for all cells in markdown with backticks.',\n",
       " 'Return the position to insert a given function doc in a notebook.',\n",
       " 'Update the `pos_dict` by moving all positions after `start_key` by `nbr`.',\n",
       " 'Insert the function doc `cells` at their correct position and updates `pos_dict`.',\n",
       " 'Finds all submodules of notebook - sorted by submodules > top level modules > manual imports. This gives notebook imports priority',\n",
       " 'Update the documentation notebook of a given module.',\n",
       " '`source_path` can be a directory or a file. Assume all modules reside in the fastai directory.',\n",
       " 'Return a dropout mask of the same type as `x`, size `sz`, with probability `p` to cancel an element.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Convert a value `x` from 0 to 1 (inclusive) to an RGBA tuple according to `cmap` times transparency `alpha_mult`.',\n",
       " 'Calculate the intrinsic attention of the input w.r.t to an output `class_id`, or the classification given by the model if `None`.\\n        For reference, see the Sequential Jacobian session at https://www.cs.toronto.edu/~graves/preprint.pdf',\n",
       " 'Create a tabulation showing the first `k` texts in top_losses along with their prediction, actual,loss, and probability of\\n        actual class. `max_len` is the maximum number of tokens displayed.',\n",
       " 'Take a step in lr,mom sched, start next stepper when the current one is complete.',\n",
       " 'Like `torch.as_tensor`, but handle lists too, and can pass multiple vector elements directly.',\n",
       " 'Recursively detach lists of tensors in `b `; put them on the CPU if `cpu=True`.',\n",
       " 'Recursively map lists of items in `b ` to their wrapped data.',\n",
       " 'Recursively map lists of tensors in `b ` to the cpu.',\n",
       " 'If `b` is not set return `requires_grad` of first param, else set `requires_grad` on all params as `b`',\n",
       " 'Return the children of `m` and its direct parameters not registered in modules.',\n",
       " 'Separate the parameters in `layer_groups` between `no_wd_types` and  bias (`bias_types`) from the rest.',\n",
       " 'Set bn layers in eval mode for all recursive children of `m`.',\n",
       " 'Initialize `m` weights with `func` and set `bias` to 0.',\n",
       " 'Initialize the non-batchnorm layers of `m` with `init_func`.',\n",
       " 'Initialize all non-batchnorm layers of `m` with `init_func`.',\n",
       " 'Tranform numpy array `a` to a tensor of the same type.',\n",
       " 'Grab the `i`-th batch in `x`, `batch_first` stating the batch dimension.',\n",
       " 'Draw 1 or shape=`size` random floats from uniform dist: min=`low`, max=`high`.',\n",
       " 'Draw 1 or shape=`size` random floats from uniform dist: min=log(`low`), max=log(`high`).',\n",
       " 'Draw 1 or shape=`size` random booleans (`True` occuring with probability `p`).',\n",
       " 'Generate int or tensor `size` of ints between `low` and `high` (included).',\n",
       " 'Try to convert `o` to int, default to `o` if not possible.',\n",
       " 'Check that `out` and `targ` have the same number of elements and flatten them.',\n",
       " 'create new OrderedDict that does not contain `module.`',\n",
       " 'Return a dictionary for updating `last_metrics` with `mets`.',\n",
       " 'Collects iterables lazily, rather than immediately.\\n        Docstring same as parent: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor\\n        Implmentation taken from this PR: https://github.com/python/cpython/pull/707',\n",
       " 'Generate documentation for fastai library in HTML (asciidoctor required)\\n    :param str src: The absolute/relative path of source file/dir',\n",
       " 'Retrieves new batch of DatasetType, and detaches it.',\n",
       " 'one_batch function is extremely slow with large datasets.  This is caching the result as an optimization.',\n",
       " 'Callback function that writes batch end appropriate data to Tensorboard.',\n",
       " 'Callback function that writes backward end appropriate data to Tensorboard.',\n",
       " 'Callback function that writes epoch end appropriate data to Tensorboard.',\n",
       " 'Writes gradient statistics for generator to Tensorboard.',\n",
       " 'Writes gradient statistics for critic to Tensorboard.',\n",
       " 'Writes model generated, original and real images to Tensorboard.',\n",
       " 'Callback function that writes batch end appropriate data to Tensorboard.',\n",
       " 'Callback function that writes backward end appropriate data to Tensorboard.',\n",
       " 'Writes model generated, original and real images to Tensorboard',\n",
       " 'Queues up an asynchronous write request to Tensorboard.',\n",
       " 'Processes queued up write requests asynchronously to Tensorboard.',\n",
       " 'Factory method to convert a batch of model images to a list of ModelImageSet.',\n",
       " 'Writes a single scalar value for a gradient statistic to Tensorboard.',\n",
       " 'Writes the average norm of the gradients to Tensorboard.',\n",
       " 'Writes the median norm of the gradients to Tensorboard.',\n",
       " 'Writes the maximum norm of the gradients to Tensorboard.',\n",
       " 'Writes the minimum norm of the gradients to Tensorboard.',\n",
       " 'Writes the number of zeroes in the gradients to Tensorboard.',\n",
       " 'Writes the average of the gradients to Tensorboard.',\n",
       " 'Writes the maximum of the gradients to Tensorboard.',\n",
       " 'Writes the minimum of the gradients to Tensorboard.',\n",
       " 'Gets list of image tensors from lists of Image objects, as a tuple of original, generated and real(target) images.',\n",
       " 'Writes original, generated and real(target) images to Tensorboard.',\n",
       " 'Writes training and validation batch images to Tensorboard.',\n",
       " 'Writes batch images of specified DatasetType to Tensorboard.',\n",
       " 'Wraps h in new Variables, to detach them from their history.',\n",
       " 'Invoked during the forward propagation of the RNN_Encoder module.\\n        Args:\\n            input (Tensor): input of shape (sentence length x batch_size)\\n\\n        Returns:\\n            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\\n            dropouth, list of tensors evaluated from each RNN layer using dropouth,',\n",
       " 'Replace tokens in ALL CAPS in `x` by their lower version and add `TK_UP` before.',\n",
       " 'Replace all Capitalized tokens in `x` by their lower version and add `TK_MAJ` before.',\n",
       " 'plots loss function as function of iterations. \\n        When used in Jupyternotebook, plot will be displayed in notebook. Else, plot will be displayed in console and both plot and loss are saved in save_path.',\n",
       " 'Plots learning rate in jupyter notebook or console, depending on the enviroment of the learner.',\n",
       " 'Plots the loss function with respect to learning rate, in log scale.',\n",
       " 'Implements the weight decay schedule as mentioned in https://arxiv.org/abs/1711.05101\\n\\n        :param layer_opt: The LayerOptimizer\\n        :param batch_per_epoch: Num batches in 1 epoch\\n        :param cycle_len: Num epochs in initial cycle. Subsequent cycle_len = previous cycle_len * cycle_mult\\n        :param cycle_mult: Cycle multiplier\\n        :param n_cycles: Number of cycles to be executed',\n",
       " 'Test if `last_loss` is NaN and interrupts training.',\n",
       " 'Compare the value monitored to its best score and maybe save the model.',\n",
       " 'Compare the value monitored to its best and maybe reduce lr.',\n",
       " 'Store completed epoch number in `learn.model_dir/name`.',\n",
       " 'Convert a notebook `fname` to html file in `dest_path`.',\n",
       " 'Convert modified notebooks in `folder` to html pages in `dest_path`.',\n",
       " 'Function that collect samples and adds padding. Flips token order if needed',\n",
       " 'Create the ragged array that will be filled when we ask for items.',\n",
       " 'Fill the row with tokens from the ragged array. --OBS-- overlap != 1 has not been implemented',\n",
       " 'Create a `TextDataBunch` from ids, labels and a `vocab`. `kwargs` are passed to the dataloader creation.',\n",
       " 'Load a `TextDataBunch` from `path/cache_name`. `kwargs` are passed to the dataloader creation.',\n",
       " 'Create a `TextDataBunch` from tokens and labels. `kwargs` are passed to the dataloader creation.',\n",
       " 'Create a `TextDataBunch` from DataFrames. `kwargs` are passed to the dataloader creation.',\n",
       " 'Create a `TextDataBunch` from texts in csv files. `kwargs` are passed to the dataloader creation.',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds_train = Dataset.from_pandas(train_dataset[[\"docstring\", \"function\"]])\n",
    "ds_validation = Dataset.from_pandas(eval_dataset[[\"docstring\", \"function\"]])\n",
    "ds_test = Dataset.from_pandas(test_dataset[[\"docstring\", \"function\"]])\n",
    "\n",
    "ds_train[\"docstring\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314713/314713 [00:00<00:00, 1990448.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [00:00<00:00, 124272.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 401/401 [00:00<00:00, 157970.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": ds_train,\n",
    "            \"validation\": ds_validation,\n",
    "            \"test\": ds_test,\n",
    "        }\n",
    "    )\n",
    "\n",
    "dataset_dict.save_to_disk(\"../docstring_len_filtered.ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['converts a style_dict to an xlsxwriter format dict\\n\\n        Parameters\\n        ----------\\n        style_dict : style dictionary to convert\\n        num_format_str : optional number format string',\n",
       " 'Convert DataFrame to Series with multi-level Index. Columns become the\\n    second level of the resulting hierarchical index\\n\\n    Returns\\n    -------\\n    stacked : Series',\n",
       " \"Parameters\\n    ----------\\n    s: string\\n        Fixed-length string to split\\n    parts: list of (name, length) pairs\\n        Used to break up string, name '_' will be filtered from output.\\n\\n    Returns\\n    -------\\n    Dict of name:contents of string at given location.\",\n",
       " 'Parse a vector of float values representing IBM 8 byte floats into\\n    native 8 byte floats.',\n",
       " 'Get number of records in file.\\n\\n        This is maybe suboptimal because we have to seek to the end of\\n        the file.\\n\\n        Side effect: returns file position to record_start.',\n",
       " 'Reads lines from Xport file and returns as dataframe\\n\\n        Parameters\\n        ----------\\n        size : int, defaults to None\\n            Number of lines to read.  If None, reads whole file.\\n\\n        Returns\\n        -------\\n        DataFrame',\n",
       " 'return a single array of a block that has a single dtype; if dtype is\\n    not None, coerce to this dtype',\n",
       " 'return an array of blocks that potentially have different dtypes',\n",
       " 'return an array of blocks that potentially have different dtypes (and\\n    are sparse)',\n",
       " 'Find the common dtype for `blocks`.\\n\\n    Parameters\\n    ----------\\n    blocks : List[Block]\\n\\n    Returns\\n    -------\\n    dtype : Optional[Union[np.dtype, ExtensionDtype]]\\n        None is returned when `blocks` is empty.',\n",
       " 'Merge blocks having same dtype, exclude non-consolidating blocks',\n",
       " 'Compare two array_like inputs of the same shape or two scalar values\\n\\n    Calls operator.eq or re.search, depending on regex argument. If regex is\\n    True, perform an element-wise regex matching.\\n\\n    Parameters\\n    ----------\\n    a : array_like or scalar\\n    b : array_like or scalar\\n    regex : bool, default False\\n\\n    Returns\\n    -------\\n    mask : array_like of bool',\n",
       " 'If two indices overlap, add suffixes to overlapping entries.\\n\\n    If corresponding suffix is empty, the entry is simply converted to string.',\n",
       " 'Apply function to all values found in index.\\n\\n    This includes transforming multiindex entries separately.\\n    Only apply function to one level of the MultiIndex if level is specified.',\n",
       " 'Faster version of set(arr) for sequences of small numbers.',\n",
       " 'Concatenate block managers into one.\\n\\n    Parameters\\n    ----------\\n    mgrs_indexers : list of (BlockManager, {axis: indexer,...}) tuples\\n    axes : list of Index\\n    concat_axis : int\\n    copy : bool',\n",
       " 'return an empty BlockManager with the items axis of len 0',\n",
       " 'Rename one of axes.\\n\\n        Parameters\\n        ----------\\n        mapper : unary callable\\n        axis : int\\n        copy : boolean, default True\\n        level : int, default None',\n",
       " 'return a dict of the counts of the function in BlockManager',\n",
       " 'Parameters\\n        ----------\\n        copy : boolean, default False\\n            Whether to copy the blocks',\n",
       " 'Parameters\\n        ----------\\n        copy : boolean, default False\\n            Whether to copy the blocks',\n",
       " \"Make deep or shallow copy of BlockManager\\n\\n        Parameters\\n        ----------\\n        deep : boolean o rstring, default True\\n            If False, return shallow copy (do not copy data)\\n            If 'all', copy data and a deep copy of the index\\n\\n        Returns\\n        -------\\n        copy : BlockManager\",\n",
       " 'Convert the blockmanager data into an numpy array.\\n\\n        Parameters\\n        ----------\\n        transpose : boolean, default False\\n            If True, transpose the return array\\n        items : list of strings or None\\n            Names of block items that will be included in the returned\\n            array. ``None`` means that all block items will be used\\n\\n        Returns\\n        -------\\n        arr : ndarray',\n",
       " 'Return ndarray from blocks with specified item order\\n        Items must be contained in the blocks',\n",
       " 'Return a dict of str(dtype) -> BlockManager\\n\\n        Parameters\\n        ----------\\n        copy : boolean, default True\\n\\n        Returns\\n        -------\\n        values : a dict of dtype -> BlockManager\\n\\n        Notes\\n        -----\\n        This consolidates based on str(dtype)',\n",
       " 'get a cross sectional for a given location in the\\n        items ; handle dups\\n\\n        return the result, is *could* be a view in the case of a\\n        single block',\n",
       " 'Join together blocks having same dtype\\n\\n        Returns\\n        -------\\n        y : BlockManager',\n",
       " 'Return values for selected item (ndarray or BlockManager).',\n",
       " 'Return the data as a SingleBlockManager if fastpath=True and possible\\n\\n        Otherwise return as a ndarray',\n",
       " 'Delete selected item (items if non-unique) in-place.',\n",
       " 'Set new item in-place. Does not consolidate. Adds new Block if not\\n        contained in the current set of items',\n",
       " 'Insert item at selected position.\\n\\n        Parameters\\n        ----------\\n        loc : int\\n        item : hashable\\n        value : array_like\\n        allow_duplicates: bool\\n            If False, trying to insert non-unique item will raise',\n",
       " \"Parameters\\n        ----------\\n        new_axis : Index\\n        indexer : ndarray of int64 or None\\n        axis : int\\n        fill_value : object\\n        allow_dups : bool\\n\\n        pandas-indexer with -1's only.\",\n",
       " 'Slice/take blocks along axis=0.\\n\\n        Overloaded for SingleBlock\\n\\n        Returns\\n        -------\\n        new_blocks : list of Block',\n",
       " 'Return a blockmanager with all blocks unstacked.\\n\\n        Parameters\\n        ----------\\n        unstacker_func : callable\\n            A (partially-applied) ``pd.core.reshape._Unstacker`` class.\\n        fill_value : Any\\n            fill_value for newly introduced missing values.\\n\\n        Returns\\n        -------\\n        unstacked : BlockManager',\n",
       " \"Delete single item from SingleBlockManager.\\n\\n        Ensures that self.blocks doesn't become empty.\",\n",
       " 'Concatenate a list of SingleBlockManagers into a single\\n        SingleBlockManager.\\n\\n        Used for pd.concat of Series objects with axis=0.\\n\\n        Parameters\\n        ----------\\n        to_concat : list of SingleBlockManagers\\n        new_axis : Index of the result\\n\\n        Returns\\n        -------\\n        SingleBlockManager',\n",
       " 'Gets called prior to a ufunc (and after)\\n\\n        See SparseArray.__array_wrap__ for detail.',\n",
       " 'Gets called after any ufunc or other array operations, necessary\\n        to pass on the index.',\n",
       " 'Construct SparseSeries from array.\\n\\n        .. deprecated:: 0.23.0\\n            Use the pd.SparseSeries(..) constructor instead.',\n",
       " 'return my self as a sparse array, do not copy by default',\n",
       " 'Return the i-th value or values in the SparseSeries by location\\n\\n        Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n\\n        Returns\\n        -------\\n        value : scalar (int) or Series (slice, sequence)',\n",
       " 'Return an object with absolute value taken. Only applicable to objects\\n        that are all numeric\\n\\n        Returns\\n        -------\\n        abs: same type as caller',\n",
       " 'Returns value occupying requested label, default to specified\\n        missing value if not present. Analogous to dict.get\\n\\n        Parameters\\n        ----------\\n        label : object\\n            Label value looking for\\n        default : object, optional\\n            Value to return if label not in index\\n\\n        Returns\\n        -------\\n        y : scalar',\n",
       " 'Retrieve single value at passed index label\\n\\n        .. deprecated:: 0.21.0\\n\\n        Please use .at[] or .iat[] accessors.\\n\\n        Parameters\\n        ----------\\n        index : label\\n        takeable : interpret the index as indexers, default False\\n\\n        Returns\\n        -------\\n        value : scalar value',\n",
       " 'Convert SparseSeries to a Series.\\n\\n        Returns\\n        -------\\n        s : Series',\n",
       " 'Make a copy of the SparseSeries. Only the actual sparse values need to\\n        be copied',\n",
       " 'Conform sparse values to new SparseIndex\\n\\n        Parameters\\n        ----------\\n        new_index : {BlockIndex, IntIndex}\\n\\n        Returns\\n        -------\\n        reindexed : SparseSeries',\n",
       " 'Cumulative sum of non-NA/null values.\\n\\n        When performing the cumulative summation, any non-NA/null values will\\n        be skipped. The resulting SparseSeries will preserve the locations of\\n        NaN values, but the fill value will be `np.nan` regardless.\\n\\n        Parameters\\n        ----------\\n        axis : {0}\\n\\n        Returns\\n        -------\\n        cumsum : SparseSeries',\n",
       " 'Analogous to Series.dropna. If fill_value=NaN, returns a dense Series',\n",
       " \"Combine Series values, choosing the calling Series's values\\n        first. Result index will be the union of the two indexes\\n\\n        Parameters\\n        ----------\\n        other : Series\\n\\n        Returns\\n        -------\\n        y : Series\",\n",
       " 'Create a cache of unique dates from an array of dates\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n    format : string\\n        Strftime format to parse time\\n    cache : boolean\\n        True attempts to create a cache of converted values\\n    convert_listlike : function\\n        Conversion function to apply on dates\\n\\n    Returns\\n    -------\\n    cache_array : Series\\n        Cache of converted, unique dates. Can be empty',\n",
       " \"Helper function for to_datetime.\\n    Adjust input argument to the specified origin\\n\\n    Parameters\\n    ----------\\n    arg : list, tuple, ndarray, Series, Index\\n        date to be adjusted\\n    origin : 'julian' or Timestamp\\n        origin offset for the arg\\n    unit : string\\n        passed unit from to_datetime, must be 'D'\\n\\n    Returns\\n    -------\\n    ndarray or scalar of adjusted date(s)\",\n",
       " \"try to parse the YYYYMMDD/%Y%m%d format, try to deal with NaT-like,\\n    arg is a passed in as an object dtype, but could really be ints/strings\\n    with nan-like/or floats (e.g. with nan)\\n\\n    Parameters\\n    ----------\\n    arg : passed value\\n    errors : 'raise','ignore','coerce'\",\n",
       " \"Returns a tuple containing the paramenter list with defaults\\n    and parameter list.\\n\\n    Examples\\n    --------\\n    >>> def f(a, b, c=2):\\n    >>>     return a * b * c\\n    >>> print(make_signature(f))\\n    (['a', 'b', 'c=2'], ['a', 'b', 'c'])\",\n",
       " 'Return a list of tuples of the (attr, formatted_value)',\n",
       " 'Returns the indices that would sort the index and its\\n        underlying data.\\n\\n        Returns\\n        -------\\n        argsorted : numpy array\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argsort',\n",
       " 'Determines if two Index objects contain the same elements.',\n",
       " 'Form the intersection of two Index objects.\\n\\n        Parameters\\n        ----------\\n        other : Index or array-like\\n        sort : False or None, default False\\n            Sort the resulting index if possible\\n\\n            .. versionadded:: 0.24.0\\n\\n            .. versionchanged:: 0.24.1\\n\\n               Changed the default to ``False`` to match the behaviour\\n               from before 0.24.0.\\n\\n        Returns\\n        -------\\n        intersection : Index',\n",
       " 'Returns the smallest element greater than or equal to the limit',\n",
       " 'Returns the largest element smaller than or equal to the limit',\n",
       " \"Extended Euclidean algorithms to solve Bezout's identity:\\n           a*x + b*y = gcd(x, y)\\n        Finds one particular solution for x, y: s, t\\n        Returns: gcd, s, t\",\n",
       " 'Conserve RangeIndex type for scalar and slice keys.',\n",
       " 'Convert the PandasArray to a :class:`numpy.ndarray`.\\n\\n        By default, this requires no coercion or copying of data.\\n\\n        Parameters\\n        ----------\\n        dtype : numpy.dtype\\n            The NumPy dtype to pass to :func:`numpy.asarray`.\\n        copy : bool, default False\\n            Whether to copy the underlying data.\\n\\n        Returns\\n        -------\\n        ndarray',\n",
       " 'Glues together two sets of strings using the amount of space requested.\\n    The idea is to prettify.\\n\\n    ----------\\n    space : int\\n        number of spaces for padding\\n    lists : str\\n        list of str which being joined\\n    strlen : callable\\n        function used to calculate the length of each str. Needed for unicode\\n        handling.\\n    justfunc : callable\\n        function used to justify str. Needed for unicode handling.',\n",
       " 'Perform ljust, center, rjust against string or list-like',\n",
       " 'internal. pprinter for iterables. you should probably use pprint_thing()\\n    rather then calling this directly.\\n\\n    bounds length of printed sequence, depending on options',\n",
       " 'internal. pprinter for iterables. you should probably use pprint_thing()\\n    rather then calling this directly.',\n",
       " 'Return a list of tuples of the (attr, formatted_value)\\n    for common attrs, including dtype, name, length\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        must be iterable\\n\\n    Returns\\n    -------\\n    list',\n",
       " 'Lag plot for time series.\\n\\n    Parameters\\n    ----------\\n    series : Time series\\n    lag : lag of the scatter plot, default 1\\n    ax : Matplotlib axis object, optional\\n    kwds : Matplotlib scatter method keyword arguments, optional\\n\\n    Returns\\n    -------\\n    class:`matplotlib.axis.Axes`',\n",
       " 'Autocorrelation plot for time series.\\n\\n    Parameters:\\n    -----------\\n    series: Time series\\n    ax: Matplotlib axis object, optional\\n    kwds : keywords\\n        Options to pass to matplotlib plotting method\\n\\n    Returns:\\n    -----------\\n    class:`matplotlib.axis.Axes`',\n",
       " 'Check a sequence of terms for instances of PandasObject.',\n",
       " 'Reconstruct an object given its type, raw value, and possibly empty\\n    (None) axes.\\n\\n    Parameters\\n    ----------\\n    typ : object\\n        A type\\n    obj : object\\n        The value to use in the type constructor\\n    axes : dict\\n        The axes to use to construct the resulting pandas object\\n\\n    Returns\\n    -------\\n    ret : typ\\n        An object of type ``typ`` with the value `obj` and possible axes\\n        `axes`.',\n",
       " 'Plots a Series on the given Matplotlib axes or the current axes\\n\\n    Parameters\\n    ----------\\n    axes : Axes\\n    series : Series\\n\\n    Notes\\n    _____\\n    Supports same kwargs as Axes.plot\\n\\n\\n    .. deprecated:: 0.23.0\\n       Use Series.plot() instead',\n",
       " 'Get the freq attribute of the ax object if set.\\n    Also checks shared axes (eg when using secondary yaxis, sharex=True\\n    or twinx)',\n",
       " 'Pretty-formats the date axis (x-axis).\\n\\n    Major and minor ticks are automatically set for the frequency of the\\n    current underlying series.  As the dynamic mode is activated by\\n    default, changing the limits of the x axis will intelligently change\\n    the positions of the ticks.',\n",
       " 'Return a unicode string representation for a particular DataFrame.',\n",
       " 'Return a html representation for a particular DataFrame.\\n\\n        Mainly for IPython notebook.',\n",
       " 'Matrix multiplication using binary `@` operator in Python>=3.5.',\n",
       " 'Write out the binary feather-format for DataFrames.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            string file path',\n",
       " 'Quickly retrieve single value at passed column and index.\\n\\n        .. deprecated:: 0.21.0\\n            Use .at[] or .iat[] accessors instead.\\n\\n        Parameters\\n        ----------\\n        index : row label\\n        col : column label\\n        takeable : interpret the index/col as indexers, default False\\n\\n        Returns\\n        -------\\n        scalar',\n",
       " 'Put single value at passed column and index.\\n\\n        .. deprecated:: 0.21.0\\n            Use .at[] or .iat[] accessors instead.\\n\\n        Parameters\\n        ----------\\n        index : row label\\n        col : column label\\n        value : scalar\\n        takeable : interpret the index/col as indexers, default False\\n\\n        Returns\\n        -------\\n        DataFrame\\n            If label pair is contained, will be reference to calling DataFrame,\\n            otherwise a new object.',\n",
       " 'Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n        axis : int\\n\\n        Notes\\n        -----\\n        If slice passed, the resulting data will be a view.',\n",
       " \"Ensure that if we don't have an index, that we can create one from the\\n        passed value.\",\n",
       " 'Add series to DataFrame in specified column.\\n\\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\\n        same length as the DataFrames index or an error will be thrown.\\n\\n        Series/TimeSeries will be conformed to the DataFrames index to\\n        ensure homogeneity.',\n",
       " 'Insert column into DataFrame at specified location.\\n\\n        Raises a ValueError if `column` is already contained in the DataFrame,\\n        unless `allow_duplicates` is set to True.\\n\\n        Parameters\\n        ----------\\n        loc : int\\n            Insertion index. Must verify 0 <= loc <= len(columns)\\n        column : string, number, or hashable object\\n            label of the inserted column\\n        value : int, Series, or array-like\\n        allow_duplicates : bool, optional',\n",
       " 'Swap levels i and j in a MultiIndex on a particular axis.\\n\\n        Parameters\\n        ----------\\n        i, j : int, string (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.',\n",
       " 'Rearrange index levels using input order. May not drop or\\n        duplicate levels.\\n\\n        Parameters\\n        ----------\\n        order : list of int or list of str\\n            List representing new level order. Reference level by number\\n            (position) or by key (label).\\n        axis : int\\n            Where to reorder levels.\\n\\n        Returns\\n        -------\\n        type of caller (new object)',\n",
       " 'Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'Infer and return an integer array of the values.\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : dtype, optional\\n        dtype to coerce\\n    copy : boolean, default False\\n\\n    Returns\\n    -------\\n    IntegerArray\\n\\n    Raises\\n    ------\\n    TypeError if incompatible types',\n",
       " 'Safely cast the values to the dtype if they\\n    are equivalent, meaning floats must be equivalent to the\\n    ints.',\n",
       " 'Coerce the input values array to numpy arrays with a mask\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : integer dtype\\n    mask : boolean 1D array, optional\\n    copy : boolean, default False\\n        if True, copy the input\\n\\n    Returns\\n    -------\\n    tuple of (values, mask)',\n",
       " 'Construction from a string, raise a TypeError if not\\n        possible',\n",
       " \"Returns a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don't include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\",\n",
       " 'Return values for sorting.\\n\\n        Returns\\n        -------\\n        ndarray\\n            The transformed values should maintain the ordering between values\\n            within the array.\\n\\n        See Also\\n        --------\\n        ExtensionArray.argsort',\n",
       " 'Parameters\\n        ----------\\n        result : array-like\\n        mask : array-like bool\\n        other : scalar or array-like\\n        op_name : str',\n",
       " 'return the length of a single non-tuple indexer which could be a slice',\n",
       " 'if we are index sliceable, then return my slicer, otherwise return None',\n",
       " 'reverse convert a missing indexer, which is a dict\\n    return the scalar indexer and a boolean indicating if we converted',\n",
       " \"create a filtered indexer that doesn't have any missing indexers\",\n",
       " \"Ensurse that a slice doesn't reduce to a Series or Scalar.\\n\\n    Any user-paseed `subset` should have this called on it\\n    to make sure we're always working with DataFrames.\",\n",
       " \"want nice defaults for background_gradient that don't break\\n    with non-numeric data. But if slice_ is passed go with that.\",\n",
       " 'validate that an positional indexer cannot enlarge its target\\n        will raise if needed, does not modify the indexer externally',\n",
       " 'Check whether there is the possibility to use ``_multi_take``.\\n        Currently the limit is that all axes being indexed must be indexed with\\n        list-likes.\\n\\n        Parameters\\n        ----------\\n        tup : tuple\\n            Tuple of indexers, one per axis\\n\\n        Returns\\n        -------\\n        boolean: Whether the current indexing can be passed through _multi_take',\n",
       " 'Create the indexers for the passed tuple of keys, and execute the take\\n        operation. This allows the take operation to be executed all at once -\\n        rather than once for each dimension - improving efficiency.\\n\\n        Parameters\\n        ----------\\n        tup : tuple\\n            Tuple of indexers, one per axis\\n\\n        Returns\\n        -------\\n        values: same type as the object being indexed',\n",
       " \"Convert indexing key into something we can use to do actual fancy\\n        indexing on an ndarray\\n\\n        Examples\\n        ix[:5] -> slice(0, 5)\\n        ix[[1,2,3]] -> [1,2,3]\\n        ix[['foo', 'bar', 'baz']] -> [i, j, k] (indices of foo, bar, baz)\\n\\n        Going by Zen of Python?\\n        'In the face of ambiguity, refuse the temptation to guess.'\\n        raise AmbiguousIndexError with integer labels?\\n        - No, prefer label-based indexing\",\n",
       " 'Transform a list of keys into a new array ready to be used as axis of\\n        the object we return (e.g. including NaNs).\\n\\n        Parameters\\n        ----------\\n        key : list-like\\n            Target labels\\n        axis: int\\n            Where the indexing is being made\\n\\n        Returns\\n        -------\\n        list-like of labels',\n",
       " 'this is pretty simple as we just have to deal with labels',\n",
       " 'Translate any partial string timestamp matches in key, returning the\\n        new key (GH 10331)',\n",
       " \"Check that 'key' is a valid position in the desired axis.\\n\\n        Parameters\\n        ----------\\n        key : int\\n            Requested position\\n        axis : int\\n            Desired axis\\n\\n        Returns\\n        -------\\n        None\\n\\n        Raises\\n        ------\\n        IndexError\\n            If 'key' is not a valid position in axis 'axis'\",\n",
       " 'Return Series values by list or array of integers\\n\\n        Parameters\\n        ----------\\n        key : list-like positional indexer\\n        axis : int (can only be zero)\\n\\n        Returns\\n        -------\\n        Series object',\n",
       " 'much simpler as we only have to deal with our valid types',\n",
       " \"require they keys to be the same type as the index (so we don't\\n        fallback)\",\n",
       " 'require integer args (and convert to label arguments)',\n",
       " 'create and return the block manager from a dataframe of series,\\n    columns, index',\n",
       " 'Conform a set of SparseSeries (with NaN fill_value) to a common SparseIndex\\n    corresponding to the locations where they all have data\\n\\n    Parameters\\n    ----------\\n    series_dict : dict or DataFrame\\n\\n    Notes\\n    -----\\n    Using the dumbest algorithm I could think of. Should put some more thought\\n    into this\\n\\n    Returns\\n    -------\\n    homogenized : dict of SparseSeries',\n",
       " 'Convert to dense DataFrame\\n\\n        Returns\\n        -------\\n        df : DataFrame',\n",
       " 'Get new SparseDataFrame applying func to each columns',\n",
       " 'Ratio of non-sparse points to total (dense) data points\\n        represented in the frame',\n",
       " 'Creates a new SparseArray from the input value.\\n\\n        Parameters\\n        ----------\\n        key : object\\n        value : scalar, Series, or array-like\\n        kwargs : dict\\n\\n        Returns\\n        -------\\n        sanitized_column : SparseArray',\n",
       " 'Returns a row (cross-section) from the SparseDataFrame as a Series\\n        object.\\n\\n        Parameters\\n        ----------\\n        key : some index contained in the index\\n\\n        Returns\\n        -------\\n        xs : Series',\n",
       " 'Returns a DataFrame with the rows/columns switched.',\n",
       " 'Return SparseDataFrame of cumulative sums over requested axis.\\n\\n        Parameters\\n        ----------\\n        axis : {0, 1}\\n            0 for row-wise, 1 for column-wise\\n\\n        Returns\\n        -------\\n        y : SparseDataFrame',\n",
       " 'Convert a conda package to its pip equivalent.\\n\\n    In most cases they are the same, those are the exceptions:\\n    - Packages that should be excluded (in `EXCLUDE`)\\n    - Packages that should be renamed (in `RENAME`)\\n    - A package requiring a specific version, in conda is defined with a single\\n      equal (e.g. ``pandas=1.0``) and in pip with two (e.g. ``pandas==1.0``)',\n",
       " 'try to do platform conversion, allow ndarray or list here',\n",
       " 'return a boolean if we have a nested object, e.g. a Series with 1 or\\n    more Series elements\\n\\n    This may not be necessarily be performant.',\n",
       " 'try to cast to the specified dtype (e.g. convert back to bool/int\\n    or could be an astype of float64->float32',\n",
       " 'interpret the dtype from a scalar or array. This is a convenience\\n    routines to infer dtype from a scalar or an array\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar/array belongs to pandas extension types is inferred as\\n        object',\n",
       " 'interpret the dtype from a scalar\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar belongs to pandas extension types is inferred as\\n        object',\n",
       " 'provide explicit type promotion and coercion\\n\\n    Parameters\\n    ----------\\n    values : the ndarray that we want to maybe upcast\\n    fill_value : what we want to fill with\\n    dtype : if None, then use the dtype of the values, else coerce to this type\\n    copy : if True always make a copy even if no upcast is required',\n",
       " 'Change string like dtypes to object for\\n    ``DataFrame.select_dtypes()``.',\n",
       " 'coerce the indexer input array to the smallest dtype possible',\n",
       " 'given a dtypes and a result set, coerce the result elements to the\\n    dtypes',\n",
       " \"Cast the elements of an array to a given dtype a nan-safe manner.\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    dtype : np.dtype\\n    copy : bool, default True\\n        If False, a view will be attempted but may fail, if\\n        e.g. the item sizes don't align.\\n    skipna: bool, default False\\n        Whether or not we should skip NaN when casting as a string-type.\\n\\n    Raises\\n    ------\\n    ValueError\\n        The dtype was a datetime64/timedelta64 dtype, but it had no unit.\",\n",
       " 'if we have an object dtype, try to coerce dates and/or numbers',\n",
       " 'if we have an object dtype, try to coerce dates and/or numbers',\n",
       " 'try to cast the array/value to a datetimelike dtype, converting float\\n    nan to iNaT',\n",
       " 'Find a common data type among the given dtypes.\\n\\n    Parameters\\n    ----------\\n    types : list of dtypes\\n\\n    Returns\\n    -------\\n    pandas extension or numpy dtype\\n\\n    See Also\\n    --------\\n    numpy.find_common_type',\n",
       " 'create np.ndarray of specified shape and dtype, filled with values\\n\\n    Parameters\\n    ----------\\n    shape : tuple\\n    value : scalar value\\n    dtype : np.dtype, optional\\n        dtype to coerce\\n\\n    Returns\\n    -------\\n    ndarray of shape, filled with value, of specified / inferred dtype',\n",
       " 'create a np.ndarray / pandas type of specified shape and dtype\\n    filled with values\\n\\n    Parameters\\n    ----------\\n    value : scalar value\\n    length : int\\n    dtype : pandas_dtype / np.dtype\\n\\n    Returns\\n    -------\\n    np.ndarray / pandas type of length, filled with value',\n",
       " 'Transform any list-like object in a 1-dimensional numpy array of object\\n    dtype.\\n\\n    Parameters\\n    ----------\\n    values : any iterable which has a len()\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `values` does not have a len()\\n\\n    Returns\\n    -------\\n    1-dimensional numpy array of dtype object',\n",
       " 'Make a scatter plot from two DataFrame columns\\n\\n    Parameters\\n    ----------\\n    data : DataFrame\\n    x : Column name for the x-axis values\\n    y : Column name for the y-axis values\\n    ax : Matplotlib axis object\\n    figsize : A tuple (width, height) in inches\\n    grid : Setting this to True will show the grid\\n    kwargs : other plotting keyword arguments\\n        To be passed to scatter function\\n\\n    Returns\\n    -------\\n    matplotlib.Figure',\n",
       " 'Grouped histogram\\n\\n    Parameters\\n    ----------\\n    data : Series/DataFrame\\n    column : object, optional\\n    by : object, optional\\n    ax : axes, optional\\n    bins : int, default 50\\n    figsize : tuple, optional\\n    layout : optional\\n    sharex : bool, default False\\n    sharey : bool, default False\\n    rot : int, default 90\\n    grid : bool, default True\\n    kwargs : dict, keyword arguments passed to matplotlib.Axes.hist\\n\\n    Returns\\n    -------\\n    collection of Matplotlib Axes',\n",
       " \"Tick creation within matplotlib is reasonably expensive and is\\n            internally deferred until accessed as Ticks are created/destroyed\\n            multiple times per draw. It's therefore beneficial for us to avoid\\n            accessing unless we will act on the Tick.\",\n",
       " 'Manage style and color based on column number and its label.\\n        Returns tuple of appropriate style and kwds which \"color\" may be added.',\n",
       " 'Return a list with distinct elements of \"objs\" (different ids).\\n    Preserves order.',\n",
       " 'Return the union or intersection of indexes.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index or list objects\\n        When intersect=True, do not accept list of lists.\\n    intersect : bool, default False\\n        If True, calculate the intersection between indexes. Otherwise,\\n        calculate the union.\\n    sort : bool, default False\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index',\n",
       " 'Return the union of indexes.\\n\\n    The behavior of sort and names is not consistent.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index or list objects\\n    sort : bool, default True\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index',\n",
       " \"Give a consensus 'names' to indexes.\\n\\n    If there's exactly one non-empty 'names', return this,\\n    otherwise, return empty.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index objects\\n\\n    Returns\\n    -------\\n    list\\n        A list representing the consensus 'names' found.\",\n",
       " 'Determine if all indexes contain the same elements.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index objects\\n\\n    Returns\\n    -------\\n    bool\\n        True if all indexes contain the same elements, False otherwise.',\n",
       " 'Convert SQL and params args to DBAPI2.0 compliant format.',\n",
       " 'Process parse_dates argument for read_sql functions',\n",
       " 'Force non-datetime columns to be read as such.\\n    Supports both string formatted and integer timestamp columns.',\n",
       " 'Returns a SQLAlchemy engine from a URI (if con is a string)\\n    else it just return con without modifying it.',\n",
       " 'Convenience function to return the correct PandasSQL subclass based on the\\n    provided parameters.',\n",
       " 'Execute SQL statement inserting data\\n\\n        Parameters\\n        ----------\\n        conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\\n        keys : list of str\\n           Column names\\n        data_iter : generator of list\\n           Each item contains a list of values to be inserted',\n",
       " 'Return a list of SQL statements that creates a table reflecting the\\n        structure of a DataFrame.  The first entry will be a CREATE TABLE\\n        statement while the rest will be CREATE INDEX statements.',\n",
       " 'Coerce to a categorical if a series is given.\\n\\n    Internal use ONLY.',\n",
       " 'utility routine to turn values into codes given the specified categories',\n",
       " \"Convert a set of codes for to a new set of categories\\n\\n    Parameters\\n    ----------\\n    codes : array\\n    old_categories, new_categories : Index\\n\\n    Returns\\n    -------\\n    new_codes : array\\n\\n    Examples\\n    --------\\n    >>> old_cat = pd.Index(['b', 'a', 'c'])\\n    >>> new_cat = pd.Index(['a', 'b'])\\n    >>> codes = np.array([0, 1, 1, 2])\\n    >>> _recode_for_categories(codes, old_cat, new_cat)\\n    array([ 1,  0,  0, -1])\",\n",
       " 'Factorize an input `values` into `categories` and `codes`. Preserves\\n    categorical dtype in `categories`.\\n\\n    *This is an internal function*\\n\\n    Parameters\\n    ----------\\n    values : list-like\\n\\n    Returns\\n    -------\\n    codes : ndarray\\n    categories : Index\\n        If `values` has a categorical dtype, then `categories` is\\n        a CategoricalIndex keeping the categories and order of `values`.',\n",
       " 'A higher-level wrapper over `_factorize_from_iterable`.\\n\\n    *This is an internal function*\\n\\n    Parameters\\n    ----------\\n    iterables : list-like of list-likes\\n\\n    Returns\\n    -------\\n    codes_list : list of ndarrays\\n    categories_list : list of Indexes\\n\\n    Notes\\n    -----\\n    See `_factorize_from_iterable` for more info.',\n",
       " 'Coerce this type to another dtype\\n\\n        Parameters\\n        ----------\\n        dtype : numpy dtype or pandas type\\n        copy : bool, default True\\n            By default, astype always returns a newly allocated object.\\n            If copy is set to False and dtype is categorical, the original\\n            object is returned.\\n\\n            .. versionadded:: 0.19.0',\n",
       " 'Get the codes.\\n\\n        Returns\\n        -------\\n        codes : integer array view\\n            A non writable view of the `codes` array.',\n",
       " \"Sets new categories inplace\\n\\n        Parameters\\n        ----------\\n        fastpath : bool, default False\\n           Don't perform validation of the categories for uniqueness or nulls\\n\\n        Examples\\n        --------\\n        >>> c = pd.Categorical(['a', 'b'])\\n        >>> c\\n        [a, b]\\n        Categories (2, object): [a, b]\\n\\n        >>> c._set_categories(pd.Index(['a', 'c']))\\n        >>> c\\n        [a, c]\\n        Categories (2, object): [a, c]\",\n",
       " \"Internal method for directly updating the CategoricalDtype\\n\\n        Parameters\\n        ----------\\n        dtype : CategoricalDtype\\n\\n        Notes\\n        -----\\n        We don't do any validation here. It's assumed that the dtype is\\n        a (valid) instance of `CategoricalDtype`.\",\n",
       " 'Set the ordered attribute to the boolean value.\\n\\n        Parameters\\n        ----------\\n        value : bool\\n           Set whether this categorical is ordered (True) or not (False).\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to the value.',\n",
       " 'Set the Categorical to be ordered.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to True.',\n",
       " 'Set the Categorical to be unordered.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to False.',\n",
       " 'Shift Categorical by desired number of periods.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to move, can be positive or negative\\n        fill_value : object, optional\\n            The scalar value to use for newly introduced missing values.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        shifted : Categorical',\n",
       " 'The numpy array interface.\\n\\n        Returns\\n        -------\\n        numpy.array\\n            A numpy array of either the specified dtype or,\\n            if dtype==None (default), the same dtype as\\n            categorical.categories.dtype.',\n",
       " 'Memory usage of my values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes',\n",
       " \"Return a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don't include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\",\n",
       " 'Return the values.\\n\\n        For internal compatibility with pandas formatting.\\n\\n        Returns\\n        -------\\n        numpy.array\\n            A numpy array of the same dtype as categorical.categories.dtype or\\n            Index if datetime / periods.',\n",
       " 'For correctly ranking ordered categorical data. See GH#15420\\n\\n        Ordered categorical data should be ranked on the basis of\\n        codes with -1 translated to NaN.\\n\\n        Returns\\n        -------\\n        numpy.array',\n",
       " 'Return a slice of myself.\\n\\n        For internal compatibility with numpy arrays.',\n",
       " 'a short repr displaying only max_vals and an optional (but default\\n        footer)',\n",
       " 'Item assignment.\\n\\n\\n        Raises\\n        ------\\n        ValueError\\n            If (one or more) Value is not in categories or if a assigned\\n            `Categorical` does not have the same categories',\n",
       " 'The minimum value of the object.\\n\\n        Only ordered `Categoricals` have a minimum!\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the `Categorical` is not `ordered`.\\n\\n        Returns\\n        -------\\n        min : the minimum of this `Categorical`',\n",
       " \"Returns the mode(s) of the Categorical.\\n\\n        Always returns `Categorical` even if only one value.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don't consider counts of NaN/NaT.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        modes : `Categorical` (sorted)\",\n",
       " 'Returns True if categorical arrays are equal.\\n\\n        Parameters\\n        ----------\\n        other : `Categorical`\\n\\n        Returns\\n        -------\\n        bool',\n",
       " 'Returns True if categoricals are the same dtype\\n          same categories, and same ordered\\n\\n        Parameters\\n        ----------\\n        other : Categorical\\n\\n        Returns\\n        -------\\n        bool',\n",
       " 'Describes this Categorical\\n\\n        Returns\\n        -------\\n        description: `DataFrame`\\n            A dataframe with frequency and counts by category.',\n",
       " 'Convert a list of objects to a timedelta index object.',\n",
       " 'Vectorized apply of DateOffset to DatetimeIndex,\\n        raises NotImplentedError for offsets without a\\n        vectorized implementation.\\n\\n        Parameters\\n        ----------\\n        i : DatetimeIndex\\n\\n        Returns\\n        -------\\n        y : DatetimeIndex',\n",
       " 'Roll provided date backward to next offset only if not on offset.',\n",
       " 'Roll provided date forward to next offset only if not on offset.',\n",
       " \"If n is positive, return tomorrow's business day opening time.\\n        Otherwise yesterday's business day's opening time.\\n\\n        Opening time always locates on BusinessDay.\\n        Otherwise, closing time may not if business hour extends over midnight.\",\n",
       " 'Roll provided date backward to next offset only if not on offset.',\n",
       " 'Roll provided date forward to next offset only if not on offset.',\n",
       " 'Define default roll function to be called in apply method.',\n",
       " 'Define default roll function to be called in apply method.',\n",
       " 'Add days portion of offset to DatetimeIndex i.\\n\\n        Parameters\\n        ----------\\n        i : DatetimeIndex\\n        roll : ndarray[int64_t]\\n\\n        Returns\\n        -------\\n        result : DatetimeIndex',\n",
       " 'Add self to the given DatetimeIndex, specialized for case where\\n        self.weekday is non-null.\\n\\n        Parameters\\n        ----------\\n        dtindex : DatetimeIndex\\n\\n        Returns\\n        -------\\n        result : DatetimeIndex',\n",
       " \"Find the day in the same month as other that has the same\\n        weekday as self.weekday and is the self.week'th such day in the month.\\n\\n        Parameters\\n        ----------\\n        other : datetime\\n\\n        Returns\\n        -------\\n        day : int\",\n",
       " 'Find the day in the same month as other that has the same\\n        weekday as self.weekday and is the last such day in the month.\\n\\n        Parameters\\n        ----------\\n        other: datetime\\n\\n        Returns\\n        -------\\n        day: int',\n",
       " 'Compute the vectorized membership of ``x in y`` if possible, otherwise\\n    use Python.',\n",
       " 'Compute the vectorized membership of ``x not in y`` if possible,\\n    otherwise use Python.',\n",
       " \"Cast an expression inplace.\\n\\n    Parameters\\n    ----------\\n    terms : Op\\n        The expression that should cast.\\n    acceptable_dtypes : list of acceptable numpy.dtype\\n        Will not cast if term's dtype in this list.\\n\\n        .. versionadded:: 0.19.0\\n\\n    dtype : str or numpy.dtype\\n        The dtype to cast to.\",\n",
       " \"search order for local (i.e., @variable) variables:\\n\\n        scope, key_variable\\n        [('locals', 'local_name'),\\n         ('globals', 'local_name'),\\n         ('locals', 'key'),\\n         ('globals', 'key')]\",\n",
       " 'Print a generic n-ary operator and its operands using infix\\n        notation',\n",
       " 'Recursively evaluate an expression in Python space.\\n\\n        Parameters\\n        ----------\\n        env : Scope\\n\\n        Returns\\n        -------\\n        object\\n            The result of an evaluated expression.',\n",
       " 'Evaluate a binary operation *before* being passed to the engine.\\n\\n        Parameters\\n        ----------\\n        env : Scope\\n        engine : str\\n        parser : str\\n        term_type : type\\n        eval_in_python : list\\n\\n        Returns\\n        -------\\n        term_type\\n            The \"pre-evaluated\" expression as an instance of ``term_type``',\n",
       " 'Convert datetimes to a comparable value in an expression.',\n",
       " 'Calculate appropriate figure size based on left and right data.',\n",
       " 'Plot left / right DataFrames in specified layout.\\n\\n        Parameters\\n        ----------\\n        left : list of DataFrames before operation is applied\\n        right : DataFrame of operation result\\n        labels : list of str to be drawn as titles of left DataFrames\\n        vertical : bool\\n            If True, use vertical layout. If False, use horizontal layout.',\n",
       " 'Convert each input to appropriate for table outplot',\n",
       " 'if the passed data is of datetime/timedelta type,\\n    this method converts it to numeric so that cut method can\\n    handle it',\n",
       " 'if the passed bin is of datetime/timedelta type,\\n    this method converts it to integer\\n\\n    Parameters\\n    ----------\\n    bins : list-like of bins\\n    dtype : dtype of data\\n\\n    Raises\\n    ------\\n    ValueError if bins are not of a compat dtype to dtype',\n",
       " 'Convert bins to a DatetimeIndex or TimedeltaIndex if the orginal dtype is\\n    datelike\\n\\n    Parameters\\n    ----------\\n    bins : list-like of bins\\n    dtype : dtype of data\\n\\n    Returns\\n    -------\\n    bins : Array-like of bins, DatetimeIndex or TimedeltaIndex if dtype is\\n           datelike',\n",
       " 'handles preprocessing for cut where we convert passed\\n    input to array, strip the index information and store it\\n    separately',\n",
       " 'handles post processing for the cut method where\\n    we combine the index information if the originally passed\\n    datatype was a series',\n",
       " 'Try to find the most capable encoding supported by the console.\\n    slightly modified from the way IPython handles the same issue.',\n",
       " \"Checks whether 'args' has length of at most 'compat_args'. Raises\\n    a TypeError if that is not the case, similar to in Python when a\\n    function is called with too many arguments.\",\n",
       " 'Check that the keys in `arg_val_dict` are mapped to their\\n    default values as specified in `compat_args`.\\n\\n    Note that this function is to be called only when it has been\\n    checked that arg_val_dict.keys() is a subset of compat_args',\n",
       " \"Checks whether 'kwargs' contains any keys that are not\\n    in 'compat_args' and raises a TypeError if there is one.\",\n",
       " 'Ensures that argument passed in arg_name is of type bool.',\n",
       " 'Potentially we might have a deprecation warning, show it\\n    but call the appropriate methods anyhow.',\n",
       " 'Return our appropriate resampler when grouping as well.',\n",
       " 'Utility frequency conversion method for Series/DataFrame.',\n",
       " 'Is the resampling from a DataFrame column or MultiIndex level.',\n",
       " 'Setup our binners.\\n\\n        Cache these as we are an immutable object',\n",
       " 'Create the BinGrouper, assume that self.set_grouper(obj)\\n        has already been called.',\n",
       " 'Call function producing a like-indexed Series on each group and return\\n        a Series with the transformed values.\\n\\n        Parameters\\n        ----------\\n        arg : function\\n            To apply to each group. Should return a Series with the same index.\\n\\n        Returns\\n        -------\\n        transformed : Series\\n\\n        Examples\\n        --------\\n        >>> resampled.transform(lambda x: (x - x.mean()) / x.std())',\n",
       " 'Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'If loffset is set, offset the result index.\\n\\n        This is NOT an idempotent routine, it will be applied\\n        exactly once to the result.\\n\\n        Parameters\\n        ----------\\n        result : Series or DataFrame\\n            the result of resample',\n",
       " 'Return the correct class for resampling with groupby.',\n",
       " 'Interpolate values according to different methods.\\n\\n        .. versionadded:: 0.18.1',\n",
       " 'Compute standard deviation of groups, excluding missing values.\\n\\n        Parameters\\n        ----------\\n        ddof : integer, default 1\\n            Degrees of freedom.',\n",
       " 'Compute variance of groups, excluding missing values.\\n\\n        Parameters\\n        ----------\\n        ddof : integer, default 1\\n            degrees of freedom',\n",
       " 'Dispatch to _upsample; we are stripping all of the _upsample kwargs and\\n        performing the original function call on the grouped object.',\n",
       " 'Downsample the cython defined function.\\n\\n        Parameters\\n        ----------\\n        how : string / cython mapped function\\n        **kwargs : kw args passed to how function',\n",
       " 'Adjust our binner when upsampling.\\n\\n        The range of a new index should not be outside specified range',\n",
       " \"Parameters\\n        ----------\\n        method : string {'backfill', 'bfill', 'pad',\\n            'ffill', 'asfreq'} method for upsampling\\n        limit : int, default None\\n            Maximum size gap to fill when reindexing\\n        fill_value : scalar, default None\\n            Value to use for missing values\\n\\n        See Also\\n        --------\\n        .fillna\",\n",
       " 'Downsample the cython defined function.\\n\\n        Parameters\\n        ----------\\n        how : string / cython mapped function\\n        **kwargs : kw args passed to how function',\n",
       " \"Parameters\\n        ----------\\n        method : string {'backfill', 'bfill', 'pad', 'ffill'}\\n            method for upsampling\\n        limit : int, default None\\n            Maximum size gap to fill when reindexing\\n        fill_value : scalar, default None\\n            Value to use for missing values\\n\\n        See Also\\n        --------\\n        .fillna\",\n",
       " \"Return my resampler or raise if we have an invalid axis.\\n\\n        Parameters\\n        ----------\\n        obj : input object\\n        kind : string, optional\\n            'period','timestamp','timedelta' are valid\\n\\n        Returns\\n        -------\\n        a Resampler\\n\\n        Raises\\n        ------\\n        TypeError if incompatible axis\",\n",
       " \"Parameters\\n    ----------\\n    arrays : generator\\n    num_items : int\\n\\n    Should be the same as CPython's tupleobject.c\",\n",
       " \"Hash an MultiIndex / list-of-tuples efficiently\\n\\n    .. versionadded:: 0.20.0\\n\\n    Parameters\\n    ----------\\n    vals : MultiIndex, list-of-tuples, or single tuple\\n    encoding : string, default 'utf8'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    ndarray of hashed values array\",\n",
       " \"Hash a single tuple efficiently\\n\\n    Parameters\\n    ----------\\n    val : single tuple\\n    encoding : string, default 'utf8'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    hash\",\n",
       " \"Hash a Categorical by hashing its categories, and then mapping the codes\\n    to the hashes\\n\\n    Parameters\\n    ----------\\n    c : Categorical\\n    encoding : string, default 'utf8'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    ndarray of hashed values array, same size as len(c)\",\n",
       " 'Hash scalar value\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash value, of length 1',\n",
       " 'Make sure the provided value for --single is a path to an existing\\n        .rst/.ipynb file, or a pandas object that can be imported.\\n\\n        For example, categorial.rst or pandas.DataFrame.head. For the latter,\\n        return the corresponding file path\\n        (e.g. reference/api/pandas.DataFrame.head.rst).',\n",
       " \"Execute a command as a OS terminal.\\n\\n        Parameters\\n        ----------\\n        *args : list of str\\n            Command and parameters to be executed\\n\\n        Examples\\n        --------\\n        >>> DocBuilder()._run_os('python', '--version')\",\n",
       " \"Call sphinx to build documentation.\\n\\n        Attribute `num_jobs` from the class is used.\\n\\n        Parameters\\n        ----------\\n        kind : {'html', 'latex'}\\n\\n        Examples\\n        --------\\n        >>> DocBuilder(num_jobs=4)._sphinx_build('html')\",\n",
       " 'Create in the build directory an html file with a redirect,\\n        for every row in REDIRECTS_FILE.',\n",
       " 'Render a DataFrame to a LaTeX tabular/longtable environment output.',\n",
       " 'r\"\"\"\\n        Combine columns belonging to a group to a single multicolumn entry\\n        according to self.multicolumn_format\\n\\n        e.g.:\\n        a &  &  & b & c &\\n        will become\\n        \\\\multicolumn{3}{l}{a} & b & \\\\multicolumn{2}{l}{c}',\n",
       " 'r\"\"\"\\n        Check following rows, whether row should be a multirow\\n\\n        e.g.:     becomes:\\n        a & 0 &   \\\\multirow{2}{*}{a} & 0 &\\n          & 1 &     & 1 &\\n        b & 0 &   \\\\cline{1-2}\\n                  b & 0 &',\n",
       " \"Checks whether the 'name' parameter for parsing is either\\n    an integer OR float that can SAFELY be cast to an integer\\n    without losing accuracy. Raises a ValueError if that is\\n    not the case.\\n\\n    Parameters\\n    ----------\\n    name : string\\n        Parameter name (used for error reporting)\\n    val : int or float\\n        The value to check\\n    min_val : int\\n        Minimum allowed value (val < min_val will result in a ValueError)\",\n",
       " 'Check if the `names` parameter contains duplicates.\\n\\n    If duplicates are found, we issue a warning before returning.\\n\\n    Parameters\\n    ----------\\n    names : array-like or None\\n        An array containing a list of the names used for the output DataFrame.\\n\\n    Returns\\n    -------\\n    names : array-like or None\\n        The original `names` parameter.',\n",
       " 'Check whether or not the `columns` parameter\\n    could be converted into a MultiIndex.\\n\\n    Parameters\\n    ----------\\n    columns : array-like\\n        Object which may or may not be convertible into a MultiIndex\\n\\n    Returns\\n    -------\\n    boolean : Whether or not columns could become a MultiIndex',\n",
       " \"Check whether or not the 'usecols' parameter\\n    is a callable.  If so, enumerates the 'names'\\n    parameter and returns a set of indices for\\n    each entry in 'names' that evaluates to True.\\n    If not a callable, returns 'usecols'.\",\n",
       " \"Check whether or not the 'parse_dates' parameter\\n    is a non-boolean scalar. Raises a ValueError if\\n    that is the case.\",\n",
       " 'extract and return the names, index_names, col_names\\n            header is a list-of-lists returned from the parsers',\n",
       " 'Infer types of values, possibly casting\\n\\n        Parameters\\n        ----------\\n        values : ndarray\\n        na_values : set\\n        try_num_bool : bool, default try\\n           try to cast values to numeric (first preference) or boolean\\n\\n        Returns:\\n        --------\\n        converted : ndarray\\n        na_count : int',\n",
       " 'Cast values to specified type\\n\\n        Parameters\\n        ----------\\n        values : ndarray\\n        cast_type : string or np.dtype\\n           dtype to cast values to\\n        column : string\\n            column name - used only for error reporting\\n\\n        Returns\\n        -------\\n        converted : ndarray',\n",
       " 'Set the columns that should not undergo dtype conversions.\\n\\n        Currently, any column that is involved with date parsing will not\\n        undergo such conversions.',\n",
       " 'Workhorse function for processing nested list into DataFrame\\n\\n        Should be replaced by np.genfromtxt eventually?',\n",
       " 'Sets self._col_indices\\n\\n        usecols_key is used if there are string usecols.',\n",
       " 'Checks whether the file begins with the BOM character.\\n        If it does, remove it. In addition, if there is quoting\\n        in the field subsequent to the BOM, remove it as well\\n        because it technically takes place at the beginning of\\n        the name, not the middle of it.',\n",
       " 'Alert a user about a malformed row.\\n\\n        If `self.error_bad_lines` is True, the alert will be `ParserError`.\\n        If `self.warn_bad_lines` is True, the alert will be printed out.\\n\\n        Parameters\\n        ----------\\n        msg : The error message to display.\\n        row_num : The row number where the parsing error occurred.\\n                  Because this row number is displayed, we 1-index,\\n                  even though we 0-index internally.',\n",
       " 'Wrapper around iterating through `self.data` (CSV source).\\n\\n        When a CSV error is raised, we check for specific\\n        error messages that allow us to customize the\\n        error message displayed to the user.\\n\\n        Parameters\\n        ----------\\n        row_num : The row number of the line being parsed.',\n",
       " 'Iterate through the lines and remove any that are\\n        either empty or contain only one whitespace value\\n\\n        Parameters\\n        ----------\\n        lines : array-like\\n            The array of lines that we are to filter.\\n\\n        Returns\\n        -------\\n        filtered_lines : array-like\\n            The same array of lines with the \"empty\" ones removed.',\n",
       " 'Try several cases to get lines:\\n\\n        0) There are headers on row 0 and row 1 and their\\n        total summed lengths equals the length of the next line.\\n        Treat row 0 as columns and row 1 as indices\\n        1) Look for implicit index: there are more columns\\n        on row 1 than row 0. If this is true, assume that row\\n        1 lists index columns and row 0 lists normal columns.\\n        2) Get index from the columns if it was listed.',\n",
       " 'For those classes for which we use ::\\n\\n    :template: autosummary/class_without_autosummary.rst\\n\\n    the documented attributes/methods have to be listed in the class\\n    docstring. However, if one of those lists is empty, we use \\'None\\',\\n    which then generates warnings in sphinx / ugly html output.\\n    This \"autodoc-process-docstring\" event connector removes that part\\n    from the processed docstring.',\n",
       " 'Pack object `o` and write it to `stream`\\n\\n    See :class:`Packer` for options.',\n",
       " 'Construct concatenation plan for given block manager and indexers.\\n\\n    Parameters\\n    ----------\\n    mgr : BlockManager\\n    indexers : dict of {axis: indexer}\\n\\n    Returns\\n    -------\\n    plan : list of (BlockPlacement, JoinUnit) tuples',\n",
       " 'Concatenate values from several join units along selected axis.',\n",
       " 'Return dtype and N/A values to use when concatenating specified units.\\n\\n    Returned N/A value may be None which means there was no casting involved.\\n\\n    Returns\\n    -------\\n    dtype\\n    na',\n",
       " 'Check if the join units consist of blocks of uniform type that can\\n    be concatenated using Block.concat_same_type instead of the generic\\n    concatenate_join_units (which uses `_concat._concat_compat`).',\n",
       " \"Reduce join_unit's shape along item axis to length.\\n\\n    Extra items that didn't fit are returned as a separate block.\",\n",
       " 'Combine multiple concatenation plans into one.\\n\\n    existing_plan is updated in-place.',\n",
       " 'Temporarily set a parameter value using the with statement.\\n        Aliasing allowed.',\n",
       " 'Convert from datetime to SIF. http://www.stata.com/help.cgi?datetime\\n\\n    Parameters\\n    ----------\\n    dates : Series\\n        Series or array containing datetime.datetime or datetime64[ns] to\\n        convert to the Stata Internal Format given by fmt\\n    fmt : str\\n        The format to convert to. Can be, tc, td, tw, tm, tq, th, ty',\n",
       " 'Convert dtype types to stata types. Returns the byte of the given ordinal.\\n    See TYPE_MAP and comments for an explanation. This is also explained in\\n    the dta spec.\\n    1 - 244 are strings of this length\\n                         Pandas    Stata\\n    251 - for int8      byte\\n    252 - for int16     int\\n    253 - for int32     long\\n    254 - for float32   float\\n    255 - for double    double\\n\\n    If there are dates to convert, then dtype will already have the correct\\n    type inserted.',\n",
       " 'Map numpy dtype to stata\\'s default format for this type. Not terribly\\n    important since users can change this in Stata. Semantics are\\n\\n    object  -> \"%DDs\" where DD is the length of the string.  If not a string,\\n                raise ValueError\\n    float64 -> \"%10.0g\"\\n    float32 -> \"%9.0g\"\\n    int64   -> \"%9.0g\"\\n    int32   -> \"%12.0g\"\\n    int16   -> \"%8.0g\"\\n    int8    -> \"%8.0g\"\\n    strl    -> \"%9s\"',\n",
       " \"Takes a bytes instance and pads it with null bytes until it's length chars.\",\n",
       " 'Parameters\\n        ----------\\n        byteorder : str\\n            Byte order of the output\\n        encoding : str\\n            File encoding\\n\\n        Returns\\n        -------\\n        value_label : bytes\\n            Bytes containing the formatted value label',\n",
       " 'Helper to call encode before writing to file for Python 3 compat.',\n",
       " 'Check for categorical columns, retain categorical information for\\n        Stata file and convert categorical data to int',\n",
       " 'Checks floating point data columns for nans, and replaces these with\\n        the generic Stata for missing value (.)',\n",
       " 'Checks column names to ensure that they are valid Stata column names.\\n        This includes checks for:\\n            * Non-string names\\n            * Stata keywords\\n            * Variables that start with numbers\\n            * Variables with names that are too long\\n\\n        When an illegal variable name is detected, it is converted, and if\\n        dates are exported, the variable name is propagated to the date\\n        conversion dictionary',\n",
       " 'Close the file if it was created by the writer.\\n\\n        If a buffer or file-like object was passed in, for example a GzipFile,\\n        then leave this file open for the caller to close. In either case,\\n        attempt to flush the file contents to ensure they are written to disk\\n        (if supported)',\n",
       " 'Generates the binary blob of GSOs that is written to the dta file.\\n\\n        Parameters\\n        ----------\\n        gso_table : OrderedDict\\n            Ordered dictionary (str, vo)\\n\\n        Returns\\n        -------\\n        gso : bytes\\n            Binary content of dta file to be placed between strl tags\\n\\n        Notes\\n        -----\\n        Output format depends on dta version.  117 uses two uint32s to\\n        express v and o while 118+ uses a uint32 for v and a uint64 for o.',\n",
       " 'Called twice during file write. The first populates the values in\\n        the map with 0s.  The second call writes the final map locations when\\n        all blocks have been written.',\n",
       " 'Update column names for conversion to strl if they might have been\\n        changed to comply with Stata naming rules',\n",
       " 'Convert columns to StrLs if either very large or in the\\n        convert_strl variable',\n",
       " 'Register Pandas Formatters and Converters with matplotlib\\n\\n    This function modifies the global ``matplotlib.units.registry``\\n    dictionary. Pandas adds custom converters for\\n\\n    * pd.Timestamp\\n    * pd.Period\\n    * np.datetime64\\n    * datetime.datetime\\n    * datetime.date\\n    * datetime.time\\n\\n    See Also\\n    --------\\n    deregister_matplotlib_converter',\n",
       " \"Remove pandas' formatters and converters\\n\\n    Removes the custom converters added by :func:`register`. This\\n    attempts to set the state of the registry back to the state before\\n    pandas registered its own units. Converters for pandas' own types like\\n    Timestamp and Period are removed completely. Converters for types\\n    pandas overwrites, like ``datetime.datetime``, are restored to their\\n    original value.\\n\\n    See Also\\n    --------\\n    deregister_matplotlib_converters\",\n",
       " 'Convert :mod:`datetime` to the Gregorian date as UTC float days,\\n    preserving hours, minutes, seconds and microseconds.  Return value\\n    is a :func:`float`.',\n",
       " 'Returns a default spacing between consecutive ticks for annual data.',\n",
       " 'Returns the indices where the given period changes.\\n\\n    Parameters\\n    ----------\\n    dates : PeriodIndex\\n        Array of intervals to monitor.\\n    period : string\\n        Name of the period to monitor.',\n",
       " \"Returns true if the ``label_flags`` indicate there is at least one label\\n    for this level.\\n\\n    if the minimum view limit is not an exact integer, then the first tick\\n    label won't be shown, so we must adjust for that.\",\n",
       " 'Return the time of day as a formatted string.\\n\\n        Parameters\\n        ----------\\n        x : float\\n            The time of day specified as seconds since 00:00 (midnight),\\n            with up to microsecond precision.\\n        pos\\n            Unused\\n\\n        Returns\\n        -------\\n        str\\n            A string in HH:MM:SS.mmmuuu format. Microseconds,\\n            milliseconds and seconds are only displayed if non-zero.',\n",
       " 'Return the :class:`~matplotlib.units.AxisInfo` for *unit*.\\n\\n        *unit* is a tzinfo instance or None.\\n        The *axis* argument is required but not used.',\n",
       " 'Sets the view limits to the nearest multiples of base that contain the\\n        data.',\n",
       " \"Sets index names to 'index' for regular, or 'level_x' for Multi\",\n",
       " 'Find the appropriate name to pin to an operation result.  This result\\n    should always be either an Index or a Series.\\n\\n    Parameters\\n    ----------\\n    left : {Series, Index}\\n    right : object\\n\\n    Returns\\n    -------\\n    name : object\\n        Usually a string',\n",
       " 'Try to find a name to attach to the result of an operation between\\n    a and b.  If only one of these has a `name` attribute, return that\\n    name.  Otherwise return a consensus name if they match of None if\\n    they have different names.\\n\\n    Parameters\\n    ----------\\n    a : object\\n    b : object\\n\\n    Returns\\n    -------\\n    name : str or None\\n\\n    See Also\\n    --------\\n    pandas.core.common.consensus_name_attr',\n",
       " 'Cast non-pandas objects to pandas types to unify behavior of arithmetic\\n    and comparison operations.\\n\\n    Parameters\\n    ----------\\n    obj: object\\n\\n    Returns\\n    -------\\n    out : object\\n\\n    Notes\\n    -----\\n    Be careful to call this *after* determining the `name` attribute to be\\n    attached to the result of the arithmetic operation.',\n",
       " 'Return a binary method that always raises a TypeError.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    invalid_op : function',\n",
       " 'Find the keyword arguments to pass to numexpr for the given operation.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    eval_kwargs : dict\\n\\n    Examples\\n    --------\\n    >>> _gen_eval_kwargs(\"__add__\")\\n    {}\\n\\n    >>> _gen_eval_kwargs(\"rtruediv\")\\n    {\\'reversed\\': True, \\'truediv\\': True}',\n",
       " 'Find the appropriate fill value to use when filling in undefined values\\n    in the results of the given operation caused by operating on\\n    (generally dividing by) zero.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    fill_value : {None, np.nan, np.inf}',\n",
       " 'Find the operation string, if any, to pass to numexpr for this\\n    operation.\\n\\n    Parameters\\n    ----------\\n    op : binary operator\\n    cls : class\\n\\n    Returns\\n    -------\\n    op_str : string or None',\n",
       " 'Find the name to attach to this method according to conventions\\n    for special and non-special methods.\\n\\n    Parameters\\n    ----------\\n    op : binary operator\\n    special : bool\\n\\n    Returns\\n    -------\\n    op_name : str',\n",
       " \"Make the appropriate substitutions for the given operation and class-typ\\n    into either _flex_doc_SERIES or _flex_doc_FRAME to return the docstring\\n    to attach to a generated method.\\n\\n    Parameters\\n    ----------\\n    op_name : str {'__add__', '__sub__', ... '__eq__', '__ne__', ...}\\n    typ : str {series, 'dataframe']}\\n\\n    Returns\\n    -------\\n    doc : str\",\n",
       " 'If a non-None fill_value is given, replace null entries in left and right\\n    with this value, but only in positions where _one_ of left/right is null,\\n    not both.\\n\\n    Parameters\\n    ----------\\n    left : array-like\\n    right : array-like\\n    fill_value : object\\n\\n    Returns\\n    -------\\n    left : array-like\\n    right : array-like\\n\\n    Notes\\n    -----\\n    Makes copies if fill_value is not None',\n",
       " 'Apply the function `op` to only non-null points in x and y.\\n\\n    Parameters\\n    ----------\\n    x : array-like\\n    y : array-like\\n    op : binary operation\\n    allowed_types : class or tuple of classes\\n\\n    Returns\\n    -------\\n    result : ndarray[bool]',\n",
       " 'If the given arithmetic operation fails, attempt it again on\\n    only the non-null elements of the input array(s).\\n\\n    Parameters\\n    ----------\\n    x : np.ndarray\\n    y : np.ndarray, Series, Index\\n    op : binary operator',\n",
       " 'If a comparison has mismatched types and is not necessarily meaningful,\\n    follow python3 conventions by:\\n\\n        - returning all-False for equality\\n        - returning all-True for inequality\\n        - raising TypeError otherwise\\n\\n    Parameters\\n    ----------\\n    left : array-like\\n    right : scalar, array-like\\n    op : operator.{eq, ne, lt, le, gt}\\n\\n    Raises\\n    ------\\n    TypeError : on inequality comparisons',\n",
       " 'Identify cases where a DataFrame operation should dispatch to its\\n    Series counterpart.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : DataFrame\\n    op : binary operator\\n\\n    Returns\\n    -------\\n    override : bool',\n",
       " 'Evaluate the frame operation func(left, right) by evaluating\\n    column-by-column, dispatching to the Series implementation.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : scalar or DataFrame\\n    func : arithmetic or comparison operator\\n    str_rep : str or None, default None\\n    axis : {None, 0, 1, \"index\", \"columns\"}\\n\\n    Returns\\n    -------\\n    DataFrame',\n",
       " 'Wrap Series left in the given index_class to delegate the operation op\\n    to the index implementation.  DatetimeIndex and TimedeltaIndex perform\\n    type checking, timezone handling, overflow checks, etc.\\n\\n    Parameters\\n    ----------\\n    op : binary operator (operator.add, operator.sub, ...)\\n    left : Series\\n    right : object\\n    index_class : DatetimeIndex or TimedeltaIndex\\n\\n    Returns\\n    -------\\n    result : object, usually DatetimeIndex, TimedeltaIndex, or Series',\n",
       " 'Assume that left or right is a Series backed by an ExtensionArray,\\n    apply the operator defined by op.',\n",
       " 'Find the appropriate operation-wrappers to use when defining flex/special\\n    arithmetic, boolean, and comparison operations with the given class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n\\n    Returns\\n    -------\\n    arith_flex : function or None\\n    comp_flex : function or None\\n    arith_special : function\\n    comp_special : function\\n    bool_special : function\\n\\n    Notes\\n    -----\\n    None is only returned for SparseArray',\n",
       " 'Adds the full suite of special arithmetic methods (``__add__``,\\n    ``__sub__``, etc.) to the class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n        special methods will be defined and pinned to this class',\n",
       " 'Adds the full suite of flex arithmetic methods (``pow``, ``mul``, ``add``)\\n    to the class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n        flex methods will be defined and pinned to this class',\n",
       " 'If the raw op result has a non-None name (e.g. it is an Index object) and\\n    the name argument is None, then passing name to the constructor will\\n    not be enough; we still need to override the name attribute.',\n",
       " 'divmod returns a tuple of like indexed series instead of a single series.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " \"Apply binary operator `func` to self, other using alignment and fill\\n    conventions determined by the fill_value, axis, and level kwargs.\\n\\n    Parameters\\n    ----------\\n    self : DataFrame\\n    other : Series\\n    func : binary operator\\n    fill_value : object, default None\\n    axis : {0, 1, 'columns', 'index', None}, default None\\n    level : int or None, default None\\n\\n    Returns\\n    -------\\n    result : DataFrame\",\n",
       " 'convert rhs to meet lhs dims if input is list, tuple or np.ndarray',\n",
       " 'For SparseSeries operation, coerce to float64 if the result is expected\\n    to have NaN or inf values\\n\\n    Parameters\\n    ----------\\n    left : SparseArray\\n    right : SparseArray\\n    opname : str\\n\\n    Returns\\n    -------\\n    left : SparseArray\\n    right : SparseArray',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.',\n",
       " 'If a `periods` argument is passed to the Datetime/Timedelta Array/Index\\n    constructor, cast it to an integer.\\n\\n    Parameters\\n    ----------\\n    periods : None, float, int\\n\\n    Returns\\n    -------\\n    periods : None or int\\n\\n    Raises\\n    ------\\n    TypeError\\n        if periods is None, float, or int',\n",
       " 'Check that the `closed` argument is among [None, \"left\", \"right\"]\\n\\n    Parameters\\n    ----------\\n    closed : {None, \"left\", \"right\"}\\n\\n    Returns\\n    -------\\n    left_closed : bool\\n    right_closed : bool\\n\\n    Raises\\n    ------\\n    ValueError : if argument is not among valid values',\n",
       " 'If the user passes a freq and another freq is inferred from passed data,\\n    require that they match.\\n\\n    Parameters\\n    ----------\\n    freq : DateOffset or None\\n    inferred_freq : DateOffset or None\\n    freq_infer : bool\\n\\n    Returns\\n    -------\\n    freq : DateOffset or None\\n    freq_infer : bool\\n\\n    Notes\\n    -----\\n    We assume at this point that `maybe_infer_freq` has been called, so\\n    `freq` is either a DateOffset object or None.',\n",
       " 'Comparing a DateOffset to the string \"infer\" raises, so we need to\\n    be careful about comparisons.  Make a dummy variable `freq_infer` to\\n    signify the case where the given freq is \"infer\" and set freq to None\\n    to avoid comparison trouble later on.\\n\\n    Parameters\\n    ----------\\n    freq : {DateOffset, None, str}\\n\\n    Returns\\n    -------\\n    freq : {DateOffset, None}\\n    freq_infer : bool',\n",
       " 'Helper for coercing an input scalar or array to i8.\\n\\n    Parameters\\n    ----------\\n    other : 1d array\\n    to_utc : bool, default False\\n        If True, convert the values to UTC before extracting the i8 values\\n        If False, extract the i8 values directly.\\n\\n    Returns\\n    -------\\n    i8 1d array',\n",
       " 'Construct a scalar type from a string.\\n\\n        Parameters\\n        ----------\\n        value : str\\n\\n        Returns\\n        -------\\n        Period, Timestamp, or Timedelta, or NaT\\n            Whatever the type of ``self._scalar_type`` is.\\n\\n        Notes\\n        -----\\n        This should call ``self._check_compatible_with`` before\\n        unboxing the result.',\n",
       " \"Unbox the integer value of a scalar `value`.\\n\\n        Parameters\\n        ----------\\n        value : Union[Period, Timestamp, Timedelta]\\n\\n        Returns\\n        -------\\n        int\\n\\n        Examples\\n        --------\\n        >>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP\\n        10000000000\",\n",
       " 'Verify that `self` and `other` are compatible.\\n\\n        * DatetimeArray verifies that the timezones (if any) match\\n        * PeriodArray verifies that the freq matches\\n        * Timedelta has no verification\\n\\n        In each case, NaT is considered compatible.\\n\\n        Parameters\\n        ----------\\n        other\\n\\n        Raises\\n        ------\\n        Exception',\n",
       " 'This getitem defers to the underlying array, which by-definition can\\n        only handle list-likes, slices, and integer scalars',\n",
       " 'Repeat elements of an array.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.repeat',\n",
       " \"Return a Series containing counts of unique values.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don't include counts of NaT values.\\n\\n        Returns\\n        -------\\n        Series\",\n",
       " 'Parameters\\n        ----------\\n        result : a ndarray\\n        fill_value : object, default iNaT\\n        convert : string/dtype or None\\n\\n        Returns\\n        -------\\n        result : ndarray with values replace by the fill_value\\n\\n        mask the result if needed, convert to the provided dtype if its not\\n        None\\n\\n        This is an internal routine.',\n",
       " 'Validate that a frequency is compatible with the values of a given\\n        Datetime Array/Index or Timedelta Array/Index\\n\\n        Parameters\\n        ----------\\n        index : DatetimeIndex or TimedeltaIndex\\n            The index on which to determine if the given frequency is valid\\n        freq : DateOffset\\n            The frequency to validate',\n",
       " \"Add a timedelta-like, Tick or TimedeltaIndex-like object\\n        to self, yielding an int64 numpy array\\n\\n        Parameters\\n        ----------\\n        delta : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]\\n\\n        Notes\\n        -----\\n        The result's name is set outside of _add_delta by the calling\\n        method (__add__ or __sub__), if necessary (i.e. for Indexes).\",\n",
       " 'Add a delta of a timedeltalike\\n        return the i8 result view',\n",
       " 'Add a delta of a TimedeltaIndex\\n        return the i8 result view',\n",
       " 'Subtract a Period Array/Index from self.  This is only valid if self\\n        is itself a Period Array/Index, raises otherwise.  Both objects must\\n        have the same frequency.\\n\\n        Parameters\\n        ----------\\n        other : PeriodIndex or PeriodArray\\n\\n        Returns\\n        -------\\n        result : np.ndarray[object]\\n            Array of DateOffset objects; nulls represented by NaT.',\n",
       " 'Add or subtract array-like of integers equivalent to applying\\n        `_time_shift` pointwise.\\n\\n        Parameters\\n        ----------\\n        other : Index, ExtensionArray, np.ndarray\\n            integer-dtype\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self',\n",
       " 'Add or subtract array-like of DateOffset objects\\n\\n        Parameters\\n        ----------\\n        other : Index, np.ndarray\\n            object-dtype containing pd.DateOffset objects\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self',\n",
       " 'Shift each value by `periods`.\\n\\n        Note this is different from ExtensionArray.shift, which\\n        shifts the *position* of each element, padding the end with\\n        missing values.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to shift by.\\n        freq : pandas.DateOffset, pandas.Timedelta, or string\\n            Frequency increment to shift by.',\n",
       " 'Return the minimum value of the Array or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Index.min : Return the minimum value in an Index.\\n        Series.min : Return the minimum value in a Series.',\n",
       " 'Return the maximum value of the Array or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Index.max : Return the maximum value in an Index.\\n        Series.max : Return the maximum value in a Series.',\n",
       " 'Wrap comparison operations to convert Period-like to PeriodDtype',\n",
       " 'Helper function to render a consistent error message when raising\\n    IncompatibleFrequency.\\n\\n    Parameters\\n    ----------\\n    left : PeriodArray\\n    right : DateOffset, Period, ndarray, or timedelta-like\\n\\n    Raises\\n    ------\\n    IncompatibleFrequency',\n",
       " 'If both a dtype and a freq are available, ensure they match.  If only\\n    dtype is available, extract the implied freq.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype\\n    freq : DateOffset or None\\n\\n    Returns\\n    -------\\n    freq : DateOffset\\n\\n    Raises\\n    ------\\n    ValueError : non-period dtype\\n    IncompatibleFrequency : mismatch between dtype and freq',\n",
       " \"Convert an datetime-like array to values Period ordinals.\\n\\n    Parameters\\n    ----------\\n    data : Union[Series[datetime64[ns]], DatetimeIndex, ndarray[datetime64ns]]\\n    freq : Optional[Union[str, Tick]]\\n        Must match the `freq` on the `data` if `data` is a DatetimeIndex\\n        or Series.\\n    tz : Optional[tzinfo]\\n\\n    Returns\\n    -------\\n    ordinals : ndarray[int]\\n    freq : Tick\\n        The frequencey extracted from the Series or DatetimeIndex if that's\\n        used.\",\n",
       " 'Construct a PeriodArray from a datetime64 array\\n\\n        Parameters\\n        ----------\\n        data : ndarray[datetime64[ns], datetime64[ns, tz]]\\n        freq : str or Tick\\n        tz : tzinfo, optional\\n\\n        Returns\\n        -------\\n        PeriodArray[freq]',\n",
       " \"Cast to DatetimeArray/Index.\\n\\n        Parameters\\n        ----------\\n        freq : string or DateOffset, optional\\n            Target frequency. The default is 'D' for week or longer,\\n            'S' otherwise\\n        how : {'s', 'e', 'start', 'end'}\\n\\n        Returns\\n        -------\\n        DatetimeArray/Index\",\n",
       " 'Shift each value by `periods`.\\n\\n        Note this is different from ExtensionArray.shift, which\\n        shifts the *position* of each element, padding the end with\\n        missing values.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to shift by.\\n        freq : pandas.DateOffset, pandas.Timedelta, or string\\n            Frequency increment to shift by.',\n",
       " 'Parameters\\n        ----------\\n        other : timedelta, Tick, np.timedelta64\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]',\n",
       " 'Parameters\\n        ----------\\n        other : TimedeltaArray or ndarray[timedelta64]\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]',\n",
       " 'Add a timedelta-like, Tick, or TimedeltaIndex-like object\\n        to self, yielding a new PeriodArray\\n\\n        Parameters\\n        ----------\\n        other : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : PeriodArray',\n",
       " 'Detect missing values. Treat None, NaN, INF, -INF as null.\\n\\n    Parameters\\n    ----------\\n    arr: ndarray or object value\\n\\n    Returns\\n    -------\\n    boolean ndarray or boolean',\n",
       " 'Parameters\\n    ----------\\n    arr: a numpy array\\n    fill_value: fill value, default to np.nan\\n\\n    Returns\\n    -------\\n    True if we can fill using this fill_value',\n",
       " 'infer the fill value for the nan/NaT from the provided\\n    scalar/ndarray/list-like if we are a NaT, return the correct dtyped\\n    element to provide proper block construction',\n",
       " 'if we have a compatible fill_value and arr dtype, then fill',\n",
       " 'Return array-like containing only true/non-NaN values, possibly empty.',\n",
       " 'Helper function to convert DataFrame and Series to matplotlib.table\\n\\n    Parameters\\n    ----------\\n    ax : Matplotlib axes object\\n    data : DataFrame or Series\\n        data for table contents\\n    kwargs : keywords, optional\\n        keyword arguments which passed to matplotlib.table.table.\\n        If `rowLabels` or `colLabels` is not specified, data index or column\\n        name will be used.\\n\\n    Returns\\n    -------\\n    matplotlib table object',\n",
       " 'fast version of transform, only applicable to\\n        builtin/cythonizable functions',\n",
       " 'Calcuate pct_change of each value to previous entry in group',\n",
       " 'sub-classes to define\\n        return a sliced object\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'If we have categorical groupers, then we want to make sure that\\n        we have a fully reindex-output to the levels. These may have not\\n        participated in the groupings (e.g. may have all been\\n        nan groups);\\n\\n        This can re-expand the output space',\n",
       " 'Overridden method to join grouped columns in output',\n",
       " \"Flatten an arbitrarily nested sequence.\\n\\n    Parameters\\n    ----------\\n    l : sequence\\n        The non string sequence to flatten\\n\\n    Notes\\n    -----\\n    This doesn't consider strings sequences.\\n\\n    Returns\\n    -------\\n    flattened : generator\",\n",
       " 'To avoid numpy DeprecationWarnings, cast float to integer where valid.\\n\\n    Parameters\\n    ----------\\n    val : scalar\\n\\n    Returns\\n    -------\\n    outval : scalar',\n",
       " 'Transform label or iterable of labels to array, for use in Index.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype\\n        If specified, use as dtype of the resulting array, otherwise infer.\\n\\n    Returns\\n    -------\\n    array',\n",
       " 'Evaluate possibly callable input using obj and kwargs if it is callable,\\n    otherwise return as it is.\\n\\n    Parameters\\n    ----------\\n    maybe_callable : possibly a callable\\n    obj : NDFrame\\n    **kwargs',\n",
       " 'Helper function for processing random_state arguments.\\n\\n    Parameters\\n    ----------\\n    state : int, np.random.RandomState, None.\\n        If receives an int, passes to np.random.RandomState() as seed.\\n        If receives an np.random.RandomState object, just returns object.\\n        If receives `None`, returns np.random.\\n        If receives anything else, raises an informative ValueError.\\n        Default None.\\n\\n    Returns\\n    -------\\n    np.random.RandomState',\n",
       " 'Returns a function that will map names/labels, dependent if mapper\\n    is a dict, Series or just a function.',\n",
       " 'return the correct fill value for the dtype of the values',\n",
       " 'utility to get the values view, mask, dtype\\n    if necessary copy and mask using the specified fill_value\\n    copy = True will force the copy',\n",
       " 'Return the missing value for `values`\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis : int or None\\n        axis for the reduction\\n\\n    Returns\\n    -------\\n    result : scalar or ndarray\\n        For 1-D values, returns a scalar of the correct missing type.\\n        For 2-D values, returns a 1-D array where each element is missing.',\n",
       " 'Sum the elements along an axis ignoring NaNs\\n\\n    Parameters\\n    ----------\\n    values : ndarray[dtype]\\n    axis: int, optional\\n    skipna : bool, default True\\n    min_count: int, default 0\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : dtype\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, np.nan])\\n    >>> nanops.nansum(s)\\n    3.0',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 2])\\n    >>> nanops.nanmedian(s)\\n    2.0',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    --------\\n    result : int\\n        The index of max value in specified axis or -1 in the NA case\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\\n    >>> nanops.nanargmax(s)\\n    4',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    --------\\n    result : int\\n        The index of min value in specified axis or -1 in the NA case\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\\n    >>> nanops.nanargmin(s)\\n    0',\n",
       " 'Parameters\\n    ----------\\n    values : ndarray[dtype]\\n    axis: int, optional\\n    skipna : bool, default True\\n    min_count: int, default 0\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : dtype\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan])\\n    >>> nanops.nanprod(s)\\n    6.0\\n\\n    Returns\\n    --------\\n    The product of all elements on a given axis. ( NaNs are treated as 1)',\n",
       " 'Wraper for np.percentile that skips missing values, specialized to\\n    1-dimensional case.\\n\\n    Parameters\\n    ----------\\n    values : array over which to find quantiles\\n    mask : ndarray[bool]\\n        locations in values that should be considered missing\\n    q : scalar or array of quantile indices to find\\n    na_value : scalar\\n        value to return for empty or all-null values\\n    interpolation : str\\n\\n    Returns\\n    -------\\n    quantiles : scalar or array',\n",
       " 'Wraper for np.percentile that skips missing values.\\n\\n    Parameters\\n    ----------\\n    values : array over which to find quantiles\\n    q : scalar or array of quantile indices to find\\n    axis : {0, 1}\\n    na_value : scalar\\n        value to return for empty or all-null values\\n    mask : ndarray[bool]\\n        locations in values that should be considered missing\\n    ndim : {1, 2}\\n    interpolation : str\\n\\n    Returns\\n    -------\\n    quantiles : scalar or array',\n",
       " 'r\"\"\"\\n    Read text from clipboard and pass to read_csv. See read_csv for the\\n    full argument list\\n\\n    Parameters\\n    ----------\\n    sep : str, default \\'\\\\s+\\'\\n        A string or regex delimiter. The default of \\'\\\\s+\\' denotes\\n        one or more whitespace characters.\\n\\n    Returns\\n    -------\\n    parsed : DataFrame',\n",
       " 'Get an iterator given an integer, slice or container.\\n\\n    Parameters\\n    ----------\\n    skiprows : int, slice, container\\n        The iterator to use to skip rows; can also be a slice.\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `skiprows` is not a slice, integer, or Container\\n\\n    Returns\\n    -------\\n    it : iterable\\n        A proper iterator to use to skip rows of a DataFrame.',\n",
       " 'Try to read from a url, file or string.\\n\\n    Parameters\\n    ----------\\n    obj : str, unicode, or file-like\\n\\n    Returns\\n    -------\\n    raw_text : str',\n",
       " \"Build an xpath expression to simulate bs4's ability to pass in kwargs to\\n    search for attributes when using the lxml parser.\\n\\n    Parameters\\n    ----------\\n    attrs : dict\\n        A dict of HTML attributes. These are NOT checked for validity.\\n\\n    Returns\\n    -------\\n    expr : unicode\\n        An XPath expression that checks for the given HTML attributes.\",\n",
       " 'Choose the parser based on the input flavor.\\n\\n    Parameters\\n    ----------\\n    flavor : str\\n        The type of parser to use. This must be a valid backend.\\n\\n    Returns\\n    -------\\n    cls : _HtmlFrameParser subclass\\n        The parser class based on the requested input flavor.\\n\\n    Raises\\n    ------\\n    ValueError\\n        * If `flavor` is not a valid backend.\\n    ImportError\\n        * If you do not have the requested `flavor`',\n",
       " 'Parse and return all tables from the DOM.\\n\\n        Returns\\n        -------\\n        list of parsed (header, body, footer) tuples from tables.',\n",
       " 'Given a list of <tr>s, return a list of text rows.\\n\\n        Parameters\\n        ----------\\n        rows : list of node-like\\n            List of <tr>s\\n\\n        Returns\\n        -------\\n        list of list\\n            Each returned row is a list of str text.\\n\\n        Notes\\n        -----\\n        Any cell with ``rowspan`` or ``colspan`` will have its contents copied\\n        to subsequent cells.',\n",
       " 'Return list of tables, potentially removing hidden elements\\n\\n        Parameters\\n        ----------\\n        tbl_list : list of node-like\\n            Type of list elements will vary depending upon parser used\\n        attr_name : str\\n            Name of the accessor for retrieving HTML attributes\\n\\n        Returns\\n        -------\\n        list of node-like\\n            Return type matches `tbl_list`',\n",
       " 'Raises\\n        ------\\n        ValueError\\n            * If a URL that lxml cannot parse is passed.\\n\\n        Exception\\n            * Any other ``Exception`` thrown. For example, trying to parse a\\n              URL that is syntactically correct on a machine with no internet\\n              connection will fail.\\n\\n        See Also\\n        --------\\n        pandas.io.html._HtmlFrameParser._build_doc',\n",
       " 'Parameters\\n    ----------\\n    l : list of arrays\\n\\n    Returns\\n    -------\\n    a set of kinds that exist in this list of arrays',\n",
       " 'return appropriate class of Series concat\\n    input is either dict or array-like',\n",
       " 'return appropriate class of DataFrame-like concat\\n    if all blocks are sparse, return SparseDataFrame\\n    otherwise, return 1st obj',\n",
       " \"provide concatenation of an array of arrays each of which is a single\\n    'normalized' dtypes (in that for example, if it's object, then it is a\\n    non-datetimelike and provide a combined dtype for the resulting array that\\n    preserves the overall dtype if possible)\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes\",\n",
       " 'Concatenate an object/categorical array of arrays, each of which is a\\n    single dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : int\\n        Axis to provide concatenation in the current implementation this is\\n        always 0, e.g. we only have 1D categoricals\\n\\n    Returns\\n    -------\\n    Categorical\\n        A single array, preserving the combined dtypes',\n",
       " 'provide concatenation of an datetimelike array of arrays each of which is a\\n    single M8[ns], datetimet64[ns, tz] or m8[ns] dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n    typs : set of to_concat dtypes\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes',\n",
       " 'concat DatetimeIndex with the same tz\\n    all inputs must be DatetimeIndex\\n    it is used in DatetimeIndex.append also',\n",
       " 'concat all inputs as object. DatetimeIndex, TimedeltaIndex and\\n    PeriodIndex are converted to object dtype before concatenation',\n",
       " 'provide concatenation of an sparse/dense array of arrays each of which is a\\n    single dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n    typs : set of to_concat dtypes\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes',\n",
       " 'Concatenates multiple RangeIndex instances. All members of \"indexes\" must\\n    be of type RangeIndex; result will be RangeIndex if possible, Int64Index\\n    otherwise. E.g.:\\n    indexes = [RangeIndex(3), RangeIndex(3, 6)] -> RangeIndex(6)\\n    indexes = [RangeIndex(3), RangeIndex(4, 6)] -> Int64Index([0,1,2,4,5])',\n",
       " 'Given an index, find the level length for each element.\\n\\n    Optional argument is a list of index positions which\\n    should not be visible.\\n\\n    Result is a dictionary of (level, inital_position): span',\n",
       " 'Convert the DataFrame in `self.data` and the attrs from `_build_styles`\\n        into a dictionary of {head, body, uuid, cellstyle}.',\n",
       " \"Update the state of the Styler.\\n\\n        Collects a mapping of {index_label: ['<property>: <value>']}.\\n\\n        attrs : Series or DataFrame\\n        should contain strings of '<property>: <value>;<prop2>: <val2>'\\n        Whitespace shouldn't matter and the final trailing ';' shouldn't\\n        matter.\",\n",
       " 'Execute the style functions built up in `self._todo`.\\n\\n        Relies on the conventions that all style functions go through\\n        .apply or .applymap. The append styles to apply as tuples of\\n\\n        (application method, *args, **kwargs)',\n",
       " 'Hide columns from rendering.\\n\\n        .. versionadded:: 0.23.0\\n\\n        Parameters\\n        ----------\\n        subset : IndexSlice\\n            An argument to ``DataFrame.loc`` that identifies which columns\\n            are hidden.\\n\\n        Returns\\n        -------\\n        self : Styler',\n",
       " 'Shade the background ``null_color`` for missing values.\\n\\n        Parameters\\n        ----------\\n        null_color : str\\n\\n        Returns\\n        -------\\n        self : Styler',\n",
       " 'Factory function for creating a subclass of ``Styler``\\n        with a custom template and Jinja environment.\\n\\n        Parameters\\n        ----------\\n        searchpath : str or list\\n            Path or paths of directories containing the templates\\n        name : str\\n            Name of your custom template to use for rendering\\n\\n        Returns\\n        -------\\n        MyStyler : subclass of Styler\\n            Has the correct ``env`` and ``template`` class attributes set.',\n",
       " 'Parameters\\n        ----------\\n        dtype : ExtensionDtype',\n",
       " 'Parameters\\n        ----------\\n        dtype : PandasExtensionDtype or string\\n\\n        Returns\\n        -------\\n        return the first matching dtype, otherwise return None',\n",
       " 'Return a string representation for a particular object.',\n",
       " \"provide compat for construction of strings to numpy datetime64's with\\n    tz-changes in 1.11 that make '2015-01-01 09:00:00Z' show a deprecation\\n    warning, when need to pass '2015-01-01 09:00:00'\",\n",
       " \"provide compat for construction of an array of strings to a\\n    np.array(..., dtype=np.datetime64(..))\\n    tz-changes in 1.11 that make '2015-01-01 09:00:00Z' show a deprecation\\n    warning, when need to pass '2015-01-01 09:00:00'\",\n",
       " 'Check if key is a float and has a decimal. If it has, return False.',\n",
       " 'we always want to get an index value, never a value',\n",
       " 'Determines if two Index objects contain the same elements.',\n",
       " 'ensure that the where is a Term or a list of Term\\n    this makes sure that we are capturing the scope of variables\\n    that are passed\\n    create the terms here with a frame_level=2 (we are 2 levels down)',\n",
       " 'Check if a given group is a metadata group for a given parent_group.',\n",
       " 'coerce the values to a DatetimeIndex if tz is set\\n    preserve the input shape if possible\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    tz : string/pickled tz object\\n    preserve_UTC : boolean,\\n        preserve the UTC of the result\\n    coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray',\n",
       " 'we take a string-like that is object dtype and coerce to a fixed size\\n    string type\\n\\n    Parameters\\n    ----------\\n    data : a numpy array of object dtype\\n    encoding : None or string-encoding\\n    errors : handler for encoding errors\\n    itemsize : integer, optional, defaults to the max length of the strings\\n\\n    Returns\\n    -------\\n    data in a fixed-length string dtype, encoded to bytes if needed',\n",
       " \"inverse of _convert_string_array\\n\\n    Parameters\\n    ----------\\n    data : fixed length string dtyped array\\n    nan_rep : the storage repr of NaN, optional\\n    encoding : the encoding of the data, optional\\n    errors : handler for encoding errors, default 'strict'\\n\\n    Returns\\n    -------\\n    an object array of the decoded data\",\n",
       " \"check for existence of this key\\n              can match the exact pathname or the pathnm w/o the leading '/'\",\n",
       " \"Open the file in the specified mode\\n\\n        Parameters\\n        ----------\\n        mode : {'a', 'w', 'r', 'r+'}, default 'a'\\n            See HDFStore docstring or tables.open_file for info about modes\",\n",
       " 'Force all buffered modifications to be written to disk.\\n\\n        Parameters\\n        ----------\\n        fsync : bool (default False)\\n          call ``os.fsync()`` on the file handle to force writing to disk.\\n\\n        Notes\\n        -----\\n        Without ``fsync=True``, flushing may not guarantee that the OS writes\\n        to disk. With fsync, the operation will block until the OS claims the\\n        file has been written; however, other caching layers may still\\n        interfere.',\n",
       " 'Retrieve pandas object stored in file\\n\\n        Parameters\\n        ----------\\n        key : object\\n\\n        Returns\\n        -------\\n        obj : same type as object stored in file',\n",
       " 'return the selection as an Index\\n\\n        Parameters\\n        ----------\\n        key : object\\n        where : list of Term (or convertible) objects, optional\\n        start : integer (defaults to None), row number to start selection\\n        stop  : integer (defaults to None), row number to stop selection',\n",
       " 'return a single column from the table. This is generally only useful to\\n        select an indexable\\n\\n        Parameters\\n        ----------\\n        key : object\\n        column: the column of interest\\n\\n        Exceptions\\n        ----------\\n        raises KeyError if the column is not found (or key is not a valid\\n            store)\\n        raises ValueError if the column can not be extracted individually (it\\n            is part of a data block)',\n",
       " 'Create a pytables index on the table\\n        Parameters\\n        ----------\\n        key : object (the node to index)\\n\\n        Exceptions\\n        ----------\\n        raises if the node is not a table',\n",
       " 'return a list of all the top-level nodes (that are not themselves a\\n        pandas storage object)',\n",
       " 'return the node with the key or None if it does not exist',\n",
       " 'return the storer object for a key, raise if not in the file',\n",
       " 'Print detailed information on the store.\\n\\n        .. versionadded:: 0.21.0',\n",
       " 'validate / deprecate formats; return the new kwargs',\n",
       " 'infer this column from the table: create and return a new object',\n",
       " 'set the values from this selection: take = take ownership',\n",
       " 'maybe set a string col itemsize:\\n               min_itemsize can be an integer or a dict with this columns name\\n               with an integer size',\n",
       " 'validate this column: return the compared against itemsize',\n",
       " 'set/update the info for this indexable with the key/value\\n            if there is a conflict raise/warn as needed',\n",
       " 'validate that kind=category does not change the categories',\n",
       " 'set the values from this selection: take = take ownership',\n",
       " 'validate that we have the same order as the existing & same dtype',\n",
       " 'set the data from this selection (and convert to the correct dtype\\n        if we can)',\n",
       " 'infer the axes of my storer\\n              return a boolean indicating if we have a valid storer or not',\n",
       " 'support fully deleting the node in its entirety (only) - where\\n        specification must be None',\n",
       " 'remove table keywords from kwargs and return\\n        raise if any keywords are passed which are not-None',\n",
       " 'write it as a collection of individual sparse series',\n",
       " 'validate that we can store the multi-index; reset and return the\\n        new object',\n",
       " 'return a tuple of my permutated axes, non_indexable at the front',\n",
       " 'return a dict of the kinds allowable columns for this object',\n",
       " 'write out a meta data array to the key as a fixed-format Series\\n\\n        Parameters\\n        ----------\\n        key : string\\n        values : ndarray',\n",
       " \"validate the min_itemisze doesn't contain items that are not in the\\n        axes this needs data_columns to be defined\",\n",
       " 'create and return the axes sniffed from the table: return boolean\\n        for success',\n",
       " 'take the input data_columns and min_itemize and create a data\\n        columns spec',\n",
       " 'create the description of the table from the axes & values',\n",
       " 'select coordinates (row numbers) from a table; return the\\n        coordinates object',\n",
       " 'return a single column from the table, generally only indexables\\n        are interesting',\n",
       " 'we have n indexable columns, with an arbitrary number of data\\n        axes',\n",
       " 'we form the data into a 2-d including indexes,values,mask\\n            write chunk-by-chunk',\n",
       " 'Parameters\\n        ----------\\n        rows : an empty memory space where we are putting the chunk\\n        indexes : an array of the indexes\\n        mask : an array of the masks\\n        values : an array of the values',\n",
       " \"Cast to a NumPy array with 'dtype'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray\\n            NumPy ndarray with 'dtype' for its dtype.\",\n",
       " 'Compute the ExtensionArray of unique values.\\n\\n        Returns\\n        -------\\n        uniques : ExtensionArray',\n",
       " 'Make an alias for a method of the underlying ExtensionArray.\\n\\n    Parameters\\n    ----------\\n    array_method : method on an Array class\\n\\n    Returns\\n    -------\\n    method',\n",
       " 'Create a comparison method that dispatches to ``cls.values``.',\n",
       " 'Determines if two Index objects contain the same elements.',\n",
       " 'Return the minimum value of the Index or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Series.min : Return the minimum value in a Series.',\n",
       " 'Returns the indices of the minimum values along an axis.\\n\\n        See `numpy.ndarray.argmin` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin',\n",
       " 'Return the maximum value of the Index or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Series.max : Return the maximum value in a Series.',\n",
       " 'Returns the indices of the maximum values along an axis.\\n\\n        See `numpy.ndarray.argmax` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax',\n",
       " 'Return a list of tuples of the (attr,formatted_value).',\n",
       " \"We don't allow integer or float indexing on datetime-like when using\\n        loc.\\n\\n        Parameters\\n        ----------\\n        key : label of the slice bound\\n        kind : {'ix', 'loc', 'getitem', 'iloc'} or None\",\n",
       " 'Add in the datetimelike methods (as we may have to override the\\n        superclass).',\n",
       " 'Compute boolean array of whether each index value is found in the\\n        passed set of values.\\n\\n        Parameters\\n        ----------\\n        values : set or sequence of values\\n\\n        Returns\\n        -------\\n        is_contained : ndarray (boolean dtype)',\n",
       " 'Return a summarized representation.\\n\\n        Parameters\\n        ----------\\n        name : str\\n            name to use in the summary representation\\n\\n        Returns\\n        -------\\n        String with a summarized representation of the index',\n",
       " 'Replaces values in a Series using the fill method specified when no\\n    replacement value is given in the replace method',\n",
       " 'Construct and returns axes if supplied in args/kwargs.\\n\\n        If require_all, raise if all axis arguments are not supplied\\n        return a tuple of (axes, kwargs).\\n\\n        sentinel specifies the default parameter when an axis is not\\n        supplied; useful to distinguish when a user explicitly passes None\\n        in scenarios where None has special meaning.',\n",
       " \"Return the space character free column resolvers of a dataframe.\\n\\n        Column names with spaces are 'cleaned up' so that they can be referred\\n        to by backtick quoting.\\n        Used in :meth:`DataFrame.eval`.\",\n",
       " 'Interchange axes and swap values axes appropriately.\\n\\n        Returns\\n        -------\\n        y : same as input',\n",
       " 'Swap levels i and j in a MultiIndex on a particular axis\\n\\n        Parameters\\n        ----------\\n        i, j : int, str (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        swapped : same type as caller (new object)\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.',\n",
       " 'Return the bool of a single element PandasObject.\\n\\n        This must be a boolean scalar value, either True or False.  Raise a\\n        ValueError if the PandasObject does not have exactly 1 element, or that\\n        element is not boolean',\n",
       " 'Check whether `key` is ambiguous.\\n\\n        By ambiguous, we mean that it matches both a level of the input\\n        `axis` and a label of the other axis.\\n\\n        Parameters\\n        ----------\\n        key: str or object\\n            label or level name\\n        axis: int, default 0\\n            Axis that levels are associated with (0 for index, 1 for columns)\\n\\n        Raises\\n        ------\\n        ValueError: `key` is ambiguous',\n",
       " 'Not a real Jupyter special repr method, but we use the same\\n        naming convention.',\n",
       " 'Get item from object for given key (DataFrame column, Panel slice,\\n        etc.). Returns default value if not found.\\n\\n        Parameters\\n        ----------\\n        key : object\\n\\n        Returns\\n        -------\\n        value : same type as items contained in object',\n",
       " 'Return the cached item, item represents a label indexer.',\n",
       " 'Set the _cacher attribute on the calling object with a weakref to\\n        cacher.',\n",
       " 'Return the cached item, item represents a positional indexer.',\n",
       " 'See if we need to update our parent cacher if clear, then clear our\\n        cache.\\n\\n        Parameters\\n        ----------\\n        clear : boolean, default False\\n            clear the item cache\\n        verify_is_copy : boolean, default True\\n            provide is_copy checks',\n",
       " 'Construct a slice of this container.\\n\\n        kind parameter is maintained for compatibility with Series slicing.',\n",
       " 'Check if we are a view, have a cacher, and are of mixed type.\\n        If so, then force a setitem_copy check.\\n\\n        Should be called just near setting a value\\n\\n        Will return a boolean if it we are a view and are cached, but a\\n        single-dtype meaning that the cacher should be updated following\\n        setting.',\n",
       " 'Return data corresponding to axis labels matching criteria.\\n\\n        .. deprecated:: 0.21.0\\n            Use df.loc[df.index.map(crit)] to select via labels\\n\\n        Parameters\\n        ----------\\n        crit : function\\n            To be called on each index (label). Should return True or False\\n        axis : int\\n\\n        Returns\\n        -------\\n        selection : same type as caller',\n",
       " \"Drop labels from specified axis. Used in the ``drop`` method\\n        internally.\\n\\n        Parameters\\n        ----------\\n        labels : single label or list-like\\n        axis : int or axis name\\n        level : int or level name, default None\\n            For MultiIndex\\n        errors : {'ignore', 'raise'}, default 'raise'\\n            If 'ignore', suppress error and existing labels are dropped.\",\n",
       " 'Replace self internals with result.\\n\\n        Parameters\\n        ----------\\n        verify_is_copy : boolean, default True\\n            provide is_copy checks',\n",
       " 'Propagate metadata from other to self.\\n\\n        Parameters\\n        ----------\\n        other : the object from which to get the attributes that we are going\\n            to propagate\\n        method : optional, a passed method name ; possibly to take different\\n            types of propagation actions based on this',\n",
       " 'After regular attribute access, try looking up the name\\n        This allows simpler access to columns for interactive use.',\n",
       " 'After regular attribute access, try setting the name\\n        This allows simpler access to columns for interactive use.',\n",
       " \"add the string-like attributes from the info_axis.\\n        If info_axis is a MultiIndex, it's first level values are used.\",\n",
       " 'Consolidate _data -- if the blocks have changed, then clear the\\n        cache',\n",
       " 'Compute NDFrame with \"consolidated\" internals (data of each dtype\\n        grouped together in a single ndarray).\\n\\n        Parameters\\n        ----------\\n        inplace : boolean, default False\\n            If False return new object, otherwise modify existing object\\n\\n        Returns\\n        -------\\n        consolidated : same type as caller',\n",
       " 'check whether we allow in-place setting with this type of value',\n",
       " 'Convert the frame to a dict of dtype -> Constructor Types that each has\\n        a homogeneous dtype.\\n\\n        .. deprecated:: 0.21.0\\n\\n        NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in\\n              as_matrix)\\n\\n        Parameters\\n        ----------\\n        copy : boolean, default True\\n\\n        Returns\\n        -------\\n        values : a dict of dtype -> Constructor Types',\n",
       " 'Return a dict of dtype -> Constructor Types that\\n        each is a homogeneous dtype.\\n\\n        Internal ONLY',\n",
       " 'Equivalent to public method `where`, except that `other` is not\\n        applied as a function even if callable. Used in __setitem__.',\n",
       " 'Validate percentiles (used by describe and quantile).',\n",
       " 'Add the operations to the cls; evaluate the doc strings again',\n",
       " 'Add the series only operations to the cls; evaluate the doc\\n        strings again.',\n",
       " 'Add the series or dataframe only operations to the cls; evaluate\\n        the doc strings again.',\n",
       " \"Retrieves the index of the first valid value.\\n\\n        Parameters\\n        ----------\\n        how : {'first', 'last'}\\n            Use this parameter to change between the first or last valid index.\\n\\n        Returns\\n        -------\\n        idx_first_valid : type of index\",\n",
       " 'Reset cached properties. If ``key`` is passed, only clears that key.',\n",
       " 'Generates the total memory usage for an object that returns\\n        either a value or Series of values',\n",
       " 'if arg is a string, then try to operate on it:\\n        - try to find a function (or attribute) on ourselves\\n        - try to find a numpy function\\n        - raise',\n",
       " 'provide an implementation for the aggregators\\n\\n        Parameters\\n        ----------\\n        arg : string, dict, function\\n        *args : args to pass on to the function\\n        **kwargs : kwargs to pass on to the function\\n\\n        Returns\\n        -------\\n        tuple of result, how\\n\\n        Notes\\n        -----\\n        how can be a string describe the required post-processing, or\\n        None if not required',\n",
       " 'return a new object with the replacement attributes',\n",
       " 'Return the size of the dtype of the item of the underlying data.\\n\\n        .. deprecated:: 0.23.0',\n",
       " 'Return the base object if the memory of the underlying data is shared.\\n\\n        .. deprecated:: 0.23.0',\n",
       " 'The data as an ndarray, possibly losing information.\\n\\n        The expectation is that this is cheap to compute, and is primarily\\n        used for interacting with our indexers.\\n\\n        - categorical -> codes',\n",
       " 'Return an ndarray of the maximum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax',\n",
       " 'Return a ndarray of the minimum argument indexer.\\n\\n        Parameters\\n        ----------\\n        axis : {None}\\n            Dummy argument for consistency with Series\\n        skipna : bool, default True\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin',\n",
       " 'Return a list of the values.\\n\\n        These are each a scalar type, which is a Python scalar\\n        (for str, int, float) or a pandas scalar\\n        (for Timestamp/Timedelta/Interval/Period)\\n\\n        Returns\\n        -------\\n        list\\n\\n        See Also\\n        --------\\n        numpy.ndarray.tolist',\n",
       " 'Return an iterator of the values.\\n\\n        These are each a scalar type, which is a Python scalar\\n        (for str, int, float) or a pandas scalar\\n        (for Timestamp/Timedelta/Interval/Period)',\n",
       " 'Memory usage of the values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False or if used on PyPy',\n",
       " \"Return the argument with an initial component of ~ or ~user\\n       replaced by that user's home directory.\\n\\n    Parameters\\n    ----------\\n    filepath_or_buffer : object to be converted if possible\\n\\n    Returns\\n    -------\\n    expanded_filepath_or_buffer : an expanded filepath or the\\n                                  input if not expandable\",\n",
       " 'Wrap comparison operations to convert timedelta-like to timedelta64',\n",
       " 'Convert an ndarray with integer-dtype to timedelta64[ns] dtype, treating\\n    the integers as multiples of the given timedelta unit.\\n\\n    Parameters\\n    ----------\\n    data : numpy.ndarray with integer-dtype\\n    unit : str, default \"ns\"\\n        The timedelta unit to treat integers as multiples of.\\n\\n    Returns\\n    -------\\n    numpy.ndarray : timedelta64[ns] array converted from data\\n    bool : whether a copy was made',\n",
       " 'Add DatetimeArray/Index or ndarray[datetime64] to TimedeltaArray.',\n",
       " 'Return a dataframe of the components (days, hours, minutes,\\n        seconds, milliseconds, microseconds, nanoseconds) of the Timedeltas.\\n\\n        Returns\\n        -------\\n        a DataFrame',\n",
       " 'Add engine to the excel writer registry.io.excel.\\n\\n    You must use this method to integrate with ``to_excel``.\\n\\n    Parameters\\n    ----------\\n    klass : ExcelWriter',\n",
       " \"Convert Excel column name like 'AB' to 0-based column index.\\n\\n    Parameters\\n    ----------\\n    x : str\\n        The Excel column name to convert to a 0-based column index.\\n\\n    Returns\\n    -------\\n    num : int\\n        The column index corresponding to the name.\\n\\n    Raises\\n    ------\\n    ValueError\\n        Part of the Excel column name was invalid.\",\n",
       " \"Convert comma separated list of column names and ranges to indices.\\n\\n    Parameters\\n    ----------\\n    areas : str\\n        A string containing a sequence of column ranges (or areas).\\n\\n    Returns\\n    -------\\n    cols : list\\n        A list of 0-based column indices.\\n\\n    Examples\\n    --------\\n    >>> _range2cols('A:E')\\n    [0, 1, 2, 3, 4]\\n    >>> _range2cols('A,C,Z:AB')\\n    [0, 2, 25, 26, 27]\",\n",
       " 'Convert `usecols` into a compatible format for parsing in `parsers.py`.\\n\\n    Parameters\\n    ----------\\n    usecols : object\\n        The use-columns object to potentially convert.\\n\\n    Returns\\n    -------\\n    converted : object\\n        The compatible format of `usecols`.',\n",
       " 'Forward fill blank entries in row but only inside the same parent index.\\n\\n    Used for creating headers in Multiindex.\\n    Parameters\\n    ----------\\n    row : list\\n        List of items in a single row.\\n    control_row : list of bool\\n        Helps to determine if particular column is in same parent index as the\\n        previous value. Used to stop propagation of empty cells between\\n        different indexes.\\n\\n    Returns\\n    ----------\\n    Returns changed row and control_row',\n",
       " 'Pop the header name for MultiIndex parsing.\\n\\n    Parameters\\n    ----------\\n    row : list\\n        The data row to parse for the header name.\\n    index_col : int, list\\n        The index columns for our data. Assumed to be non-null.\\n\\n    Returns\\n    -------\\n    header_name : str\\n        The extracted header name.\\n    trimmed_row : list\\n        The original data row with the header name removed.',\n",
       " \"Replace a number with its hexadecimal representation. Used to tag\\n    temporary variables with their calling scope's id.\",\n",
       " 'Return a prettier version of obj\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        Object to pretty print\\n\\n    Returns\\n    -------\\n    s : str\\n        Pretty print object repr',\n",
       " \"Resolve a variable name in a possibly local context\\n\\n        Parameters\\n        ----------\\n        key : str\\n            A variable name\\n        is_local : bool\\n            Flag indicating whether the variable is local or not (prefixed with\\n            the '@' symbol)\\n\\n        Returns\\n        -------\\n        value : object\\n            The value of a particular variable\",\n",
       " 'Replace a variable name, with a potentially new value.\\n\\n        Parameters\\n        ----------\\n        old_key : str\\n            Current variable name to replace\\n        new_key : str\\n            New variable name to replace `old_key` with\\n        new_value : object\\n            Value to be replaced along with the possible renaming',\n",
       " \"Get specifically scoped variables from a list of stack frames.\\n\\n        Parameters\\n        ----------\\n        stack : list\\n            A list of stack frames as returned by ``inspect.stack()``\\n        scopes : sequence of strings\\n            A sequence containing valid stack frame attribute names that\\n            evaluate to a dictionary. For example, ('locals', 'globals')\",\n",
       " 'Update the current scope by going back `level` levels.\\n\\n        Parameters\\n        ----------\\n        level : int or None, optional, default None',\n",
       " 'Add a temporary variable to the scope.\\n\\n        Parameters\\n        ----------\\n        value : object\\n            An arbitrary object to be assigned to a temporary variable.\\n\\n        Returns\\n        -------\\n        name : basestring\\n            The name of the temporary variable created.',\n",
       " 'Return the full scope for use with passing to engines transparently\\n        as a mapping.\\n\\n        Returns\\n        -------\\n        vars : DeepChainMap\\n            All variables in this scope.',\n",
       " 'Construct Series from array.\\n\\n        .. deprecated :: 0.23.0\\n            Use pd.Series(..) constructor instead.',\n",
       " 'Return object Series which contains boxed values.\\n\\n        .. deprecated :: 0.23.0\\n\\n           Use ``astype(object)`` instead.\\n\\n        *this is an internal non-public method*',\n",
       " 'Return selected slices of an array along given axis as a Series.\\n\\n        .. deprecated:: 0.24.0\\n\\n        See Also\\n        --------\\n        numpy.ndarray.compress',\n",
       " 'Return the i-th value or values in the Series by location.\\n\\n        Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n\\n        Returns\\n        -------\\n        scalar (int) or Series (slice, sequence)',\n",
       " 'Return a unicode string representation for a particular DataFrame.',\n",
       " 'Convert Series to DataFrame.\\n\\n        Parameters\\n        ----------\\n        name : object, default None\\n            The passed name should substitute for the series name (if it has\\n            one).\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame representation of Series.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([\"a\", \"b\", \"c\"],\\n        ...               name=\"vals\")\\n        >>> s.to_frame()\\n          vals\\n        0    a\\n        1    b\\n        2    c',\n",
       " \"Convert Series to SparseSeries.\\n\\n        Parameters\\n        ----------\\n        kind : {'block', 'integer'}, default 'block'\\n        fill_value : float, defaults to NaN (missing)\\n            Value to use for filling NaN values.\\n\\n        Returns\\n        -------\\n        SparseSeries\\n            Sparse representation of the Series.\",\n",
       " 'Set the Series name.\\n\\n        Parameters\\n        ----------\\n        name : str\\n        inplace : bool\\n            whether to modify `self` directly or return a copy',\n",
       " 'Swap levels i and j in a MultiIndex.\\n\\n        Parameters\\n        ----------\\n        i, j : int, str (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        Series\\n            Series with levels swapped in MultiIndex.\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.',\n",
       " 'Rearrange index levels using input order.\\n\\n        May not drop or duplicate levels.\\n\\n        Parameters\\n        ----------\\n        order : list of int representing new level order\\n               (reference level by number or key)\\n\\n        Returns\\n        -------\\n        type of caller (new object)',\n",
       " 'Perform a reduction operation.\\n\\n        If we have an ndarray as a value, then simply perform the operation,\\n        otherwise delegate to the object.',\n",
       " 'Conform Series to new index with optional filling logic.\\n\\n        .. deprecated:: 0.21.0\\n            Use ``Series.reindex`` instead.',\n",
       " 'Return Series without null values.\\n\\n        .. deprecated:: 0.23.0\\n            Use :meth:`Series.dropna` instead.',\n",
       " \"Cast to DatetimeIndex of Timestamps, at *beginning* of period.\\n\\n        Parameters\\n        ----------\\n        freq : str, default frequency of PeriodIndex\\n            Desired frequency.\\n        how : {'s', 'e', 'start', 'end'}\\n            Convention for converting period to timestamp; start of period\\n            vs. end.\\n        copy : bool, default True\\n            Whether or not to return a copy.\\n\\n        Returns\\n        -------\\n        Series with DatetimeIndex\",\n",
       " 'Convert Series from DatetimeIndex to PeriodIndex with desired\\n        frequency (inferred from index if not passed).\\n\\n        Parameters\\n        ----------\\n        freq : str, default None\\n            Frequency associated with the PeriodIndex.\\n        copy : bool, default True\\n            Whether or not to return a copy.\\n\\n        Returns\\n        -------\\n        Series\\n            Series with index converted to PeriodIndex.',\n",
       " 'Create a 0-dim ndarray containing the fill value\\n\\n    Parameters\\n    ----------\\n    arr : SparseArray\\n\\n    Returns\\n    -------\\n    fill_value : ndarray\\n        0-dim ndarray with just the fill value.\\n\\n    Notes\\n    -----\\n    coerce fill_value to arr dtype if possible\\n    int64 SparseArray can have NaN as fill_value if there is no missing',\n",
       " 'Perform a binary operation between two arrays.\\n\\n    Parameters\\n    ----------\\n    left : Union[SparseArray, ndarray]\\n    right : Union[SparseArray, ndarray]\\n    op : Callable\\n        The binary operation to perform\\n    name str\\n        Name of the callable.\\n\\n    Returns\\n    -------\\n    SparseArray',\n",
       " 'return an ndarray for our input,\\n    in a platform independent manner',\n",
       " \"Convert ndarray to sparse format\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    kind : {'block', 'integer'}\\n    fill_value : NaN or another value\\n    dtype : np.dtype, optional\\n    copy : bool, default False\\n\\n    Returns\\n    -------\\n    (sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)\",\n",
       " 'The percent of non- ``fill_value`` points, as decimal.\\n\\n        Examples\\n        --------\\n        >>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\\n        >>> s.density\\n        0.6',\n",
       " 'Get the location of the first missing value.\\n\\n        Returns\\n        -------\\n        int',\n",
       " \"Returns a Series containing counts of unique values.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don't include counts of NaN, even if NaN is in sp_values.\\n\\n        Returns\\n        -------\\n        counts : Series\",\n",
       " 'Tests whether all elements evaluate True\\n\\n        Returns\\n        -------\\n        all : bool\\n\\n        See Also\\n        --------\\n        numpy.all',\n",
       " 'Tests whether at least one of elements evaluate True\\n\\n        Returns\\n        -------\\n        any : bool\\n\\n        See Also\\n        --------\\n        numpy.any',\n",
       " 'Sum of non-NA/null values\\n\\n        Returns\\n        -------\\n        sum : float',\n",
       " 'Mean of non-NA/null values\\n\\n        Returns\\n        -------\\n        mean : float',\n",
       " 'Tokenize a Python source code string.\\n\\n    Parameters\\n    ----------\\n    source : str\\n        A Python source code string',\n",
       " 'Replace ``&`` with ``and`` and ``|`` with ``or`` so that bitwise\\n    precedence is changed to boolean precedence.\\n\\n    Parameters\\n    ----------\\n    tok : tuple of int, str\\n        ints correspond to the all caps constants in the tokenize module\\n\\n    Returns\\n    -------\\n    t : tuple of int, str\\n        Either the input or token or the replacement values',\n",
       " 'Filter out AST nodes that are subclasses of ``superclass``.',\n",
       " 'Return a function that raises a NotImplementedError with a passed node\\n    name.',\n",
       " 'Decorator to disallow certain nodes from parsing. Raises a\\n    NotImplementedError instead.\\n\\n    Returns\\n    -------\\n    disallowed : callable',\n",
       " 'Return a function to create an op class with its symbol already passed.\\n\\n    Returns\\n    -------\\n    f : callable',\n",
       " 'return a boolean whether I can attempt conversion to a TimedeltaIndex',\n",
       " 'Returns a FrozenList with other concatenated to the end of self.\\n\\n        Parameters\\n        ----------\\n        other : array-like\\n            The array-like whose elements we are concatenating.\\n\\n        Returns\\n        -------\\n        diff : FrozenList\\n            The collection difference between self and other.',\n",
       " 'Returns a FrozenList with elements from other removed from self.\\n\\n        Parameters\\n        ----------\\n        other : array-like\\n            The array-like whose elements we are removing self.\\n\\n        Returns\\n        -------\\n        diff : FrozenList\\n            The collection difference between self and other.',\n",
       " 'Return a unicode string representation for this object.',\n",
       " 'Find indices to insert `value` so as to maintain order.\\n\\n        For full documentation, see `numpy.searchsorted`\\n\\n        See Also\\n        --------\\n        numpy.searchsorted : Equivalent function.',\n",
       " 'Segregate Series based on type and coerce into matrices.\\n\\n    Needs to handle a lot of exceptional cases.',\n",
       " 'Extract from a masked rec array and create the manager.',\n",
       " 'Segregate Series based on type and coerce into matrices.\\n    Needs to handle a lot of exceptional cases.',\n",
       " 'Sanitize an index type to return an ndarray of the underlying, pass\\n    through a non-Index.',\n",
       " 'Sanitize input data to an ndarray, copy if specified, coerce to the\\n    dtype if specified.',\n",
       " \"Make sure a valid engine is passed.\\n\\n    Parameters\\n    ----------\\n    engine : str\\n\\n    Raises\\n    ------\\n    KeyError\\n      * If an invalid engine is passed\\n    ImportError\\n      * If numexpr was requested but doesn't exist\\n\\n    Returns\\n    -------\\n    string engine\",\n",
       " 'Make sure a valid parser is passed.\\n\\n    Parameters\\n    ----------\\n    parser : str\\n\\n    Raises\\n    ------\\n    KeyError\\n      * If an invalid parser is passed',\n",
       " \"Parameters\\n        ----------\\n        codes : optional list\\n            Codes to check for validity. Defaults to current codes.\\n        levels : optional list\\n            Levels to check for validity. Defaults to current levels.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If length of levels and codes don't match, if the codes for any\\n            level would exceed level bounds, or there are any duplicate levels.\",\n",
       " 'return a boolean if we need a qualified .info display',\n",
       " 'return the number of bytes in the underlying data\\n        deeply introspect the level data if deep=True\\n\\n        include the engine hashtable\\n\\n        *this is in internal routine*',\n",
       " 'Return a list of tuples of the (attr,formatted_value)',\n",
       " 'return if the index is monotonic increasing (only equal or\\n        increasing) values.',\n",
       " 'validate and return the hash for the provided key\\n\\n        *this is internal for use for the cython routines*\\n\\n        Parameters\\n        ----------\\n        key : string or tuple\\n\\n        Returns\\n        -------\\n        np.uint64\\n\\n        Notes\\n        -----\\n        we need to stringify if we have mixed levels',\n",
       " 'Return vector of label values for requested level,\\n        equal to the length of the index\\n\\n        **this is an internal method**\\n\\n        Parameters\\n        ----------\\n        level : int level\\n        unique : bool, default False\\n            if True, drop duplicated values\\n\\n        Returns\\n        -------\\n        values : ndarray',\n",
       " 'Append a collection of Index options together\\n\\n        Parameters\\n        ----------\\n        other : Index or list/tuple of indices\\n\\n        Returns\\n        -------\\n        appended : Index',\n",
       " 'Make new MultiIndex with passed list of codes deleted\\n\\n        Parameters\\n        ----------\\n        codes : array-like\\n            Must be a list of tuples\\n        level : int or level name, default None\\n\\n        Returns\\n        -------\\n        dropped : MultiIndex',\n",
       " 'Rearrange levels using input order. May not drop or duplicate levels\\n\\n        Parameters\\n        ----------',\n",
       " 'we categorizing our codes by using the\\n        available categories (all, not just observed)\\n        excluding any missing ones (-1); this is in preparation\\n        for sorting, where we need to disambiguate that -1 is not\\n        a valid valid',\n",
       " 'Parameters\\n        ----------\\n        keyarr : list-like\\n            Indexer to convert.\\n\\n        Returns\\n        -------\\n        tuple (indexer, keyarr)\\n            indexer is an ndarray or None if cannot convert\\n            keyarr are tuple-safe keys',\n",
       " \"Create index with target's values (move/add/delete values as necessary)\\n\\n        Returns\\n        -------\\n        new_index : pd.MultiIndex\\n            Resulting index\\n        indexer : np.ndarray or None\\n            Indices of output values in original index.\",\n",
       " 'Slice index between two labels / tuples, return new MultiIndex\\n\\n        Parameters\\n        ----------\\n        before : label or tuple, can be partial. Default None\\n            None defaults to start\\n        after : label or tuple, can be partial. Default None\\n            None defaults to end\\n\\n        Returns\\n        -------\\n        truncated : MultiIndex',\n",
       " 'Determines if two MultiIndex objects have the same labeling information\\n        (the levels themselves do not necessarily have to be the same)\\n\\n        See Also\\n        --------\\n        equal_levels',\n",
       " 'Return True if the levels of both MultiIndex objects are the same',\n",
       " 'Form the intersection of two MultiIndex objects.\\n\\n        Parameters\\n        ----------\\n        other : MultiIndex or array / Index of tuples\\n        sort : False or None, default False\\n            Sort the resulting MultiIndex if possible\\n\\n            .. versionadded:: 0.24.0\\n\\n            .. versionchanged:: 0.24.1\\n\\n               Changed the default from ``True`` to ``False``, to match\\n               behaviour from before 0.24.0\\n\\n        Returns\\n        -------\\n        Index',\n",
       " 'Compute set difference of two MultiIndex objects\\n\\n        Parameters\\n        ----------\\n        other : MultiIndex\\n        sort : False or None, default None\\n            Sort the resulting MultiIndex if possible\\n\\n            .. versionadded:: 0.24.0\\n\\n            .. versionchanged:: 0.24.1\\n\\n               Changed the default value from ``True`` to ``None``\\n               (without change in behaviour).\\n\\n        Returns\\n        -------\\n        diff : MultiIndex',\n",
       " 'Make new MultiIndex inserting new item at location\\n\\n        Parameters\\n        ----------\\n        loc : int\\n        item : tuple\\n            Must be same length as number of levels in the MultiIndex\\n\\n        Returns\\n        -------\\n        new_index : Index',\n",
       " 'Make new index with passed location deleted\\n\\n        Returns\\n        -------\\n        new_index : MultiIndex',\n",
       " 'routine to ensure that our data is of the correct\\n    input dtype for lower-level routines\\n\\n    This will coerce:\\n    - ints -> int64\\n    - uint -> uint64\\n    - bool -> uint64 (TODO this should be uint8)\\n    - datetimelike -> i8\\n    - datetime64tz -> i8 (in local tz)\\n    - categorical -> codes\\n\\n    Parameters\\n    ----------\\n    values : array-like\\n    dtype : pandas_dtype, optional\\n        coerce to this dtype\\n\\n    Returns\\n    -------\\n    (ndarray, pandas_dtype, algo dtype as a string)',\n",
       " 'reverse of _ensure_data\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    dtype : pandas_dtype\\n    original : ndarray-like\\n\\n    Returns\\n    -------\\n    Index for extension types, otherwise ndarray casted to dtype',\n",
       " 'Parameters\\n    ----------\\n    values : arraylike\\n\\n    Returns\\n    -------\\n    tuples(hashtable class,\\n           vector class,\\n           values,\\n           dtype,\\n           ndtype)',\n",
       " 'Compute locations of to_match into values\\n\\n    Parameters\\n    ----------\\n    to_match : array-like\\n        values to find positions of\\n    values : array-like\\n        Unique set of values\\n    na_sentinel : int, default -1\\n        Value to mark \"not found\"\\n\\n    Examples\\n    --------\\n\\n    Returns\\n    -------\\n    match : ndarray of integers',\n",
       " 'Compute the isin boolean array\\n\\n    Parameters\\n    ----------\\n    comps : array-like\\n    values : array-like\\n\\n    Returns\\n    -------\\n    boolean array same length as comps',\n",
       " 'Parameters\\n    ----------\\n    values : arraylike\\n    dropna : boolean\\n\\n    Returns\\n    -------\\n    (uniques, counts)',\n",
       " \"Returns the mode(s) of an array.\\n\\n    Parameters\\n    ----------\\n    values : array-like\\n        Array over which to check for duplicate values.\\n    dropna : boolean, default True\\n        Don't consider counts of NaN/NaT.\\n\\n        .. versionadded:: 0.24.0\\n\\n    Returns\\n    -------\\n    mode : Series\",\n",
       " 'Specialized Cython take which sets NaN values in one pass',\n",
       " 'difference of n between self,\\n    analogous to s-s.shift(n)\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    n : int\\n        number of periods\\n    axis : int\\n        axis to shift on\\n\\n    Returns\\n    -------\\n    shifted',\n",
       " 'For arbitrary (MultiIndexed) SparseSeries return\\n    (v, i, j, ilabels, jlabels) where (v, (i, j)) is suitable for\\n    passing to scipy.sparse.coo constructor.',\n",
       " 'Convert a SparseSeries to a scipy.sparse.coo_matrix using index\\n    levels row_levels, column_levels as the row and column\\n    labels respectively. Returns the sparse_matrix, row and column labels.',\n",
       " 'Convert a scipy.sparse.coo_matrix to a SparseSeries.\\n    Use the defaults given in the SparseSeries constructor.',\n",
       " 'Wrap comparison operations to convert datetime-like to datetime64',\n",
       " 'Convert data based on dtype conventions, issuing deprecation warnings\\n    or errors where appropriate.\\n\\n    Parameters\\n    ----------\\n    data : np.ndarray or pd.Index\\n    copy : bool\\n\\n    Returns\\n    -------\\n    data : np.ndarray or pd.Index\\n    copy : bool\\n\\n    Raises\\n    ------\\n    TypeError : PeriodDType data is passed',\n",
       " 'If a timezone is inferred from data, check that it is compatible with\\n    the user-provided timezone, if any.\\n\\n    Parameters\\n    ----------\\n    tz : tzinfo or None\\n    inferred_tz : tzinfo or None\\n\\n    Returns\\n    -------\\n    tz : tzinfo or None\\n\\n    Raises\\n    ------\\n    TypeError : if both timezones are present but do not match',\n",
       " 'Check that a dtype, if passed, represents either a numpy datetime64[ns]\\n    dtype or a pandas DatetimeTZDtype.\\n\\n    Parameters\\n    ----------\\n    dtype : object\\n\\n    Returns\\n    -------\\n    dtype : None, numpy.dtype, or DatetimeTZDtype\\n\\n    Raises\\n    ------\\n    ValueError : invalid dtype\\n\\n    Notes\\n    -----\\n    Unlike validate_tz_from_dtype, this does _not_ allow non-existent\\n    tz errors to go through',\n",
       " 'If the given dtype is a DatetimeTZDtype, extract the implied\\n    tzinfo object from it and check that it does not conflict with the given\\n    tz.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, str\\n    tz : None, tzinfo\\n\\n    Returns\\n    -------\\n    tz : consensus tzinfo\\n\\n    Raises\\n    ------\\n    ValueError : on tzinfo mismatch',\n",
       " 'If a timezone is not explicitly given via `tz`, see if one can\\n    be inferred from the `start` and `end` endpoints.  If more than one\\n    of these inputs provides a timezone, require that they all agree.\\n\\n    Parameters\\n    ----------\\n    start : Timestamp\\n    end : Timestamp\\n    tz : tzinfo or None\\n\\n    Returns\\n    -------\\n    tz : tzinfo or None\\n\\n    Raises\\n    ------\\n    TypeError : if start and end timezones do not agree',\n",
       " 'Localize a start or end Timestamp to the timezone of the corresponding\\n    start or end Timestamp\\n\\n    Parameters\\n    ----------\\n    ts : start or end Timestamp to potentially localize\\n    is_none : argument that should be None\\n    is_not_none : argument that should not be None\\n    freq : Tick, DateOffset, or None\\n    tz : str, timezone object or None\\n\\n    Returns\\n    -------\\n    ts : Timestamp',\n",
       " 'Return an iterator over the boxed values\\n\\n        Yields\\n        -------\\n        tstamp : Timestamp',\n",
       " 'subtract DatetimeArray/Index or ndarray[datetime64]',\n",
       " 'Add a timedelta-like, Tick, or TimedeltaIndex-like object\\n        to self, yielding a new DatetimeArray\\n\\n        Parameters\\n        ----------\\n        other : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : DatetimeArray',\n",
       " 'Calculate TimedeltaArray of difference between index\\n        values and index converted to PeriodArray at specified\\n        freq. Used for vectorized offsets\\n\\n        Parameters\\n        ----------\\n        freq : Period frequency\\n\\n        Returns\\n        -------\\n        TimedeltaArray/Index',\n",
       " 'Returns numpy array of datetime.time. The time part of the Timestamps.',\n",
       " 'Convert Datetime Array to float64 ndarray of Julian Dates.\\n        0 Julian date is noon January 1, 4713 BC.\\n        http://en.wikipedia.org/wiki/Julian_day',\n",
       " 'Validate the docstring for the given func_name\\n\\n    Parameters\\n    ----------\\n    func_name : function\\n        Function whose docstring will be evaluated (e.g. pandas.read_csv).\\n\\n    Returns\\n    -------\\n    dict\\n        A dictionary containing all the information obtained from validating\\n        the docstring.',\n",
       " \"Import Python object from its name as string.\\n\\n        Parameters\\n        ----------\\n        name : str\\n            Object name to import (e.g. pandas.Series.str.upper)\\n\\n        Returns\\n        -------\\n        object\\n            Python object that can be a class, method, function...\\n\\n        Examples\\n        --------\\n        >>> Docstring._load_obj('pandas.Series')\\n        <class 'pandas.core.series.Series'>\",\n",
       " 'Find the Python object that contains the source code of the object.\\n\\n        This is useful to find the place in the source code (file and line\\n        number) where a docstring is defined. It does not currently work for\\n        all cases, but it should help find some (properties...).',\n",
       " 'File name where the object is implemented (e.g. pandas/core/frame.py).',\n",
       " 'Check if the docstrings method can return something.\\n\\n        Bare returns, returns valued None and returns from nested functions are\\n        disconsidered.\\n\\n        Returns\\n        -------\\n        bool\\n            Whether the docstrings method can return something.',\n",
       " 'Convert numpy types to Python types for the Excel writers.\\n\\n        Parameters\\n        ----------\\n        val : object\\n            Value to be written into cells\\n\\n        Returns\\n        -------\\n        Tuple with the first element being the converted value and the second\\n            being an optional format',\n",
       " \"checks that path's extension against the Writer's supported\\n        extensions.  If it isn't supported, raises UnsupportedFiletypeError.\",\n",
       " 'Parse specified sheet(s) into a DataFrame\\n\\n        Equivalent to read_excel(ExcelFile, ...)  See the read_excel\\n        docstring for more info on accepted parameters',\n",
       " 'Validate that the where statement is of the right type.\\n\\n    The type may either be String, Expr, or list-like of Exprs.\\n\\n    Parameters\\n    ----------\\n    w : String term expression, Expr, or list-like of Exprs.\\n\\n    Returns\\n    -------\\n    where : The original where clause if the check was successful.\\n\\n    Raises\\n    ------\\n    TypeError : An invalid data type was passed in for w (e.g. dict).',\n",
       " 'loose checking if s is a pytables-acceptable expression',\n",
       " 'convert the expression that is in the term to something that is\\n        accepted by pytables',\n",
       " 'quote the string if not encoded\\n            else encode and return',\n",
       " 'wrapper around numpy.result_type which overcomes the NPY_MAXARGS (32)\\n    argument limit',\n",
       " \"If 'Series.argmin' is called via the 'numpy' library,\\n    the third parameter in its signature is 'out', which\\n    takes either an ndarray or 'None', so check if the\\n    'skipna' parameter is either an instance of ndarray or\\n    is None, since 'skipna' itself should be a boolean\",\n",
       " \"If 'Series.argmax' is called via the 'numpy' library,\\n    the third parameter in its signature is 'out', which\\n    takes either an ndarray or 'None', so check if the\\n    'skipna' parameter is either an instance of ndarray or\\n    is None, since 'skipna' itself should be a boolean\",\n",
       " \"If 'Categorical.argsort' is called via the 'numpy' library, the\\n    first parameter in its signature is 'axis', which takes either\\n    an integer or 'None', so check if the 'ascending' parameter has\\n    either integer type or is None, since 'ascending' itself should\\n    be a boolean\",\n",
       " \"If 'NDFrame.clip' is called via the numpy library, the third\\n    parameter in its signature is 'out', which can takes an ndarray,\\n    so check if the 'axis' parameter is an instance of ndarray, since\\n    'axis' itself should either be an integer or None\",\n",
       " \"If this function is called via the 'numpy' library, the third\\n    parameter in its signature is 'dtype', which takes either a\\n    'numpy' dtype or 'None', so check if the 'skipna' parameter is\\n    a boolean or not\",\n",
       " \"If this function is called via the 'numpy' library, the third\\n    parameter in its signature is 'axis', which takes either an\\n    ndarray or 'None', so check if the 'convert' parameter is either\\n    an instance of ndarray or is None\",\n",
       " \"'args' and 'kwargs' should be empty, except for allowed\\n    kwargs because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\",\n",
       " \"'args' and 'kwargs' should be empty because all of\\n    their necessary parameters are explicitly listed in\\n    the function signature\",\n",
       " 'Ensure that the axis argument passed to min, max, argmin, or argmax is\\n    zero or None, as otherwise it will be incorrectly ignored.\\n\\n    Parameters\\n    ----------\\n    axis : int or None\\n\\n    Raises\\n    ------\\n    ValueError',\n",
       " 'Load msgpack pandas object from the specified\\n    file path\\n\\n    THIS IS AN EXPERIMENTAL LIBRARY and the storage format\\n    may not be stable until a future release.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : string File path, BytesIO like or string\\n    encoding : Encoding for decoding msgpack str type\\n    iterator : boolean, if True, return an iterator to the unpacker\\n               (default is False)\\n\\n    Returns\\n    -------\\n    obj : same type as object stored in file',\n",
       " 'Convert strings to complex number instance with specified numpy type.',\n",
       " 'Unpack a packed object, return an iterator\\n    Note: packed lists will be returned as tuples',\n",
       " 'At this point, the data either has a `read` attribute (e.g. a file\\n        object or a StringIO) or is a string that is a JSON document.\\n\\n        If self.chunksize, we prepare the data for the `__next__` method.\\n        Otherwise, we read it into memory for the `read` method.',\n",
       " 'The function read_json accepts three input types:\\n            1. filepath (string-like)\\n            2. file-like object (e.g. open file object, StringIO)\\n            3. JSON string\\n\\n        This method turns (1) into (2) to simplify the rest of the processing.\\n        It returns input types (2) and (3) unchanged.',\n",
       " 'Combines a list of JSON objects into one JSON object.',\n",
       " \"Checks that dict has only the appropriate keys for orient='split'.\",\n",
       " 'Take a conversion function and possibly recreate the frame.',\n",
       " 'Return a formatter function for a range of timedeltas.\\n    These will all have the same format argument\\n\\n    If box, then show the return in quotes',\n",
       " 'Separates the real and imaginary parts from the complex number, and\\n    executes the _trim_zeros_float method on each of those.',\n",
       " 'Trims zeros, leaving just one before the decimal points if need be.',\n",
       " 'Alter default behavior on how float is formatted in DataFrame.\\n    Format float in engineering format. By accuracy, we mean the number of\\n    decimal digits after the floating point.\\n\\n    See also EngFormatter.',\n",
       " 'For each index in each level the function returns lengths of indexes.\\n\\n    Parameters\\n    ----------\\n    levels : list of lists\\n        List of values on for level.\\n    sentinel : string, optional\\n        Value which states that no new index starts on there.\\n\\n    Returns\\n    ----------\\n    Returns list of maps. For each level returns map of indexes (key is index\\n    in row and value is length of index).',\n",
       " 'Appends lines to a buffer.\\n\\n    Parameters\\n    ----------\\n    buf\\n        The buffer to write to\\n    lines\\n        The lines to append.',\n",
       " 'Calculate display width considering unicode East Asian Width',\n",
       " 'Render a DataFrame to a list of columns (as lists of strings).',\n",
       " 'Render a DataFrame to a console-friendly tabular output.',\n",
       " 'Render a DataFrame to a LaTeX tabular/longtable environment output.',\n",
       " 'Returns a function to be applied on each value to format it',\n",
       " 'Returns the float values converted into strings using\\n        the parameters given at initialisation, as a numpy array',\n",
       " 'Given an Interval or IntervalIndex, return the corresponding interval with\\n    closed bounds.',\n",
       " 'helper for interval_range to check if start/end are valid types',\n",
       " 'helper for interval_range to check type compat of start/end/freq',\n",
       " \"Provide method name lookup and completion\\n        Only provide 'public' methods.\",\n",
       " \"Add accessors to cls from the delegate class.\\n\\n        Parameters\\n        ----------\\n        cls : the class to add the methods/properties to\\n        delegate : the class to get methods/properties & doc-strings\\n        accessors : string list of accessors to add\\n        typ : 'property' or 'method'\\n        overwrite : boolean, default False\\n           overwrite the method/property in the target class if it exists.\",\n",
       " 'evaluate and return the expression of the op on a and b\\n\\n        Parameters\\n        ----------\\n\\n        op :    the actual operand\\n        op_str: the string version of the op\\n        a :     left operand\\n        b :     right operand\\n        use_numexpr : whether to try to use numexpr (default True)',\n",
       " 'evaluate the where condition cond on a and b\\n\\n        Parameters\\n        ----------\\n\\n        cond : a boolean array\\n        a :    return if cond is True\\n        b :    return if cond is False\\n        use_numexpr : whether to try to use numexpr (default True)',\n",
       " 'Convert CSS declarations to ExcelWriter style\\n\\n        Parameters\\n        ----------\\n        declarations_str : str\\n            List of CSS declarations.\\n            e.g. \"font-weight: bold; background: blue\"\\n\\n        Returns\\n        -------\\n        xlstyle : dict\\n            A style as interpreted by ExcelWriter when found in\\n            ExcelCell.style.',\n",
       " 'Write a DataFrame to the feather-format\\n\\n    Parameters\\n    ----------\\n    df : DataFrame\\n    path : string file path, or file-like object',\n",
       " 'A special case for _generate_range_overflow_safe where `periods * stride`\\n    can be calculated without overflowing int64 bounds.',\n",
       " 'Check to see if we can set a locale, and subsequently get the locale,\\n    without raising an Exception.\\n\\n    Parameters\\n    ----------\\n    lc : str\\n        The locale to attempt to set.\\n    lc_var : int, default `locale.LC_ALL`\\n        The category of the locale being set.\\n\\n    Returns\\n    -------\\n    is_valid : bool\\n        Whether the passed locale can be set',\n",
       " 'Return a list of normalized locales that do not throw an ``Exception``\\n    when set.\\n\\n    Parameters\\n    ----------\\n    locales : str\\n        A string where each locale is separated by a newline.\\n    normalize : bool\\n        Whether to call ``locale.normalize`` on each locale.\\n\\n    Returns\\n    -------\\n    valid_locales : list\\n        A list of valid locales.',\n",
       " 'Ensure that an array object has a float dtype if possible.\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array whose data type we want to enforce as float.\\n\\n    Returns\\n    -------\\n    float_arr : The original array cast to the float dtype if\\n                possible. Otherwise, the original array is returned.',\n",
       " 'Ensure that an array-like object is a Categorical (if not already).\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array that we want to convert into a Categorical.\\n\\n    Returns\\n    -------\\n    cat_arr : The original array cast as a Categorical. If it already\\n              is a Categorical, we return as is.',\n",
       " 'evaluate if the tipo is a subclass of the klasses\\n    and not a datetimelike',\n",
       " 'Check whether an array-like is a periodical index.\\n\\n    .. deprecated:: 0.24.0\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array-like to check.\\n\\n    Returns\\n    -------\\n    boolean\\n        Whether or not the array-like is a periodical index.\\n\\n    Examples\\n    --------\\n    >>> is_period([1, 2, 3])\\n    False\\n    >>> is_period(pd.Index([1, 2, 3]))\\n    False\\n    >>> is_period(pd.PeriodIndex([\"2017-01-01\"], freq=\"D\"))\\n    True',\n",
       " 'Check whether an array-like is a datetime array-like or DatetimeIndex.\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array-like to check.\\n\\n    Returns\\n    -------\\n    boolean\\n        Whether or not the array-like is a datetime array-like or\\n        DatetimeIndex.\\n\\n    Examples\\n    --------\\n    >>> is_datetime_arraylike([1, 2, 3])\\n    False\\n    >>> is_datetime_arraylike(pd.Index([1, 2, 3]))\\n    False\\n    >>> is_datetime_arraylike(pd.DatetimeIndex([1, 2, 3]))\\n    True',\n",
       " 'Return a boolean if the condition is satisfied for the arr_or_dtype.\\n\\n    Parameters\\n    ----------\\n    arr_or_dtype : array-like, str, np.dtype, or ExtensionArrayType\\n        The array-like or dtype object whose dtype we want to extract.\\n    condition : callable[Union[np.dtype, ExtensionDtype]]\\n\\n    Returns\\n    -------\\n    bool',\n",
       " 'Get the dtype instance associated with an array\\n    or dtype object.\\n\\n    Parameters\\n    ----------\\n    arr_or_dtype : array-like\\n        The array-like or dtype object whose dtype we want to extract.\\n\\n    Returns\\n    -------\\n    obj_dtype : The extract dtype instance from the\\n                passed in array or dtype object.\\n\\n    Raises\\n    ------\\n    TypeError : The passed in object is None.',\n",
       " 'Return a boolean if the condition is satisfied for the arr_or_dtype.\\n\\n    Parameters\\n    ----------\\n    arr_or_dtype : array-like\\n        The array-like or dtype object whose dtype we want to extract.\\n    condition : callable[Union[np.dtype, ExtensionDtypeType]]\\n\\n    Returns\\n    -------\\n    bool : if the condition is satisifed for the arr_or_dtype',\n",
       " 'Get a numpy dtype.type-style object for a dtype object.\\n\\n    This methods also includes handling of the datetime64[ns] and\\n    datetime64[ns, TZ] objects.\\n\\n    If no dtype can be found, we return ``object``.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, type\\n        The dtype object whose numpy dtype.type-style\\n        object we want to extract.\\n\\n    Returns\\n    -------\\n    dtype_object : The extracted numpy dtype.type-style object.',\n",
       " 'Check whether the dtype is a date-like dtype. Raises an error if invalid.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype, type\\n        The dtype to check.\\n\\n    Raises\\n    ------\\n    TypeError : The dtype could not be casted to a date-like dtype.\\n    ValueError : The dtype is an illegal date-like dtype (e.g. the\\n                 the frequency provided is too specific)',\n",
       " 'Convert input into a pandas only dtype object or a numpy dtype object.\\n\\n    Parameters\\n    ----------\\n    dtype : object to be converted\\n\\n    Returns\\n    -------\\n    np.dtype or a pandas dtype\\n\\n    Raises\\n    ------\\n    TypeError if not a dtype',\n",
       " 'groupby & merge; we are always performing a left-by type operation\\n\\n    Parameters\\n    ----------\\n    by: field to group\\n    on: duplicates field\\n    left: left frame\\n    right: right frame\\n    _merge_pieces: function for merging\\n    check_duplicates: boolean, default True\\n        should we check & clean duplicates',\n",
       " \"Parameters\\n    ----------\\n    left_keys: ndarray, Index, Series\\n    right_keys: ndarray, Index, Series\\n    sort: boolean, default False\\n    how: string {'inner', 'outer', 'left', 'right'}, default 'inner'\\n\\n    Returns\\n    -------\\n    tuple of (left_indexer, right_indexer)\\n        indexers into the left_keys, right_keys\",\n",
       " 'Create a join index by rearranging one index to match another\\n\\n        Parameters\\n        ----------\\n        index: Index being rearranged\\n        other_index: Index used to supply values not found in index\\n        indexer: how to rearrange index\\n        how: replacement is only necessary if indexer based on other_index\\n\\n        Returns\\n        -------\\n        join_index',\n",
       " 'Note: has side effects (copy/delete key columns)\\n\\n        Parameters\\n        ----------\\n        left\\n        right\\n        on\\n\\n        Returns\\n        -------\\n        left_keys, right_keys',\n",
       " \"Check whether 'other' is equal to self.\\n\\n        By default, 'other' is considered equal if either\\n\\n        * it's a string matching 'self.name'.\\n        * it's an instance of this type and all of the\\n          the attributes in ``self._metadata`` are equal between\\n          `self` and `other`.\\n\\n        Parameters\\n        ----------\\n        other : Any\\n\\n        Returns\\n        -------\\n        bool\",\n",
       " 'Auxiliary function for :meth:`str.cat`\\n\\n    Parameters\\n    ----------\\n    list_of_columns : list of numpy arrays\\n        List of arrays to be concatenated with sep;\\n        these arrays may not contain NaNs!\\n    sep : string\\n        The separator string for concatenating the columns\\n\\n    Returns\\n    -------\\n    nd.array\\n        The concatenation of list_of_columns with sep',\n",
       " 'Find groups in each string in the Series using passed regular\\n    expression. This function is called from\\n    str_extract(expand=False), and can return Series, DataFrame, or\\n    Index.',\n",
       " 'For each subject string in the Series, extract groups from the\\n    first match of regular expression pat. This function is called from\\n    str_extract(expand=True), and always returns a DataFrame.',\n",
       " \"Strip whitespace (including newlines) from each string in the\\n    Series/Index.\\n\\n    Parameters\\n    ----------\\n    to_strip : str or unicode\\n    side : {'left', 'right', 'both'}, default 'both'\\n\\n    Returns\\n    -------\\n    Series or Index\",\n",
       " 'Decode character string in the Series/Index using indicated encoding.\\n    Equivalent to :meth:`str.decode` in python2 and :meth:`bytes.decode` in\\n    python3.\\n\\n    Parameters\\n    ----------\\n    encoding : str\\n    errors : str, optional\\n\\n    Returns\\n    -------\\n    Series or Index',\n",
       " 'Encode character string in the Series/Index using indicated encoding.\\n    Equivalent to :meth:`str.encode`.\\n\\n    Parameters\\n    ----------\\n    encoding : str\\n    errors : str, optional\\n\\n    Returns\\n    -------\\n    encoded : Series/Index of objects',\n",
       " 'Copy a docstring from another source function (if present)',\n",
       " \"Return the Unicode normal form for the strings in the Series/Index.\\n        For more information on the forms, see the\\n        :func:`unicodedata.normalize`.\\n\\n        Parameters\\n        ----------\\n        form : {'NFC', 'NFKC', 'NFD', 'NFKD'}\\n            Unicode form\\n\\n        Returns\\n        -------\\n        normalized : Series/Index of objects\",\n",
       " 'Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on',\n",
       " 'Raise exception with existing traceback.\\n    If traceback is not passed, uses sys.exc_info() to get traceback.',\n",
       " 'converts a style_dict to an openpyxl style object\\n        Parameters\\n        ----------\\n        style_dict : style dictionary to convert',\n",
       " \"Convert ``color_spec`` to an openpyxl v2 Color object\\n        Parameters\\n        ----------\\n        color_spec : str, dict\\n            A 32-bit ARGB hex string, or a dict with zero or more of the\\n            following keys.\\n                'rgb'\\n                'indexed'\\n                'auto'\\n                'theme'\\n                'tint'\\n                'index'\\n                'type'\\n        Returns\\n        -------\\n        color : openpyxl.styles.Color\",\n",
       " \"Convert ``side_spec`` to an openpyxl v2 Side object\\n        Parameters\\n        ----------\\n        side_spec : str, dict\\n            A string specifying the border style, or a dict with zero or more\\n            of the following keys (or their synonyms).\\n                'style' ('border_style')\\n                'color'\\n        Returns\\n        -------\\n        side : openpyxl.styles.Side\",\n",
       " 'construct and return a row or column based frame apply object',\n",
       " 'we have an empty result; at least 1 axis is 0\\n\\n        we will try to apply the function to an empty\\n        series in order to see if this is a reduction function',\n",
       " 'infer the results to the same shape as the input object',\n",
       " \"Numpy version of itertools.product.\\n    Sometimes faster (for large inputs)...\\n\\n    Parameters\\n    ----------\\n    X : list-like of list-likes\\n\\n    Returns\\n    -------\\n    product : list of ndarrays\\n\\n    Examples\\n    --------\\n    >>> cartesian_product([list('ABC'), [1, 2]])\\n    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='|S1'),\\n    array([1, 2, 1, 2, 1, 2])]\\n\\n    See Also\\n    --------\\n    itertools.product : Cartesian product of input iterables.  Equivalent to\\n        nested for-loops.\",\n",
       " 'str->list\\n    Convert XML to URL List.\\n    From Biligrab.',\n",
       " 'From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110',\n",
       " 'Downloads a Sina video by its unique vid.\\n    http://video.sina.com.cn/',\n",
       " 'Downloads a Sina video by its unique vkey.\\n    http://video.sina.com/',\n",
       " 'self, str->None\\n        \\n        Keyword arguments:\\n        self: self\\n        vid: The video ID for BokeCC cloud, something like\\n        FE3BB999594978049C33DC5901307461\\n        \\n        Calls the prepare() to download the video.\\n        \\n        If no title is provided, this method shall try to find a proper title\\n        with the information providin within the\\n        returned content of the API.',\n",
       " 'Format text with color or other effects into ANSI escaped string.',\n",
       " 'str->dict\\n    Information for CKPlayer API content.',\n",
       " 'Splicing URLs according to video ID to get video details',\n",
       " 'Override the original one\\n        Ugly ugly dirty hack',\n",
       " 'str, str, str, bool, bool ->None\\n\\n    Download Acfun video by vid.\\n\\n    Call Acfun API, decide which site to use, and pass the job to its\\n    extractor.',\n",
       " 'str, str->True\\n    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!\\n    You can basicly download anything with this function\\n    but better leave it alone with',\n",
       " 'Scans through a string for substrings matched some patterns (first-subgroups only).\\n\\n    Args:\\n        text: A string to be scanned.\\n        patterns: Arbitrary number of regex patterns.\\n\\n    Returns:\\n        When only one pattern is given, returns a string (None if no match found).\\n        When more than one pattern are given, returns a list of strings ([] if no match found).',\n",
       " 'Scans through a string for substrings matched some patterns.\\n\\n    Args:\\n        text: A string to be scanned.\\n        patterns: a list of regex pattern.\\n\\n    Returns:\\n        a list if matched. empty if not.',\n",
       " 'Parses the query string of a URL and returns the value of a parameter.\\n\\n    Args:\\n        url: A URL.\\n        param: A string representing the name of the parameter.\\n\\n    Returns:\\n        The value of the parameter.',\n",
       " 'Decompresses data for Content-Encoding: deflate.\\n    (the zlib compression is used.)',\n",
       " 'Gets the content of a URL via sending a HTTP GET request.\\n\\n    Args:\\n        url: A URL.\\n        headers: Request headers used by the client.\\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\\n\\n    Returns:\\n        The content as a string.',\n",
       " 'Post the content of a URL via sending a HTTP POST request.\\n\\n    Args:\\n        url: A URL.\\n        headers: Request headers used by the client.\\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\\n\\n    Returns:\\n        The content as a string.',\n",
       " \"Overload default print function as py (<3.3) does not support 'flush' keyword.\\n    Although the function name can be same as print to get itself overloaded automatically,\\n    I'd rather leave it with a different name and only overload it when importing to make less confusion.\",\n",
       " 'JSON, int, int, int->str\\n    \\n    Get a proper title with courseid+topicID+partID.',\n",
       " 'int->None\\n    \\n    Download a WHOLE course.\\n    Reuse the API call to save time.',\n",
       " 'int, int, int->None\\n    \\n    Download ONE PART of the course.',\n",
       " 'int, int->list\\n        \\n        Get the height of the videos.\\n        \\n        Since brightcove is using 3 kinds of links: rtmp, http and https,\\n        we will be using the HTTPS one to make it secure.\\n        \\n        If somehow akamaihd.net is blocked by the Great Fucking Wall,\\n        change the \"startswith https\" to http.',\n",
       " 'Preview version of Xception network. Not tested yet - use at own risk. No pretrained model yet.',\n",
       " 'Method returns a RNN_Learner object, that wraps an instance of the RNN_Encoder module.\\n\\n        Args:\\n            opt_fn (Optimizer): the torch optimizer function to use\\n            emb_sz (int): embedding size\\n            n_hid (int): number of hidden inputs\\n            n_layers (int): number of hidden layers\\n            kwargs: other arguments\\n\\n        Returns:\\n            An instance of the RNN_Learner class.',\n",
       " 'Return list of files in `path` that have a suffix in `extensions`; optionally `recurse`.',\n",
       " 'Load an empty `DataBunch` from the exported file in `path/fname` with optional `tfms`.',\n",
       " 'Reconstruct one of the underlying item for its data `t`.',\n",
       " 'Create a new `ItemList` from `items`, keeping the same attributes.',\n",
       " 'Create an `ItemList` in `path` from the filenames that have a suffix in `extensions`.\\n        `recurse` determines if we search subfolders.',\n",
       " 'Create an `ItemList` in `path` from the inputs in the `cols` of `df`.',\n",
       " 'Create an `ItemList` in `path` from the inputs in the `cols` of `path/csv_name`',\n",
       " 'Use only a sample of `sample_pct`of the full dataset and an optional `seed`.',\n",
       " 'Only keep elements for which `func` returns `True`.',\n",
       " 'Only keep filenames in `include` folder or reject the ones in `exclude`.',\n",
       " 'Keep random sample of `items` with probability `p` and an optional `seed`.',\n",
       " \"Don't split the data and create an empty validation set.\",\n",
       " 'Split the data between `train_idx` and `valid_idx`.',\n",
       " 'Split the data according to the indexes in `valid_idx`.',\n",
       " 'Split the data depending on the folder (`train` or `valid`) in which the filenames are.',\n",
       " 'Split the items randomly by putting `valid_pct` in the validation set, optional `seed` can be passed.',\n",
       " 'Split the items into train set with size `train_size * n` and valid set with size `valid_size * n`.',\n",
       " 'Split the data by result of `func` (which returns `True` for validation set).',\n",
       " 'Split the data by using the names in `valid_names` for validation.',\n",
       " 'Split the data by using the names in `fname` for the validation set. `path` will override `self.path`.',\n",
       " 'Split the data from the `col` in the dataframe in `self.inner_df`.',\n",
       " 'Return `label_cls` or guess one from the first element of `labels`.',\n",
       " 'Label `self.items` from the values in `cols` in `self.inner_df`.',\n",
       " 'Give a label to each filename depending on its folder.',\n",
       " 'Apply the re in `pat` to determine the label of every filename.  If `full_path`, search in the full name.',\n",
       " 'Generate classes from `items` by taking the sorted unique values.',\n",
       " 'Use the labels in `train_labels` and `valid_labels` to label the data. `label_cls` will overwrite the default.',\n",
       " 'Set `tfms` to be applied to the xs of the train and validation set.',\n",
       " 'Set `tfms` to be applied to the ys of the train and validation set.',\n",
       " 'Read the default class processors if none have been set.',\n",
       " 'Create an `DataBunch` from self, `path` will override `self.path`, `kwargs` are passed to `DataBunch.create`.',\n",
       " 'Add test set containing `items` with an arbitrary `label`.',\n",
       " 'Add test set containing items from `test_folder` and an arbitrary `label`.',\n",
       " 'Create a `LabelLists` with empty sets from the serialized `state`.',\n",
       " 'Create a `LabelLists` with empty sets from the serialized file in `path/fn`.',\n",
       " 'For inference, will briefly replace the dataset with one that only contains `item`.',\n",
       " 'Create `pd.DataFrame` containing `items` from `self.x` and `self.y`.',\n",
       " 'Save `self.to_df()` to a CSV file in `self.path`/`dest`.',\n",
       " 'Export the minimal state and save it in `fn` to load an empty version for inference.',\n",
       " 'Load the state in `fn` to create an empty `LabelList` for inference.',\n",
       " 'Launch the processing on `self.x` and `self.y` with `xp` and `yp`.',\n",
       " 'Set the `tfms` and `tfm_y` value to be applied to the inputs and targets.',\n",
       " 'Create a new `ItemList` from `items`, keeping the same attributes.',\n",
       " 'Parse the docstring into its components.\\n\\n    :return: a dictionary of form\\n              {\\n                  \"short_description\": ...,\\n                  \"long_description\": ...,\\n                  \"params\": [{\"name\": ..., \"doc\": ...}, ...],\\n                  \"vals\": [{\"name\": ..., \"doc\": ...}, ...],\\n                  \"return\": ...\\n              }',\n",
       " \"Return env var value if it's defined and not an empty string, or return Unknown\",\n",
       " 'Suggest how to improve the setup to speed things up',\n",
       " 'Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.',\n",
       " 'Exponentially anneal from `start` to `end` as pct goes from 0.0 to 1.0.',\n",
       " 'Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.',\n",
       " 'Create an `optim.Optimizer` from `opt_func` with `lr`. Set lr on `layer_groups`.',\n",
       " 'Create a new `OptimWrapper` from `self` with another `layer_groups` but the same hyper-parameters.',\n",
       " 'Create a new `OptimWrapper` from `self` with another `layer_groups` but the same hyper-parameters.',\n",
       " 'Set beta (or alpha as makes sense for given optimizer).',\n",
       " 'Read the values inside the optimizer for the hyper-parameters.',\n",
       " 'Set `val` inside the optimizer dictionary at `key`.',\n",
       " 'Read a hyperparameter `key` in the optimizer dictionary.',\n",
       " 'Return the inner state of the `Callback`, `minimal` or not.',\n",
       " 'Update metric computation with `last_output` and `last_target`.',\n",
       " 'Initialize our optimization params based on our annealing schedule.',\n",
       " 'Take one step forward on the annealing schedule for the optim params.',\n",
       " 'Distributed training of Imagenette.\\n    Fastest multi-gpu speed is if you run with: python -m fastai.launch',\n",
       " 'Download a dataset.\\n\\n        Args:\\n            save_path : str\\n                A directory to save the data to.\\n            dataset : str, optional\\n                A specific dataset to download.\\n                Note: this must include the file extension.\\n                If None, options will be presented for you\\n                to choose from.\\n\\n        Returns:\\n            save_path_full : str\\n                The absolute path to the downloaded data.',\n",
       " 'A basic critic for images `n_channels` x `in_size` x `in_size`.',\n",
       " 'A basic generator from `noise_sz` to images `n_channels` x `in_size` x `in_size`.',\n",
       " 'Define loss functions for a GAN from `loss_gen` and `loss_crit`.',\n",
       " 'Compute accuracy after expanding `y_true` to the size of `y_pred`.',\n",
       " 'Put the model in generator mode if `gen_mode`, in critic mode otherwise.',\n",
       " 'Evaluate the `output` with the critic then uses `self.loss_funcG` to combine it with `target`.',\n",
       " 'Create some `fake_pred` with the generator from `input` and compare them to `real_pred` in `self.loss_funcD`.',\n",
       " 'Create the optimizers for the generator and critic if necessary, initialize smootheners.',\n",
       " \"Clamp the weights with `self.clip` if it's not None, return the correct input.\",\n",
       " 'Put the various losses in the recorder and show a sample image.',\n",
       " 'Switch the model, if `gen_mode` is provided, in the desired mode.',\n",
       " 'Create a WGAN from `data`, `generator` and `critic`.',\n",
       " 'Shows `ys` (target images) on a figure of `figsize`.',\n",
       " 'Get the indexes of the layers where the size of the activation changes.',\n",
       " 'Search for `n_images` images on Google, matching `search_term` and `size` requirements,\\n    download them into `path`/`search_term` and verify them, using `max_workers` threads.',\n",
       " 'Build Google Images Search Url params and return them as a string.',\n",
       " 'Return a Google Images Search URL for a given search term.',\n",
       " 'Parse the Google Images Search for urls and return the image metadata as tuples (fname, url).',\n",
       " 'Parse the google images html to img tuples containining `(fname, url)`',\n",
       " 'Parse the Google Images Search for urls and return the image metadata as tuples (fname, url).\\n    Use this for downloads of >100 images. Requires `selenium`.',\n",
       " \"Downloads images in `img_tuples` to `label_path`. \\n    If the directory doesn't exist, it'll be created automatically.\\n    Uses `parallel` to speed things up in `max_workers` when the system has enough CPU cores.\\n    If something doesn't work, try setting up `max_workers=0` to debug.\",\n",
       " 'Downloads a single image from Google Search results to `label_path`\\n    given an `img_tuple` that contains `(fname, url)` of an image to download.\\n    `i` is just an iteration number `int`.',\n",
       " 'Setup path to save images to, init the UI, and render the widgets.',\n",
       " 'Download button click handler: validate search term and download images.',\n",
       " 'Cleanup learn model weights disturbed during LRFinder exploration.',\n",
       " 'Default constructor for the WeightDrop module\\n\\n        Args:\\n            module (torch.nn.Module): A pytorch layer being wrapped\\n            dropout (float): a dropout value to apply\\n            weights (list(str)): the parameters of the wrapped **module**\\n                which should be fractionally dropped.',\n",
       " 'for each string defined in self.weights, the corresponding\\n        attribute in the wrapped module is referenced, then deleted, and subsequently\\n        registered as a new parameter with a slightly modified name.\\n\\n        Args:\\n            None\\n\\n         Returns:\\n             None',\n",
       " \"Uses pytorch's built-in dropout function to apply dropout to the parameters of\\n        the wrapped module.\\n\\n        Args:\\n            None\\n        Returns:\\n            None\",\n",
       " 'Load a saved `DataBunch` from `path/file`. `file` can be file-like (file or buffer)',\n",
       " 'Create a `DataBunch` from `train_ds`, `valid_ds` and maybe `test_ds` with a batch size of `bs`. Passes `**dl_kwargs` to `DataLoader()`',\n",
       " 'Returns appropriate `Dataset` for validation, training, or test (`ds_type`).',\n",
       " 'Returns a list of all DeviceDataLoaders. If you need a specific DeviceDataLoader, access via the relevant property (`train_dl`, `valid_dl`, etc) as the index of DLs in this list is not guaranteed to remain constant.',\n",
       " 'Save the `DataBunch` in `self.path/file`. `file` can be file-like (file or buffer)',\n",
       " 'Add the `items` as a test set. Pass along `label` otherwise label them with `EmptyLabel`.',\n",
       " 'Get one batch from the data loader of `ds_type`. Optionally `detach` and `denorm`.',\n",
       " 'Get `item` into a batch. Optionally `detach` and `denorm`.',\n",
       " 'Export the minimal state of `self` for inference in `self.path/file`. `file` can be file-like (file or buffer)',\n",
       " 'Check the underlying data in the training set can be properly loaded.',\n",
       " 'Explore lr from `start_lr` to `end_lr` over `num_it` iterations in `learn`. If `stop_div`, stops when loss diverges.',\n",
       " 'Add mixup https://arxiv.org/abs/1710.09412 to `learn`.',\n",
       " 'Create a `ClassificationInterpretation` object from `learner` on `ds_type` with `tta`.',\n",
       " 'If we have `last_metrics` plot them in our pbar graph',\n",
       " 'accumulated step and reset samples, True will result in no stepping',\n",
       " 'step the rest of the accumulated grads if not perfectly divisible',\n",
       " 'Create an instance of `ClassificationInterpretation`',\n",
       " 'Plot the confusion matrix, with `title` and using `cmap`.',\n",
       " 'Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.',\n",
       " '`k` largest(/smallest) losses and indexes, defaulting to all losses (sorted by `largest`).',\n",
       " 'Calculates the F-beta score (the weighted harmonic mean of precision and recall).\\n    This is the micro averaged version where the true positives, false negatives and\\n    false positives are calculated globally (as opposed to on a per label basis).\\n\\n    beta == 1 places equal weight on precision and recall, b < 1 emphasizes precision and\\n    beta > 1 favors recall.',\n",
       " 'Distributed training of Imagenet. Fastest speed is if you run with: python -m fastai.launch',\n",
       " 'Cut off the body of a typically pretrained `model` at `cut` (int) or cut the model as specified by `cut(model)` (function).',\n",
       " 'Model head that takes `nf` features, runs through `lin_ftrs`, and about `nc` classes.',\n",
       " 'Create an instance of `ClassificationInterpretation`. `tta` indicates if we want to use Test Time Augmentation.',\n",
       " 'Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.',\n",
       " 'Show images in `top_losses` along with their prediction, actual, loss, and probability of predicted class in a multilabeled dataset.',\n",
       " 'Sorts `ds_type` dataset by top losses and returns dataset and sorted indices.',\n",
       " 'For a LabelList `ll_input`, resize each image to `size` using `resize_method` and `padding_mode`.',\n",
       " 'Gets the indices for the most similar images in `ds_type` dataset',\n",
       " 'Gets activations at the layer specified by `hook`, applies `pool` of dim `pool_dim` and concatenates',\n",
       " 'Computes the similarity function between each embedding of `t1` and `t2` matrices.',\n",
       " 'Returns the `n` largest indices from a numpy array `arr`.',\n",
       " 'Sorts `similarities` and return the indexes in pairs ordered by highest similarity.',\n",
       " 'Returns an image widget for specified file name `img`.',\n",
       " 'Make a horizontal box with `children` and `layout`.',\n",
       " 'Create a list of images, filenames and labels but first removing files that are not supposed to be displayed.',\n",
       " 'Relabel images by moving from parent dir with old label `class_old` to parent dir with new label `class_new`.',\n",
       " \"Handler for 'Next Batch' button click. Delete all flagged images and renders next batch.\",\n",
       " 'Check if current batch contains already deleted images.',\n",
       " 'Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Args:\\n            img (Tensor): Tensor image of size (C, H, W).\\n        Returns:\\n            Tensor: Image with n_holes of dimension length x length cut out of it.',\n",
       " 'Store all collected nbval tests for evaluation on finish',\n",
       " 'Make report in form of two notebooks.\\n\\n        Use nbdime diff-web to present the difference between reference\\n        cells and test cells.',\n",
       " \"BatchNorm layers to have parameters in single precision.\\n    Find all layers and convert them back to float. This can't\\n    be done with built in .apply as that function will apply\\n    fn to all modules, parameters, and buffers. Thus we wouldn't\\n    be able to guard the float conversion based on the module type.\",\n",
       " 'Creates a fp32 copy of model parameters and sets optimizer parameters',\n",
       " 'Start coverage reporting in kernel.\\n\\n    Currently supported kernel languages are:\\n     - Python',\n",
       " 'Finish coverage reporting in kernel.\\n\\n    The coverage should previously have been started with\\n    setup_coverage.',\n",
       " 'Create a suffix for nbval data file depending on pytest-cov config.',\n",
       " 'Convert `b` to an int or list of ints (if `is_listy`); raises exception if not convertible',\n",
       " 'List of label subdirectories in imagenet-style `folder`.',\n",
       " 'Given `arrs` is [a,b,...] and `mask`index - return[(a[mask],a[~mask]),(b[mask],b[~mask]),...].',\n",
       " 'Randomly split `arrs` with `valid_pct` ratio. good for creating validation set.',\n",
       " 'Build log-stepped array from `start` to `stop` in `n` steps.',\n",
       " 'Download `url` to `dest` unless it exists and not `overwrite`.',\n",
       " 'Return `Path(path)/Path(fname)`, `path` defaults to current dir.',\n",
       " 'Return `ndarray` of `str` of lines of text from `path`.',\n",
       " 'Split `kwargs` between those expected by `func` and the others.',\n",
       " 'Same as `np.array` but also handles generators. `kwargs` are passed to `np.array` with `dtype`.',\n",
       " 'Put the texts in `items` in an HTML table, `widths` are the widths of the columns in %.',\n",
       " 'Call `func` on every element of `arr` in parallel using `max_workers`.',\n",
       " 'Like `plt.subplots` but with consistent axs shape, `kwargs` passed to `fig.suptitle` with `title`',\n",
       " 'Return the representation of the first  `n_max` elements in `items`.',\n",
       " 'Create and return a tmp filename, optionally at a specific path. `os.remove` when done with it.',\n",
       " 'Subclass this method if you want to customize the way this `ItemBase` is shown on `ax`.',\n",
       " 'Create a seuence Conv2d->BatchNorm2d->LeakyReLu layer.',\n",
       " 'starts with conv layer - `ch_in` channels in - then has `num_blocks` `ResLayer`',\n",
       " 'Create a Learner for collaborative filtering on `data`.',\n",
       " 'Create a `DataBunch` suitable for collaborative filtering from `ratings`.',\n",
       " 'Fetch item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)',\n",
       " 'Bias for item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)',\n",
       " 'Bias for item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)',\n",
       " 'Draws a representation of a random forest in IPython.\\n    Parameters:\\n    -----------\\n    t: The tree you wish to draw\\n    df: The data used to train the tree. This is used to get the names of the features.',\n",
       " \"Changes Scikit learn's random forests to give each tree a random sample of\\n    n random rows.\",\n",
       " 'Execute notebook `fname` with `metadata` for preprocessing.',\n",
       " 'Create the documentation notebook for module `mod_name` in path `dest_path`',\n",
       " 'Search a given `path_dir` and return all the modules contained inside except those in `exclude`',\n",
       " 'Read a notebook in `fname` and return its corresponding json',\n",
       " 'Build a dictionary containing the position of the `cells`.',\n",
       " 'Create documentation links for all cells in markdown with backticks.',\n",
       " 'Return the position to insert a given function doc in a notebook.',\n",
       " 'Update the `pos_dict` by moving all positions after `start_key` by `nbr`.',\n",
       " 'Insert the function doc `cells` at their correct position and updates `pos_dict`.',\n",
       " 'Finds all submodules of notebook - sorted by submodules > top level modules > manual imports. This gives notebook imports priority',\n",
       " 'Update the documentation notebook of a given module.',\n",
       " '`source_path` can be a directory or a file. Assume all modules reside in the fastai directory.',\n",
       " 'Return a dropout mask of the same type as `x`, size `sz`, with probability `p` to cancel an element.',\n",
       " 'Split a RNN `model` in groups for differential learning rates.',\n",
       " 'Convert a value `x` from 0 to 1 (inclusive) to an RGBA tuple according to `cmap` times transparency `alpha_mult`.',\n",
       " 'Calculate the intrinsic attention of the input w.r.t to an output `class_id`, or the classification given by the model if `None`.\\n        For reference, see the Sequential Jacobian session at https://www.cs.toronto.edu/~graves/preprint.pdf',\n",
       " 'Create a tabulation showing the first `k` texts in top_losses along with their prediction, actual,loss, and probability of\\n        actual class. `max_len` is the maximum number of tokens displayed.',\n",
       " 'Take a step in lr,mom sched, start next stepper when the current one is complete.',\n",
       " 'Like `torch.as_tensor`, but handle lists too, and can pass multiple vector elements directly.',\n",
       " 'Recursively detach lists of tensors in `b `; put them on the CPU if `cpu=True`.',\n",
       " 'Recursively map lists of items in `b ` to their wrapped data.',\n",
       " 'Recursively map lists of tensors in `b ` to the cpu.',\n",
       " 'If `b` is not set return `requires_grad` of first param, else set `requires_grad` on all params as `b`',\n",
       " 'Return the children of `m` and its direct parameters not registered in modules.',\n",
       " 'Separate the parameters in `layer_groups` between `no_wd_types` and  bias (`bias_types`) from the rest.',\n",
       " 'Set bn layers in eval mode for all recursive children of `m`.',\n",
       " 'Initialize `m` weights with `func` and set `bias` to 0.',\n",
       " 'Initialize the non-batchnorm layers of `m` with `init_func`.',\n",
       " 'Initialize all non-batchnorm layers of `m` with `init_func`.',\n",
       " 'Tranform numpy array `a` to a tensor of the same type.',\n",
       " 'Grab the `i`-th batch in `x`, `batch_first` stating the batch dimension.',\n",
       " 'Draw 1 or shape=`size` random floats from uniform dist: min=`low`, max=`high`.',\n",
       " 'Draw 1 or shape=`size` random floats from uniform dist: min=log(`low`), max=log(`high`).',\n",
       " 'Draw 1 or shape=`size` random booleans (`True` occuring with probability `p`).',\n",
       " 'Generate int or tensor `size` of ints between `low` and `high` (included).',\n",
       " 'Try to convert `o` to int, default to `o` if not possible.',\n",
       " 'Check that `out` and `targ` have the same number of elements and flatten them.',\n",
       " 'create new OrderedDict that does not contain `module.`',\n",
       " 'Return a dictionary for updating `last_metrics` with `mets`.',\n",
       " 'Collects iterables lazily, rather than immediately.\\n        Docstring same as parent: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor\\n        Implmentation taken from this PR: https://github.com/python/cpython/pull/707',\n",
       " 'Generate documentation for fastai library in HTML (asciidoctor required)\\n    :param str src: The absolute/relative path of source file/dir',\n",
       " 'Retrieves new batch of DatasetType, and detaches it.',\n",
       " 'one_batch function is extremely slow with large datasets.  This is caching the result as an optimization.',\n",
       " 'Callback function that writes batch end appropriate data to Tensorboard.',\n",
       " 'Callback function that writes backward end appropriate data to Tensorboard.',\n",
       " 'Callback function that writes epoch end appropriate data to Tensorboard.',\n",
       " 'Writes gradient statistics for generator to Tensorboard.',\n",
       " 'Writes gradient statistics for critic to Tensorboard.',\n",
       " 'Writes model generated, original and real images to Tensorboard.',\n",
       " 'Callback function that writes batch end appropriate data to Tensorboard.',\n",
       " 'Callback function that writes backward end appropriate data to Tensorboard.',\n",
       " 'Writes model generated, original and real images to Tensorboard',\n",
       " 'Queues up an asynchronous write request to Tensorboard.',\n",
       " 'Processes queued up write requests asynchronously to Tensorboard.',\n",
       " 'Factory method to convert a batch of model images to a list of ModelImageSet.',\n",
       " 'Writes a single scalar value for a gradient statistic to Tensorboard.',\n",
       " 'Writes the average norm of the gradients to Tensorboard.',\n",
       " 'Writes the median norm of the gradients to Tensorboard.',\n",
       " 'Writes the maximum norm of the gradients to Tensorboard.',\n",
       " 'Writes the minimum norm of the gradients to Tensorboard.',\n",
       " 'Writes the number of zeroes in the gradients to Tensorboard.',\n",
       " 'Writes the average of the gradients to Tensorboard.',\n",
       " 'Writes the maximum of the gradients to Tensorboard.',\n",
       " 'Writes the minimum of the gradients to Tensorboard.',\n",
       " 'Gets list of image tensors from lists of Image objects, as a tuple of original, generated and real(target) images.',\n",
       " 'Writes original, generated and real(target) images to Tensorboard.',\n",
       " 'Writes training and validation batch images to Tensorboard.',\n",
       " 'Writes batch images of specified DatasetType to Tensorboard.',\n",
       " 'Wraps h in new Variables, to detach them from their history.',\n",
       " 'Invoked during the forward propagation of the RNN_Encoder module.\\n        Args:\\n            input (Tensor): input of shape (sentence length x batch_size)\\n\\n        Returns:\\n            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\\n            dropouth, list of tensors evaluated from each RNN layer using dropouth,',\n",
       " 'Replace tokens in ALL CAPS in `x` by their lower version and add `TK_UP` before.',\n",
       " 'Replace all Capitalized tokens in `x` by their lower version and add `TK_MAJ` before.',\n",
       " 'plots loss function as function of iterations. \\n        When used in Jupyternotebook, plot will be displayed in notebook. Else, plot will be displayed in console and both plot and loss are saved in save_path.',\n",
       " 'Plots learning rate in jupyter notebook or console, depending on the enviroment of the learner.',\n",
       " 'Plots the loss function with respect to learning rate, in log scale.',\n",
       " 'Implements the weight decay schedule as mentioned in https://arxiv.org/abs/1711.05101\\n\\n        :param layer_opt: The LayerOptimizer\\n        :param batch_per_epoch: Num batches in 1 epoch\\n        :param cycle_len: Num epochs in initial cycle. Subsequent cycle_len = previous cycle_len * cycle_mult\\n        :param cycle_mult: Cycle multiplier\\n        :param n_cycles: Number of cycles to be executed',\n",
       " 'Test if `last_loss` is NaN and interrupts training.',\n",
       " 'Compare the value monitored to its best score and maybe save the model.',\n",
       " 'Compare the value monitored to its best and maybe reduce lr.',\n",
       " 'Store completed epoch number in `learn.model_dir/name`.',\n",
       " 'Convert a notebook `fname` to html file in `dest_path`.',\n",
       " 'Convert modified notebooks in `folder` to html pages in `dest_path`.',\n",
       " 'Function that collect samples and adds padding. Flips token order if needed',\n",
       " 'Create the ragged array that will be filled when we ask for items.',\n",
       " 'Fill the row with tokens from the ragged array. --OBS-- overlap != 1 has not been implemented',\n",
       " 'Create a `TextDataBunch` from ids, labels and a `vocab`. `kwargs` are passed to the dataloader creation.',\n",
       " 'Load a `TextDataBunch` from `path/cache_name`. `kwargs` are passed to the dataloader creation.',\n",
       " 'Create a `TextDataBunch` from tokens and labels. `kwargs` are passed to the dataloader creation.',\n",
       " 'Create a `TextDataBunch` from DataFrames. `kwargs` are passed to the dataloader creation.',\n",
       " 'Create a `TextDataBunch` from texts in csv files. `kwargs` are passed to the dataloader creation.',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = datasets.load_from_disk(\"../docstring_len_filtered.ds\")\n",
    "dataset_dict[\"train\"][\"docstring\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
